AAMAS01g,Chairs' Welcome,,,,Edith,,Elkind,University of Oxford,Manuela,,Veloso,CMU and JPMorgan,Noa,,Agmon,Bar Ilan University,Matthew,E,Taylor,Washington State University and Borealis AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KN01a,Synthesizing Explainable Behavior for Human-AI Collaboration,,"As AI technologies enter our everyday lives at an ever increasing pace, there is a greater need for AI systems to work synergistically with humans.  This requires AI systems to exhibit behavior that is explainable to humans. Synthesizing such behavior requires AI systems to reason not only with their own models of the task at hand, but also about the mental models of the human collaborators. Using several case-studies from our ongoing research, I will discuss how such multi-model planning forms the basis for explainable behavior.",Explicability; Explanation; Human-AI Interaction; Human-AI Collaboration,Subbarao,,Kambhampati,Arizona State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KN02c,Preferences and Ethical Priorities: Thinking Fast and Slow in AI,,"In AI, the ability to model and reason with preferences allows for more personalized services. Ethical priorities are also essential, if we want AI systems to make decisions that are ethically acceptable. Both data-driven and symbolic methods can be used to model preferences and ethical priorities, and to combine them in the same system, as two agents that need to cooperate. We describe two approaches to design AI systems that can reason with both preferences and ethical priorities. We then generalize this setting to follow Kahneman's theory of thinking fast and slow in the human's mind. According to this theory, we make decision by employing and combining two very different systems: one accounts for intuition and immediate but imprecise actions, while the other one models correct and complex logical reasoning. We discuss how such two systems could possibly be exploited and adapted to design machines that allow for both data-driven and logical reasoning, and exhibit degrees of personalized and ethically acceptable behavior.
",Multi-agent system; Knowledge Representation; Decision Theory,Francesca,,Rossi,IBM Research,Andrea,,Loreggia,University of Padova,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KN03e,Responsible Autonomy,,"
The main challenge that artificial intelligence research is facing nowadays is how to guarantee the development of responsible technology. And, in particular, how to guarantee that autonomy is responsible. The social fears on the actions taken by AI can only be appeased by providing ethical certification and transparency of systems. (See for instance the Barcelona declaration https://www.iiia.csic.es/barcelonadeclaration/) However, this is certainly not an easy task. As we very well know in the multiagent systems field, the prediction accuracy of system outcomes has limits as multiagent systems are actually examples of complex systems. And AI will be social, there will be thousands of AI systems interacting among themselves and with a multitude of humans; AI will necessarily be multiagent.

Although we cannot provide complete guarantees on outcomes, we must be able to define with accuracy what autonomous behaviour is acceptable (ethical), to provide repair methods for anomalous behaviour and to explain the rationale of AI decisions. Ideally, we should be able to guarantee responsible behaviour of individual AI systems by construction.

I understand by an ethical AI system one that is capable of deciding what are the most convenient norms, abide by them and make them evolve and adapt. The area of multiagent systems has developed a number of theoretical and practical tools that properly combined can provide a path to develop such systems, that is, provide means to build ethical-by-construction systems: agreement technologies to decide on acceptable ethical behaviour, normative frameworks to represent and reason on ethics, and electronic institutions to operationalise ethical interactions. Along my career, I have contributed with tools on these three areas. In this keynote, I will describe a methodology to support their combination that incorporates some new ideas from law, and organisational theory.
",Responsible AI; Agreement Technologies; Normative Systems,Carles,,Sierra,IIIA-CSIC,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc01,Empathic Agents: A Hybrid Normative/Consequentialistic Approach,,"Complex information systems operate with increasing degrees of autonomy. Consequently, such systems should not only optimize for simple metrics (like clicks and views) that reflect the system provider's preferences but also consider norms or rules, as well as the preferences of other agents that are affected by the systems' actions. As a means to achieve such behavior, we propose the design and development of empathic agents that use a mixed rule/utility-based approach when deciding on how to act, considering both their own and others' utility functions. The agents make use of formal argumentation to reach an agreement on how to act in case of inconsistent beliefs. A promising domain for applying our empathic agents is recommender systems.",Agent architectures; Agent-oriented software engineering; Argumentation,Timotheus,,Kampik,Umeå University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc02,Integrating Agent Advice and Previous Task Solutions in Multiagent Reinforcement Learning,,"Reinforcement learning methods have successfully been applied to build autonomous agents that solve challenging sequential decision-making problems. However, agents need a long time to learn a task, especially when multiple autonomous agents are in the environment. This research aims to propose a Transfer Learning framework to accelerate learning by combining two knowledge sources: (i) previously learned tasks; and (ii) advice from a more experienced agent. The definition of such framework requires answering several challenging research questions, including: How to abstract and represent knowledge, in order to allow generalization and posterior reuse?, How and when to transfer and receive knowledge in an efficient manner?, and How to consistently combine knowledge from several sources?",Transfer Learning; Reinforcement Learning; Multiagent Learning,Felipe,Leno Da,Silva,University of Sao Paulo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc03,Multi-Agent Coordination under Uncertain Communication,,"Multi-agent coordination problems have many layers of complexity that make them interesting research problems. What uniquely separates multi-agent coordination problems from large single-agent planning and execution problems is that in multi-agent problems, different agents have different understandings of global state. The way that diverging beliefs are reconciled is through communication. My thesis builds a real-time executive that manages and reasons about communication between agents and provides the supplementing theory to prove that these algorithms are practical and efficient.",Single and multiagent planning and scheduling; Coordination and control models for multiagent systems,Nikhil,,Bhargava,Massachusetts Institute of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc04,Teaching Agents Through Correction,,We motivate and describe a novel task which is modelled on interactions between apprentices and expert teachers. In the task an agent must learn to build towers constrained by rules. The teacher provides verbal corrective feedback from which the agent learns. The agent starts out unaware of the constraints as well as the domain concepts in which the constraints are expressed. Therefore an agent that takes advantage of the linguistic evidence must  learn the denotations of neologisms and adapt its conceptualisation of the planning domain to incorporate those denotations.  We show that an agent which does utilise linguistic evidence outperforms a strong baseline which does not.,human-robot interaction; interactive learning; knowledge representation and reasoning,Mattias,,Appelgren,University of Edinburgh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc05,Sharing is Caring: Dynamic Mechanism for Shared Resource Ownership,Doctoral Consortium,"Shared ownership of computing resources has long been in the practice; here, multiple agents pool their resources together to achieve high utility and low wastefulness. Sharing incentive, non-wastefulness and strategyproofness are three of the most desirable properties for a feasible system. However, Freeman et al. [2018] showed the fact that these three properties are incompatible in a dynamic setting and thus, a trade off must be maintained. In this work, we propose a dynamic allocation mechanism which fairly allocates the shared resources among the agents, and partially satisfies the above desiderata. Our mechanism outperforms the mechanisms proposed by Freeman et al. [2018] in the single resource case in terms of social welfare both in synthetic and real-life data. We also show that in the single resource case, our mechanism allocates the resources in a way that creates a market equilibrium and thus naturally satisfies several additional properties.",Algorithmic Game Theory; Resource Allocation; Market Mechanism,Ridi,,Hossain,National University of Singapore,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc06,Mechanism Design with Unstructured Beliefs,,"Mechanism design is the task to design algorithms, toward desired objectives, that is robust to potential manipulation by strategic players. Traditionally, it is assumed that the mechanism designer and the players in the economy share some common knowledge. However, as pointed out by Wilson, such common knowledge is ``rarely present in experiments and never in practice'', and ``only by repeated weakening of common knowledge assumptions will the theory approximate reality.'' In the work, we mainly focus on designing resilient mechanisms that work properly even in such a less foreseeable environment.

Bayesian auction design is a very flourishing topic in the field of mechanism design, where an important simplifying assumption is both the seller and the players know the exact distributions of all players' valuations. In this work we first consider the {\em query complexity} of Bayesian mechanisms, where we only allow the seller to have limited oracle accesses to the players' value distributions via simple queries. Then we further weaken the assumption by considering an information structure where the knowledge about the distributions can be {\em arbitrarily scattered} among the players. In both of these two unstructured information settings, we design mechanisms that are constant approximations to the optimal Bayesian mechanisms with full information.


Finally, we study an envy-free allocation problem that the unstructured beliefs need to be taken into consideration. In particular, we model an environment where each player is unaware of the bundles (or allocated items) of other players, but still knows he does not receive the worst bundle. We present both conceptual and algorithmic results for this new envy-free allocation domain.",Bayesian Auction; Query Complexity; Information Elicitation; Fair Allocation; Maximin-Aware Allocation,Bo,,Li,Stony Brook University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc07,Explainable Agency in Intelligent Agents,Doctoral Consortium,"Explainability of intelligent agents has gained attention in recent years with their widespread utilization in society. Most work in Explainable AI (XAI) pay little attention to the humans that interact with these models, which risks resulting in unsatisfactory explanations. Theories of explainability and the nature of explanation has been widely explored in cognitive psychology and philosophy. This  thesis aims to incorporate these insights to build explainable models and interfaces that can provide better and sufficient explanations to the interacting human.",Explainable AI; Interpretable Machine Learning; Causal Explanation,Prashan,,Madumal,University of Melbourne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc08,Complexity of Distances in Elections,Doctoral Consortium,"Our research investigates the influence of distances on elections and the associated problems such as bribery, manipulation, and control. Distances can be used in elections in many different ways. In the case of interference problems, distances can be used to limit
the changes or to calculate costs. At the same time, distances can themselves be used to design fair elections. Our goals here are to define appropriate frameworks and to determine the complexity of the problems with respect to approximation and parameterization.",Elections; Distances; Scoring Systems; Bribery; Manipulation,Tobias,,Hogrebe,Heinrich-Heine-Universität Düsseldorf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc09,Studies on the Computational Modeling and Design of Financial Markets,,"Electronic marketplaces and the automation of trading have transformed the financial market from a human-decision ecosystem to an algorithmic one. Questions and challenges that arise from this new algorithmic ecosystem naturally lend themselves to computational approaches. My research builds computational models to understand trading behaviors, designs market mechanisms robust to manipulation, and proposes algorithms to facilitate order matchings.",Market manipulation; agent-based simulation; trading agents; mechanism design,Xintong,,Wang,University of Michigan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc10,Conversational Narrative Interfaces for <i>Sensemaking</i>,,"This PhD thesis aims at studying the generation of dialogues within the context of narratives based on the theory of inferential pragmatics. When referring to inferential pragmatics, we consider the non-semantic elements of the meaning of statements that can be inferred from its implications. Our goal is to generate dialogue statements based on a model that integrates pragmatic context knowledge. This model aims to generate misunderstandings arising within a dialogue between a human user and a virtual agent, focusing on contextual inconsistencies.
",Dialogue Models; Social Agents; Narratives; Speech Act Theory; Pragmatics,Andreea-Oana,,Petac,École Nationale d'Ingénieurs de Brest,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc11,Bridging the Gap Between High-Level Reasoning in Strategic Agent Coordination and Low-Level Agent Development,,"Recent advances in fields such as computer vision and natural language processing have paved the way for developing agents capable of automatically interpreting their surrounding environment. Concurrently, advances in artificial intelligence have made the coordination of many such agents possible. However, there is little work considering both the low-level reasoning that allows agents to interpret their environment, such as deep learning techniques, and the high-level reasoning that coordinates such agents. By considering both together, we can better handle real-world scenarios, for example by planning at a high level with low-level uncertainty in mind, or even by improving low-level processing by using high-level reasoning to place the agent in the best scenario for success.",security games; computational sustainability; uncertainty; sensors,Elizabeth,,Bondi,University of Southern California,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc12,Enhanced Learning from Multiple Demonstrations with a Flexible Two-level Structure Approach,,"Learning from demonstration (LfD) has been emerged as a successful transfer learning technique to speed up reinforcement learning (RL). However, the effectiveness of the LfD heavily depends on the quality of the demonstrations. This work investigates how to enable efficient human-agent (or agent-agent) knowledge transfer and allow the RL agent to extract useful information from multiple demonstrations of different quality. In particular, we aim to avoid the effect of noise or bad examples from the collected demonstration data. Inspired by the multi-armed contextual bandit problem and Human Agent Transfer algorithm, we developed a Flexible Two-level Structured Approach to address the above challenges. Evaluated with Mario, Cart Pole and RC Car domains, the experimental results show that this approach holds the promising capacity to successfully leveraging the demonstrations of different quality.",Learning from Demonstration; Multi-Armed Bandit; Reinforcement Learning; Transfer Learning,Su,,Zhang,Washington State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc13,Aspects of Transparency in Machine Learning,,"The fact that machine learning is growing more and more entrenched in almost every aspect of society, combined with the opacity of various of its algorithms has induced the relatively young research area of transparent machine learning. The aim of this domain is to provide explanations for automated decisions to increase public trust. In my thesis, I am going to consider certain problems that arise from this research agenda. Particularly, I so far considered, the dilemma of conflicting explanations and the issue of privacy concerns arising from transparency.","Cooperative games: theory & analysis; Social choice theory; Game theory for practical applications; Values in MAS (privacy, safety, security, transparency, $dots$; Transparent Machine Learning",Martin,,Strobel,National University Singapore,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc14,Persuasive Social Robots using Social Power Dynamics,,"Social Power, the potential for social influence, is a pervasive social process in human interactions. On the other hand, recent advances on Social Robotics raise the question whether a social robot can be used as a persuasive agent. To date, different attempts have been performed using several approaches to tackle this research question. However, few studies looked at the concept of social power in Human-Robot Interaction (HRI) and how it can be beneficial to the development of persuasion skills. This is the precisely the goal of the work that is described here. In this text, we briefly report the results of our recent advancements for this objective and draw suggestion for speculating on future directions.",Social Power; Persuasion; Trust; Social Robot; Human-Robot Interaction; HRI,Mojgan,,Hashemian,INESC-ID & Universidade de Lisboa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc15,Strategic Location and Network Formation Games,,"Game theory is a powerful approach to analyze settings which result from selfish behavior. This research aims to investigate models from different game theory areas: Network Creation Games, Schellings Segregation and Strategic Facility Location. I propose extensions of the classical simple models with more realistic assumptions and to analyze these models with respect to the existence of stable states, the Price of Anarchy and convergence dynamics.",Location Analysis; Facility Location Games; Approximate Pure Subgame Perfect Equilibria; Agent-based Simulation; Dynamics,Louise,,Molitor,Hasso Plattner Institute,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc16,Improving Deep Reinforcement Learning via Transfer,Doctoral Consortium,"While achieving the state-of-the-art performance in complex sequential tasks, deep reinforcement learning (deep RL) remains extremely data inefficient. Many approaches have been studied to improve the data efficiency of deep RL algorithms. This dissertation focuses on leveraging various transfer learning techniques to tackle this problem. We first show that positive transfer can be achieved cross-domains via direct weight transfer if the two agents share a certain amount of similarities. Then we look into how could the similarity between cross-domain tasks be quantified, such that we only transfer useful information from one task to another while blocking information that might have a negative effect. The third direction we studied is the human-agent transfer mechanism, which we integrate human knowledge via supervised pre-training on a set of demonstration data collected from a human then transfer to an agent. Lastly, several future directions are proposed for the remainder of this dissertation.","Reinforcement learning; Deep learning; Learning agent capabilities (agent models, communication, observation)",Yunshu,,Du,Washington State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc17,Novel Hedonic Games and Lottery Systems,,"We present here work on two types of matching problems, namely Hedonic Games, also known as Coalition Formation Games, and on quota-based lottery systems such as the one used in Singapore to allocate public housing.  We introduce two Hedonic Games, and investigate the computational complexity of finding optimal partitions of agents into coalitions, or finding --- or determining the existence of --- stable coalition structures. We propose a new stability notion for hedonic games and prove its distinctness from existing notions of stability. We present results from investigations into the fairness of lotteries with diversity quotas inspired by the public housing allocation system in Singapore.",Coalition formation (non-strategic); Cooperative games: computation;  Cooperative games:theory & analysis,Jacob,,Schlueter,University of Kentucky,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc18,Adaptable Decentralized Task Allocation of Swarm Agents,Doctoral Consortium,"Scalable task allocation in dynamic real-world domains often requires efficient, robust, decentralized, and adaptable approaches. Response threshold reinforcement is a biologically-inspired model of probabilistic action that has been shown to lead to efficient task allocation among swarm agents that do not reason or communicate, making it a highly scalable and low cost solution. The model leads agents to specialize, resulting in reduced costs of interference and task switching, as well as to improved efficiency and adaptability to initially unknown environments. While initial specialization of this and other models is investigated in much of existing literature, subsequent re-adaptation to domain changes is seldom verified. Our goal is to investigate the robustness of response threshold reinforcement to various environmental changes, as well as to compare this model to other decentralized approaches.",agent cooperation: biologically-inspired approaches and methods; multi-robot systems; agent societies: self-organization,Vera,A.,Kazakova,University of Central Florida,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc19,Problems in Computational Mechanism Design,,"My research area is interdisciplinary to Mechanism Design and Algorithm Design. The problems I find interesting in Computational Mechanism Design are Voting preferences, Environment protection by reducing the emission of harmful gases from automobiles, Shareable good allocation on a network and Peer grading. In this paper, I explain two of such problems and roadmaps for them. First is the domain of preferences in the voting. It is often observed that preferences are never completely arbitrary; instead, they possess correlated structures. After learning the preferences, the next main task is to aggregate them and have an outcome out of it. My interest is to explore the different domains of preferences so that we can have specific desirable properties in the social choice function. Another problem I am interested in is, to devise a mechanism which incentivises the riders to prefer to share the ride than to ride solo,
where the objective of the mechanism is to reduce the emission of harmful gases and control the pollution in the environment by reducing the total travelled distance by the vehicles. A significant challenge when addressing the problem of ridesharing is that it needs to explore a vast decision space while computing solutions fast enough to provide users with the experience of real-time booking and service.",Computational social choice; preferential domains; sampling; algorithms; Ridesharing,Garima,,Shakya,Indian Institute of technology Kanpur,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc20,Intelligent Multi-Purpose Healthcare Bot Facilitating Shared Decision Making,,"Patient decision aids (PtDAs) have been promoted to facilitate personalized information retrieval and decision support; nonetheless, although promoted for more than 20 years, they have generally failed to gain a foothold in the general delivery of healthcare. Intelligent interactive agent technologies could address the design features necessary to facilitate support and shared-decision making.  In this thesis, we develop and build a PtDA for Prostate cancer using intelligent agent technology.  The proposed system, called ALAN, has a multi-layered architecture with three layers. While the first layer (User-Interface) is responsible to effectively interact with users (patients and physicians), the bottom layer (Data) handles requests regarding storing and retrieving the data. Unlike most existing bots, our core objective is to enable ALAN with learning abilities, which can evolve in the course of time and improve its behaviour with minimum distraction of the user.  To this end, reinforcement learning and deep learning algorithms are employed in the main layer, i.e., Analytical Decision Making. This research is expected to have impact on delivery of personalized healthcare.",Shared-decision making; Patient decision aids; Multi-agent systems; Chatbots; Reinforcement learning,Mohammad Mehdi,,Afsar,University of Calgary,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc21,Proactive Distributed Constraint Optimization Problems,Doctoral Mentoring Program,"Current approaches that model dynamism in DCOPs solve a sequence of static problems, reacting to changes in the environment as the agents observe them. Such approaches thus ignore possible predictions on future changes. To overcome this limitation, we introduce (finite-horizon) Proactive Dynamic DCOPs (PD-DCOPs) and Infinite-Horizon PD-DCOPs (IPD-DCOPs) to model dynamic DCOPs in the presence of exogenous uncertainty. In contrast to reactive approaches, PD-DCOPs and IPD-DCOPs are able to explicitly model the possible changes to the problem, and take such information into account proactively, when solving the dynamically changing problem. The additional expressivity of these formalisms allows them to model a wider variety of distributed optimization problems. Our work presents both theoretical and practical contributions that advance current dynamic DCOP models.",Distributed Problem Solving;Distributed Constraint Optimization;DCOP,Khoi,,Hoang,Washington University in St. Louis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc22,Using Social and Physiological Signals for User Adaptation in Conversational Agents,Doctoral Consortium,"In face-to-face communication, humans subconsciously emit social signals which are picked up and used by their interlocutors as feedback for how well the previously communicated messages have been received. The feedback is then used in order to adapt the way the coming messages are being produced and sent to the interlocutor, leading to the communication to become as efficient and enjoyable as possible. Currently however, it is rare to find conversational agents utilizing this feedback channel for altering how the multimodal output is produced during interactions with users, largely due to the complex nature of the problem.
In most regards, humans have a significant advantage over conversational agents in interpreting and acting on social signals. Humans are however restricted to a limited set of sensors, ""the five senses"", which conversational agents are not. This makes it possible for conversational agents to use specialized sensors to pick up physiological signals, such as skin temperature, respiratory rate or pupil dilation, which carry valuable information about the user with respect to the conversation. This thesis work aims at developing methods for utilizing both social and physiological signals emitted by humans in order to adapt the output of the conversational agent, allowing for an increase in conversation quality. These methods will primarily be based on automatically learning adaptive behavior from examples of real human interactions using machine learning methods.","Learning agent capabilities (agent models, communication, observation); Deep learning; Single and multi-agent planning and scheduling",Patrik,,Jonell,KTH Royal Institute of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de002,NAKED: N-Ary Graphs from Knowledge Bases Expressed in Datalog±,,"In this demonstration paper, we introduce NAKED: a new generator for n-ary logic-based argumentation frameworks instantiated from inconsistent knowledge bases expressed using \datalog. The tool allows to import a knowledge base in DLGP format, generate, visualise and export the corresponding argumentation hypergraph. We show its application on a use-case from the NoAW project.",Logic-based argumentation; Datalog±; Agent reasoning,Bruno,,Yun,"INRIA GraphIK, Université de Montpellier",Madalina,,Croitoru,"INRIA GraphIK, Université de Montpellier",Srdjan,,Vesic,"CRIL - CNRS, Université d’Artois",Pierre,,Bisquert,"IATE, INRA, INRIA GraphIK",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de003,ONECG: Online Negotiation Environment for Coalitional Games,,"Coalitional games can be used to model a variety of problems in the real world.In coalitional game theory, how players form coalitions and divide payoffs is one fundamental issue to be answered.This demo presents an online negotiation environment for coalitional games (ONECG), in which coalitional negotiation can be conduced in a distributed way between people, agents, or in mixed settings via offer exchange and natural language communication.ONECG also allows configuration of specifications of coalitional games, and supports the rapid development of new negotiating agents through a set of well-defined APIs.This new environment is helpful to facilitate research on training human negotiation skills in coalitional games as well as the design of negotiation agents.",Negotiation; Human-agent interaction; Coalitional negotiation games; Cooperative game theory,Siqi,,Chen,Tianjin University,Yonghao,,Cui,Tianjin University,Cong,,Shang,Tianjin University,Jianye,,Hao,Tianjin University,Gerhard,,Weiss,Maastricht University,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de005,PAPOW: Papow Aggregates Preferences and Orderings to select Winners,,"In this demonstration paper, we introduce PAPOW: Papow Aggregates Preferences and Orderings to select Winners. The tool allows for demographic filtering of voters depending on their characteristics. We show its application on a use-case from the NoAW H2020 project.",Voting Theory; Demographic Filtering; Preference Aggregation,Martin,,Jedwabny,IATE INRA & University of Buenos Aires,Pierre,,Bisquert,"IATE INRA, INRIA GraphIK",Madalina,,Croitoru,INRIA GraphIK,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de006,Multi-Agent Path Finding on Real Robots,,"Multi-agent path finding (MAPF) deals with the problem of finding a collision-free path for a set of agents in a graph. It is an abstract version of the problem to coordinate movement for a set of mobile robots. This demo presents software guiding through the MAPF task, starting from the problem formulation and finishing with execution of plans on real robots. Users can design grid-like maps, specify initial and goal locations of robots, generate plans using various abstract models implemented in the Picat programming language, simulate and visualize execution of these plans, and translate the plans to command sequences for Ozobots, small robots developed for teaching programming.
",path planning; multi-robot; Ozobot,Roman,,Barták,Charles University,Ivan,,Krasi&#269;enko,Charles University,Ji&#269;í,,Švancara,Charles University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de011,Deploying a Shareholder Rights Management System onto a Distributed Ledger,Demonstration,"This work demonstrates how a multi-company shareholder rights management system has been implemented using Distributed Ledger Technology (DLT). In this demo, we use a permissioned blockchain to store our corporate data, such as the list of all registered companies, each company's  shareholders and how many shares everyone holds. It is assumed that the nodes of the blockchain are controlled by the main stakeholder agents but we show that users who do not run a node can still use multiple websites to access company information. On top of this, we show our system can be used to allow any shareholder to participate in elections for company matters. Lastly, we describe how we designed our system's architecture so that it could be implemented even on a public blockchain.",Distributed Ledger Technology; Blockchain; Shareholder Rights Management,Luke,,Riley,King's College London,Grammateia,,Kotsialou,King's College London,Amrita,,Dhillon,King's College London,Toktam,,Mahmoodi,King's College London,Peter,,McBurney,King's College London,Richard,,Pearce,Crowdcube,,,,,,,,,,,,,,,,,,,,,,,,
de012,ALBidS: A Decision Support System for Strategic Bidding in Electricity Markets,Demonstration,"This work demonstrates a system that provides decision support to players in electricity market negotiations. This contribution is provided by ALBidS (Adaptive Learning strategic Bidding System), a decision support system that includes a large number of distinct market negotiation strategies, and learns which should be used in each context in order to provide the best expected response. The learning process on the best negotiation strategies to use at each moment is developed by means of several integrated reinforcement learning algorithms. ALBidS is integrated with MASCEM (Multi-Agent Simulator of Competitive Electricity Markets), which enables the simulation of realistic market scenarios using real data.",Multi-agent simulation; electricity markets; decision support systems; machine learning,Tiago,,Pinto,Polytechnic of Porto,Zita,,Vale,Polytechnic of Porto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de013,Decision Support System for Opponents Selection in Electricity Markets Bilateral Negotiations,Demonstration,"This paper presents a new multi-agent decision support system with the purpose of aiding bilateral contract negotiators in the pre-negotiation phase, through the analysis of their possible opponents. The application area of this system is the electricity market, in which players trade a certain volume of energy at a specified price. Consequently, the main output of this system is the recommendation of the best opponent(s) to trade with and the target energy volume to trade with each of the opponents. These recommendations are achieved through the analysis of the possible opponents’ past behavior, namely by learning on their past actions. The result is the forecasting of the expected prices against each opponent depending on the volume to trade. The expected prices are then used by a game-theory based model, to reach the final decision on the best opponents to negotiate with and the ideal target volume to be negotiated with each of them.",Decision support systems; bilateral negotiations; ma-chine learning; power and energy systems,Francisco,,Silva,Polytechnic of Porto,Tiago,,Pinto,Polytechnic of Porto,Zita,,Vale,Polytechnic of Porto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de014,Practical Application of a Multi-Agent Systems Society for Energy Management and Control,Demonstration,"Power and energy systems lack decision-support systems that enable studying big problems as a whole. The interoperability between multi-agent systems that address specific parts of the global problem is essential. Ontologies ease interoperability between heterogeneous systems providing semantic meaning to the information exchanged between the various parties. This paper presents the practical application of a society of multi-agent systems, which uses ontologies to enable the interoperability between different types of agent-based simulators, directed to the simulation and operation of electricity markets, smart grids and residential energy management. Real data-based demonstration shows the proposed approach advantages in enabling comprehensive, autonomous and intelligent power system simulation studies.","Multi-agent simulation; power and energy systems, semantic interoperability",Tiago,,Pinto,Polytechnic of Porto,Gabriel,,Santos,Polytechnic of Porto,Zita,,Vale,Polytechnic of Porto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de015,Computational Argumentation-based Clinical Decision Support,,"This demonstration highlights the design of the CONSULT system, a modular decision-support system (DSS) intended to help patients suffering from chronic conditions self-manage their treatments. The system takes input from multiple sources, including commercial wellness sensors and a patient's electronic health record, to inform a computational argumentation engine that constructs weighted opinions using these inputs and knowledge about their sources, and uses an interaction agent driven by argumentation-based dialogue to respond to user queries.",Argumentation; Human-agent interaction; Decision-support tool,Martin,,Chapman,King's College London,Panagiotis,,Balatsoukas,King's College London,Mark,,Ashworth,King's College London,Vasa,,Curcin,King's College London,Nadin,,Kökciyan,King's College London,Kai,,Essers,King's College London,Isabel,,Sassoon,King's College London,Sanjay,,Modgil,King's College London,Simon,,Parsons,King's College London,Elizabeth,I.,Sklar,King's College London,,,,,,,,
de016,ConCon: A Contract Conflict Identifier,,"Contracts are the main medium through which people and legal entities formalise their trade relations, be they the exchange of goods or the specification of mutual obligations. While electronic contracts allow automated processes to verify their correctness, most agreements in the real world are still encoded in contracts written in natural language, necessitating substantial human revision effort to eliminate possible conflicting statements, especially for long and complex contracts. We demonstrate the ConCon (Contract Conflicts) tool, to automatically read natural language contracts and indicate potential conflicts among their clauses. Using our tool, legal professionals and the general public can benefit from a ranking of potential conflicts between the clauses in a contract, saving time and effort from legal experts in contract proof-reading.",Natural Language Processing; Norms; Norm Conflicts; Semantic Representation,João Paulo,,Aires,Pontifical Catholic University of Rio Grande do Sul,Roger,,Granada,Pontifical Catholic University of Rio Grande do Sul,Felipe,,Meneguzzi,Pontifical Catholic University of Rio Grande do Sul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de018,Learning an Effective Control Policy for a Robotic Drumstick via Self-Supervision,,"We train a neural network to control a drumstick fastened to a motor. The network takes a temporally arranged sequence of desired strikes, or a rhythm, as input and outputs a sequence of motor velocities controlling the drumstick's physical movement. We use a new method of training, we call Collaborative Network Training, in which three networks work together to directly minimize a non-differentiable loss function. In this work, the goal is to minimize the difference between the input sequence and the resulting drumstick strikes on a surface produced by the network outputs. The resulting policy learned by the network works in real-time and has a precision of 10 milliseconds.",robot learning; controls,Mason,,Bretan,Samsung Research America,Siddharth,,Sanan,Samsung Research America,Larry,,Heck,Samsung Research America,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de020,Peer-to-Peer Negotiation for Optimising Journeys of Electric Vehicles on a Tour of Europe,,"Real-world transportation networks provide rich and complex environments, well-suited to the deployment of multi-agent systems. In this demonstration, we simulate a population of electric vehicles making a journey between two cities. The challenge for the vehicles lies in making decisions of how best to recharge their batteries using the small number of charging stations that are available on their route. We investigate several scenarios that use a combination of conventional planning techniques alongside automated negotiation, and evaluate their effects on the efficiency of the system.",Negotiation;Emergent behaviour;Transportation;Electric vehicle,Seyed Ali,,Hosseini,Fetch.ai,Diarmid,,Campbell,Fetch.ai,Marco,,Favorito,Fetch.ai,Jonathan,,Ward,Fetch.ai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de021,eXplainable Modeling (XM): Data Analysis for Intelligent Agents,,"Intelligent agents perform key tasks in several application domains by processing sensor data and taking actions that maximize reward functions based on internal models of the environment and the agent itself. In this paper we present eXplainable Modeling (XM), a Python software which supports data analysis for intelligent agents. XM enables to analyze state-models, namely models of the agent states, discovered from sensor traces by data-driven methods, and to interpret them for improved situation awareness. The main features of the tool are described through the analysis of a real case study concerning aquatic drones for water monitoring.",Data analysis; situation assessment; interpretability; data visualization; time series segmentation; clustering; model generation; activity recognition; model explainability,Alberto,,Castellini,University of Verona,Francesco,,Masillo,University of Verona,Riccardo,,Sartea,University of Verona,Alessandro,,Farinelli,University of Verona,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de022,Using Game Theory in Real Time in the Real World: A Conservation Case Study,,"In the real world, real-time data are now widely available, especially in security domains. Security cameras, aerial imagery, and even social media keep defenders informed when protecting important events, locations, and people. Further, advances in artificial intelligence have led to tools that can interpret these data automatically. Game theoretic models, for example, have shown great success in security. However, most of them ignore real-time information. In this paper, we demonstrate the potential to use real-time information from imagery to better inform our decisions in game theoretic models for security. As a concrete example, a conservation group called Air Shepherd uses conservation drones equipped with thermal infrared cameras to locate poachers at night and alert park rangers. They have also used lights aboard the drones, or signaled, to warn poachers of their presence, which often deters the poachers. We propose a system that (i) allocates drones and humans strategically throughout a protected area, (ii) detects poachers in the thermal infrared videos recorded by the conservation drones flying through the protected area in the predetermined location, and (iii) recommends moving to the location and/or signaling to the poacher that a patroller is nearby depending on real-time detections. View the demonstration: http://bit.ly/aamas19-demo-bondi-et-al.",security games; computational sustainability; uncertainty; sensors; unmanned aerial vehicles,Elizabeth,,Bondi,University of Southern California,Hoon,,Oh,Carnegie Mellon University,Haifeng,,Xu,Harvard University,Fei,,Fang,Carnegie Mellon University,Bistra,,Dilkina,University of Southern California,Milind,,Tambe,University of Southern California,,,,,,,,,,,,,,,,,,,,,,,,
de024,Tangible Robotic Fleet Control,,"The use of multi-robot teams for field missions is increasing in number and
scope, requiring command and control interfaces to be adapted to the
operator's needs. Instead of working on interaction modalities that emerge
from the engineering realm (screen, gestures or voice), we look at how the
humanitarian and military logistics teams collaborate: using physical
maps. In this demo, we present our command center, which consists of a swarm
of small tabletop robots used to visualize and control a fleet of flying
robots. To ensure the scalabilty and robustness of our control system, we
leverage decentralized behaviors written in a swarm-specific programming
language. We set an example scenario, where the operator must command the
fleet to search an area for simulated features of interest using the
tabletop robots over a map. The actions of the operator send the flying
robots to individual waypoint targets. Meanwhile, the command center
monitors the operator: if he is not alone, he may lack focus, and the fleet
thus switches to an autonomous deployment mode until the operator's full
attention is back.",tangible interface; swarm interaction; aerial swarm systems,David,,St-Onge,Polytechnique Montreal,Vivek-Shankar,,Varadharajan,Polytechnique Montreal,Giovanni,,Beltrame,Polytechnique Montreal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de025,RE-ORG: An Online Repositioning Guidance Agent,,"Continuous matching of supply and demand through online/real-time decision making is a problem of critical importance in many urban environments. Examples include bike/scooter sharing systems, car sharing, food/ grocery delivery, smart vending machines and other similar aggregation systems. In these systems, apart from supply (e.g., bikes/scooters, cars, restaurants/supermarkets) and demand (e.g., customers), there may also be matching resources that help match supply and demand (e.g., trucks, car drivers, delivery boys). The success of these systems depends on the ability to have supply available at the ``right"" locations at the ``right"" time.

The matching decisions taken by the company operators are typically performed based on the current status of supply, demand and location of matching resources. In this paper, we present RE-ORG (Repositioning agEnt for Online spatio-tempoRal matchinG problems) to provide guidance for matching resources to have supply at the right locations at the right time to serve demand. Apart from providing the guidance on matching decisions, RE-ORG also provides a real time status of the supply, demand and matching resources.",Sequential Matching; Repositioning Guidance; Optimization,Muralidhar,,Konda,Singapore Management University,Pradeep,,Varakantham,Singapore Management University,Aayush,,Saxena,Singapore Management University,Meghna,,Lowalekar,Singapore Management University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de026,STV: Model Checking for Strategies under Imperfect Information,Demonstration,"We present an experimental tool for verification of strategic abilities
under imperfect information, as well as strategy synthesis.
The problem is well known to be hard, both theoretically and in
practice. The tool, called StraTegic Verifier (STV), implements
several recently developed algorithms to overcome the complexity.",formal methods; alternating-time temporal logic; imperfect information; strategy synthesis; model checking,Damian,,Kurpiewski,Polish Academy of Sciences,Wojciech,,Jamroga,Polish Academy of Sciences,Micha&#322;,,Knapik,Polish Academy of Sciences,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de027,Deciding the Winner of a Debate Using Bipolar Argumentation,Demonstration,"Bipolar Argumentation Frameworks (BAFs) are an important class of argumentation frameworks useful for capturing, reasoning with, and deriving conclusions from debates. They have the potential to make solid contributions to real-world multi-agent systems and human-agent interaction in domains such as legal reasoning, healthcare and politics. Despite this fact, practical systems implementing BAFs are largely lacking. In this demonstration, we provide a software system implementing novel algorithms for calculating extensions (winning sets of arguments) of BAFs. Participants in the demonstration will be able to input their own debates into our system, and watch a graphical representation of the algorithms as they process information and decide which sets of arguments are winners of the debate.",Argumentation; Structured Argumentation; Bipolar Argumentation,Amin,,Karamlou,Imperial College London,Kristijonas,,&#268;yras,Imperial College London,Francesca,,Toni,Imperial College London,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de028,Traffic3D: A New Traffic Simulation Paradigm,,"The field of Deep Reinforcement Learning has evolved significantly over the last few years. However, an important and not yet fully-attained goal is to produce intelligent agents which can be successfully taken out of the laboratory and employed in the real-world. Intelligent agents that are successfully deployable in real-world settings require substantial prior exposure to their intended environments. When this is not practical or possible, the agents benefit from being trained and tested on powerful test-beds, effectively replicating the real-world. To achieve traffic management at an unprecedented level of efficiency, in this work, we demonstrate a significantly richer new traffic simulation environment; Traffic3D, a platform to effectively simulate and evaluate a variety of 3D road traffic scenarios, closely mimicking real-world traffic characteristics, including faithful simulation of individual vehicle behavior, precise physics of movement and photo-realism. In addition to deep reinforcement learning, Traffic3D also facilitates research in several other domains such as imitation learning, learning by interaction, visual question answering, object detection and segmentation, unsupervised representation learning and procedural generation.",Virtual Reality 3D-Traffic Simulator; Intelligent Transportation Systems; Traffic and Transportation; Machine Learning; Deep Learning,Deepeka,,Garg,Aston University,Maria,,Chli,Aston University,George,,Vogiatzis,Aston University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de029,Implementing Business Processes in JaCaMo+ by Exploiting Accountability and Responsibility,,"In this demo paper we exemplify a business process programming approach that provides proper abstractions for capturing goals and relationships between the actors. To this end, we propose a paradigm shift from a procedural (activity-oriented) to an agent-based approach, where the agent technology is the means through which processes are implemented, while relationships are based upon two fundamental concepts in human organizations: responsibility and accountability. Specifically, we discuss how a business process specified via accountability and responsibility relationships can be implemented in JaCaMo+, an extension to the well-known JaCaMo agent platform in which social commitments are made available as programming constructs.","BPMN, Accountability; Responsibility; Commitments; JaCaMo",Matteo,,Baldoni,Università degli Studi di Torino,Cristina,,Baroglio,Università desgli Studi di Torino,Roberto,,Micalizio,Università desgli Studi di Torino,Stefano,,Tedeschi,Università desgli Studi di Torino,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de031,For The Record - A Public Goods Game For Exploring Human-Robot Collaboration,,"For The Record is a digital game that involves a social dilemma between a mixed team of humans and agents. Inspired by the standard public goods games, the collective goal is accessible to all team members, independently of their individual contributions. As a result, each player faces in each round the decision between cooperating with the team and defecting to obtain an individual benefit. The digital game itself allows exploring the complexity of human cooperation when teaming with agents. Moreover, playing it on a touch screen creates an additional opportunity to explore these interactions when teaming with social robots.",Social Dilemma; Public Goods Game; Pro-social Computing,Filipa,,Correia,Universidade de Lisboa,Samuel,,Mascarenhas,Universidade de Lisboa,Samuel,,Gomes,Universidade de Lisboa,Silvia,,Tulli,Universidade de Lisboa,Fernando,P.,Santos,Princeton University & ATP-group,Francisco,C.,Santos,Universidade de Lisboa & ATP-group,Rui,,Prada,Universidade de Lisboa,Francisco,S.,Melo,Universidade de Lisboa,Ana,,Paiva,Universidade de Lisboa,,,,,,,,,,,,
de032,An Accessible Toolkit for the Creation of Socio-EmotionalAgents,,"FAtiMA Toolkit is a collection of open-source tools that is designed
to facilitate the creation and use of cognitive agents with socioemotional skills. The toolkit was developed with a focus on accessibility so it could be used by both researchers and game developers.
It provides a computational model of emotions that is based on
the OCC appraisal theory as well as an explicit dialogue structure
that is familiar to game developers while maintaining the flexibility
of an approach based on autonomous agents. Among various use
cases, the toolkit has been successfully applied by an external game
studio in their development of two serious games.",Embodied Agents; Affective Computing; Cognitive Architecture,Manuel,,Guimarães,"INESC-ID & Instituto Superior Técnico, Universidade de Lisboa",Samuel,,Mascarenhas,"INESC-ID & Instituto Superior Técnico, Universidade de Lisboa",Rui,,Prada,"INESC-ID & Instituto Superior Técnico, Universidade de Lisboa",Pedro,A.,Santos,"INESC-ID & Instituto Superior Técnico, Universidade de Lisboa",João,,Dias,"INESC-ID & Instituto Superior Técnico, Universidade de Lisboa",,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0021,Stochastic Variance Reduction for Deep Q-learning,Extended Abstract,"Recent advances in deep reinforcement learning have achieved human-level performance on a variety of real-world applications. However, the current algorithms still suffer from poor gradient estimation with excessive variance, resulting in unstable training and poor sample efficiency. In our paper, we proposed an innovative optimization strategy by utilizing stochastic variance reduced gradient (SVRG) techniques. With extensive experiments on Atari domain, our method outperforms the deep q-learning baselines on 18 out of 20 games.",Deep Q-learning; Stochastic variance reduction; Gradient variance,Wei-ye,,Zhao,"University of California, Berkeley",Jian,,Peng,University of Illinois at Urbana-Champaign,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0026,"Credulous Acceptability, Poison Games and Modal Logic",Extended Abstract,"The Poison Game is a two-player game in which players alternatively move a token on a graph's nodes and such that one player can influence which edges the other player is able to traverse. It operationalizes the notion of existence of credulously acceptable arguments in an argumentation framework or, equivalently, the existence of non-trivial semi-kernels. We develop a modal logic (poison modal logic, PML) tailored to represent winning positions in such a game, thereby identifying the precise modal reasoning that underlies the notion of credulous acceptability in argumentation. We study model-theoretic and decidability properties of PML, and position it with respect to recently studied logics at the cross-road of modal logic, argumentation, and graph games.",Modal logic; Dynamic logic; Memory logic; Poison Game; Games on graphs; Argumentation theory; Credulous acceptability,Davide,,Grossi,University of Groningen,Simon,,Rey,"Sorbonne Université, ENS Paris-Saclay",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0028,Learning Efficient Communication in Cooperative Multi-Agent Environment,Extended Abstract,"Reinforcement learning in cooperate multi-agent scenarios is important for real-world applications. While several attempts before tried to resolve it without explicit communication, we present a communication-filtering actor-critic algorithm that trains decentralized policies which could exchange filtered information in multi-agent settings, using centrally computed critics. Communication could potentially be an effective way for multi-agent cooperation. We supposed that, when in execution phase without central critics, high-quality communication between agents could help agents have better performance in cooperative situations. However, information sharing among all agents or in predefined communication architectures that existing methods adopt can be problematic. Therefore, we use a neural network to filter information between agents. Empirically, we show the strength of our model in two general cooperative settings and vehicle lane changing scenarios. Our approach outperforms several state-of-the-art models solving multi-agent problems.",Collective intelligence; Multiagent learning,Yuhang,,Zhao,Peking University,Xiujun,,Ma,Peking University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0030,Integrating Personality and Mood with Agent Emotions,Extended Abstract,"An intelligent agent should be able to show different emotional behaviours in different interaction situations to become believable and establish close relationships with human counterparts. It is widely accepted that personality and mood play an important role in modulating emotions. However, current computational accounts of emotion for intelligent agents do not effectively integrate the notions of personality and mood in the process of emotion generation. Previous attempts that have been made are mostly based on the assumptions of the researcher, rather than on empirical data and scientific validation. In this paper, we present the results of a novel supervised machine learning approach used to train a network of emotions that integrates the factors of personality and mood, which provides a high emotion intensity prediction accuracy.",Computational emotion model; EEGS; personality; mood; machine learning; emotion intensity prediction,Suman,,Ojha,University of Technology Sydney,Jonathan,,Vitale,University of Technology Sydney,Syed Ali,,Raza,University of Technology Sydney,Richard,,Billingsley,University of Technology Sydney,Mary-Anne,,Williams,University of Technology Sydney,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0042,Local Distance Restricted Bribery in Voting,Extended Abstract,"Studying complexity of various bribery problems has been one of the main research focus in computational social choice. In all the models of bribery studied so far, the briber has to pay every voter some amount of money depending on what the briber wants the voter to report and the briber has some budget at her disposal. Although these models successfully capture many real world applications, in many other scenarios, the voters may be unwilling to deviate too much from their true preferences. In this paper, we study the computational complexity of the problem of finding a preference profile which is as close to the true preference profile as possible and still achieves the briber’s goal subject to budget constraints. We call this problem Local Distance Restricted $Bribery. We consider three important measures of distances, namely, swap distance, footrule distance, and maximum displacement distance, and resolve the complexity of the local distance restricted problem for many common voting rules.",Computational social choice; bribery; algorithms; complexity,Palash,,Dey,Indian Institute of Technology Kharagpur,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0043,Ethically Aligned Multi-agent Coordination to Enhance Social Welfare,Extended Abstract,"In multi-agent systems (MASs), the complex interactions among self-interested agents can be modelled as stochastic games. Existing decision support approaches dealing with such situations focus on minimizing individual agent's regret through outperforming other agents in the competitive aspect of the game. Such an approach often results in social welfare not being maximized in the process. In this paper, we propose the regret-minimization-social-welfare-maximization (RMSM) approach. It contains a novel method to quantify how an agent's sacrifice increases and decreases over time based on queueing system dynamics. In this way, ensuring fairness of distribution of sacrifice among agents and compensating for their previous sacrifices can be translated into maintaining the stability of a queueing system.",Organisations and institutions; Ethically aligned design,Han,,Yu,Nanyang Technological University,Zhiqi,,Shen,Nanyang Technological University,Lizhen,,Cui,Shandong University,Yongqing,,Zheng,Shandong University,Victor,R.,Lesser,University of Massachusetts Amherst,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0046,Social Mobilization to Reposition Indiscriminately Parked Shareable Bikes,,"With rapid growth of shareable bikes comes the problem of indiscriminately parked bikes blocking traffic. We propose a centralized pricing based dynamic incentive mechanism to mobilize the participants via crowdsourcing with regarding to reposition the indiscriminately parked bikes. We formalize the key component of the proposed incentive mechanism into two decision-making model: individual decision-making model Cost-refundable, Multiple Resources Constrained Multiple Armed Bandit (CRMR-MAB) and overall decision-making model multi-dimensional and multiple choice Knapsack problem (MMKP). We proposed a comprehensive decision algorithm GA-WSLS which combines the two. Realistic simulation based on real-world dataset from Singapore demonstrated significant advantages of the proposed approach over 7 existing approaches.",Crowdsourcing; Multi-Armed Bandit; Multi-dimensional Multiple Choice Knapsack Problem,Zelei,,Liu,Jilin University,Han,,Yu,Nanyang Technological University,Leye,,Wang,Hong Kong University of Science and Technology,Liang,,Hu,Jilin University,Qiang,,Yang,Hong Kong University of Science and Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0047,A Regulation Enforcement Solution for Multi-agent Reinforcement Learning,,"Human behaviors are regularized by a variety of norms or regulations, either to maintain orders or to enhance social welfare. However, if artificially intelligent (AI) agents make decisions on behalf of human beings, it is possible that an AI agent can opt to disobey the regulations (being defective) for self-interests.
In this paper, we aim to answer the following question: In a decentralized environment (no centralized authority can control agents), given that not all agents are compliant to regulations at first, can we develop a mechanism such that it is in the self-interest of non-compliant agents to comply after all. We first introduce the problem as Regulation Enforcement and formulate it using reinforcement learning and game theory. Then we propose our solution based on the key idea that although we could not alter how defective agents choose to behave, we can, however, leverage the aggregated power of compliant agents to boycott the defective ones.
We conducted simulated experiments on two scenarios: Replenishing Resource Management Dilemma and Diminishing Reward Shaping Enforcement, using deep multi-agent reinforcement learning algorithms. We further use empirical game-theoretic analysis to show that the method alters the resulting empirical payoff matrices in a way that promotes compliance (making mutual compliant a Nash Equilibrium).",multi-agent reinforcement learning; empirical game-theoretic analysis; reward shaping;,Fan-Yun,,Sun,National Taiwan University,Yen-Yu,,Chang,National Taiwan University,Yueh-Hua,,Wu,National Taiwan University,Shou-De,,Lin,National Taiwan University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0048,Bayes-ToMoP: A Fast Detection and Best Response Algorithm Towards Sophisticated Opponents,,"Multiagent algorithms often aim to accurately predict the behaviors of other agents and find a best response accordingly. Previous works usually assume an opponent uses a stationary strategy or randomly switches among several stationary ones. However, an opponent may exhibit more sophisticated behaviors by adopting more advanced reasoning strategies, e.g., using a Bayesian reasoning strategy. This paper proposes a novel approach called Bayes-ToMoP which can efficiently detect the strategy of opponents using either stationary or higher-level reasoning strategies. Bayes-ToMoP also supports the detection of previously unseen policies and learning a best-response policy accordingly. We also propose a deep version of Bayes-ToMoP by extending Bayes-ToMoP with DRL techniques. Experimental results show both Bayes-ToMoP and deep Bayes-ToMoP outperform the state-of-the-art approaches when faced with different types of opponents in two-agent competitive games.",Multiagent learning; policy reuse; theory of mind,Tianpei,,Yang,Tianjin University,Jianye,,Hao,Tianjin University,Zhaopeng,,Meng,Tianjin University,Yan,,Zheng,Tianjin University,Chongjie,,Zhang,Tsinghua University,Ze,,Zheng,"Beifang Investigation,Design & Research CO.LTD",,,,,,,,,,,,,,,,,,,,,,,,
ea0066,The Multimodal Correction Detection Problem,,"In order for socially aware agents to be truly useful, they should have abilities associated with human intelligence, such as the ability to detect their own mistakes from user reactions. This is an instance of implicit feedback.

In this work we address the problem of detecting an agent's mistakes by identifying when the user tries to correct the agent. We refer to this problem as the Correction Detection task. We use a multimodal approach, using both the voice (acoustics and non-verbal sounds) as well as the transcript of the user's spoken commands.",Human-agent interaction; Correction detection; Implicit feedback; Multimodal deep learning architecture; Socially aware personal assistant,Amos,,Azaria,Ariel University,Keren,,Nivasch,Ariel University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0068,Multi-agent Path Planning with Non-constant Velocity Motion,Extended Abstract,"Multi-agent path planning has wide application in fields such as robotics, transportation, logistics, computer games, etc.. To formulate the multi-agent path finding as a concisely discretized problem, most of the previous works did not construct a detailed motion model of each agent. While many elegant algorithms were proposed in the literature, a method to efficiently plan the paths for multi agents with non-constant velocity is still lacking. In this paper, we propose two methods CRISE and COB to extend the existing algorithms for non-constant velocity motion path planning.",Path Planning; Multiagent System,Ngai Meng,,Kou,Cainiao Smart Logistics Network Limited,Cheng,,Peng,Cainiao Smart Logistics Network Limited,Xiaowei,,Yan,Cainiao Smart Logistics Network Limited,Zhiyuan,,Yang,Cainiao Smart Logistics Network Limited,Heng,,Liu,Cainiao Smart Logistics Network Limited,Kai,,Zhou,Cainiao Smart Logistics Network Limited,Haibing,,Zhao,Cainiao Smart Logistics Network Limited,Lijun,,Zhu,Cainiao Smart Logistics Network Limited,Yinghui,,Xu,Cainiao Smart Logistics Network Limited,,,,,,,,,,,,
ea0069,Installing Resilience in Distributed Constraint Optimization Operated by Physical Multi-Agent Systems,,"We study the notion of k-resilient distribution of graph-structured computations supporting agent decisions, over dynamic and physical multi-agent systems. We devise a replication-based self-organizing distributed repair method, namely DRPM[MGM-2], to repair the distribution as to ensure the system still performs collective decisions and remains resilient to upcoming changes. We focus on a particular type of distributed reasoning process to repair, where computations are decision variables and constraints distributed over a set of agents. We experimentally evaluate the performances of our repair method on different topologies of multi-agent systems (uniform or problem-dependent) operating stateless DCOP algorithms (Max-Sum and A-DSA) to solve classical DCOP benchmarks (random graph, graph coloring, Ising model) while agents are disappearing.",DCOP; Resilience; Adaptation; MGM-2; A-DSA; Max-Sum,Pierre,,Rust,Orange Labs Research,Gauthier,,Picard,MINES Saint-Etienne Laboratoire Hubert Curien UMR CNRS 5516,Fano,,Ramparany,Orange Labs Research,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0071,Engineering Business Processes through Accountability and Agents,,"Business processes often involve multiple interacting parties which, by executing a set of combined and coordinated tasks, jointly realize a business goal. Multiagent systems, and especially multiagent organizations, provide useful tools and abstractions for the design and development of such distributed
structures. However, this goal decomposition together with the agents' autonomy require a mechanism to make clear the responsibilities of each component w.r.t. the global picture. We claim that an explicit representation of accountability and responsibility relationships can increase the robustness of such systems by guiding and systematizing both design and development. We explain the use of such concepts
as engineering concepts, and present two patterns for developing agents by starting from a specification of the system in terms of accountability and responsibility requirements. The patterns are grounded in the notions of accountability fitting and responsibility assumption.",Accountability; Responsibility; Multiagent Organizations; JaCaMo,Matteo,,Baldoni,Università desgli Studi di Torino,Cristina,,Baroglio,Università desgli Studi di Torino,Olivier,,Boissier,MINES Saint-Etienne,Roberto,,Micalizio,Università desgli Studi di Torino,Stefano,,Tedeschi,Università degli Studi di Torino,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0074,"WITHDRAWN (2/22/19, MB) Deep Reinforcement Learning from Policy-Dependent Human Feedback",,,,A,,A,S,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0080,Student-Project-Resource Matching-Allocation Problems: Two-Sided Matching Meets Resource Allocation,,"We consider a student-project-resource matching-allocation problem, in which \emph{students} (resp. \emph{resources}) are matched (resp. allocated) to \emph{projects}. A project's capacity for students is endogenously determined by the resources allocated to it. Traditionally, (1) resources are allocated to projects based on some expectations, and then (2) students are matched with projects based on the capacities determined by (1). Although resource allocation and two-sided matching are well-understood, unless the expectations used in the first problem are correct, we obtain a suboptimal outcome. Thus, it is desirable to solve this problem as a whole without dividing it.

In this paper, we show that finding a nonwasteful matching is $\text{FP}^{\text{NP}}[\text{log}]$-hard, and that deciding whether a stable matching (i.e. nonwasteful and fair) exists is $\text{NP}^{\text{NP}}$-complete. We also show that no strategyproof mechanism can satisfy fairness and very weak efficiency requirements. Then, we develop a new strategyproof mechanism, called Sample and Vote Deferred Acceptance (SVDA), that strikes a good balance between fairness and efficiency.",Matching; Resource Allocation; Computational Complexity,Anisse,,Ismaili,"RIKEN, Center for Advanced Intelligence Project AIP",Kentaro,,Yahiro,Kyushu University,Tomoaki,,Yamaguchi,Kyushu University,Makoto,,Yokoo,Kyushu University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0081,Complexity and Approximations in Robust Coalition Formation via Max-Min <i>k</i>-Partitioning,,"Coalition formation is beneficial to multi-agent systems, especially when the value of a coalition depends on the relationship among its members. However, an attack can significantly damage a coalition structure by disabling agents. Therefore, getting prepared in advance for such an attack is particularly important. We study a robust $k$-coalition formation problem modeled by max-min $k$-partition of a weighted graph.
We show that this problem is $\Sigma_2^P$-complete, which holds even for $k=2$ and arbitrary weights, or $k=3$ and non-negative weights. We also propose the Iterated Best Response (IBR) algorithm which provides a run-time absolute bound for the approximation error and can be generalized to the max-min optimization version of any $\Sigma_2^P$-complete problem. We tested IBR on fairly large instances of both synthetic graphs and real life networks, yielding near optimal results in a reasonable time.",Coalition Formation; <i>k</i>-Partition; Robustness; Complexity,Anisse,,Ismaili,RIKEN AIP Center for Advanced Intelligence Project,Noam,,Hazon,Ariel University,Emi,,Watanabe,Kyushu University,Makoto,,Yokoo,Kyushu University,Sarit,,Kraus,Bar-Ilan University,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0084,Inverse Kinematics and Sensitivity Minimization of an n-Stack Stewart Platform,,"An autonomous system is presented to solve the problem of in space assembly, which can be used to further the NASA goal of deep space exploration. A prototype of an autonomous manipulator called ""Assemblers"" was fabricated from an aggregation of Stewart Platform robots for the purpose of researching autonomous in space assembly capabilities. Selecting inverse kinematic poses, defined by a set of translations and rotations, for the Assembler requires coordination between each Stewart Platform and is an underconstrained non-linear optimization problem. For assembly tasks, it is ideal that the pose selected has the least sensitivity to disturbances possible. A method of sensitivity reduction is proposed by minimizing the Frobenius Norm (FN) of the Jacobian of the forward kinematics. The effectiveness of the FN method will be demonstrated through a Monte Carlo simulation to model random motion internal to the structure.",kinematics; Stewart Platform; in-space assembly; robotics; autonomy,David,,Balaban,University of Massachusetts Amherst,John,,Cooper,NASA Langley Research Center,Erik,,Komendera,Virginia Tech,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0085,Contradict the Machine: A Hybrid Approach to Identifying Unknown Unknowns,Extended Abstract,"Machine predictions that are highly confident yet incorrect, i.e. unknown unknowns, are crucial errors to identify, especially in high-stakes settings like medicine or law. We describe a hybrid approach to identifying unknown unknowns that combines the previous algorithmic and crowdsourcing strategies. Our method uses a set of decision rules to approximate how the model makes high confidence predictions. We present the rules to crowd workers, and challenge them to generate instances that contradict the rules. To select the most promising rule to next present to workers, we use a multi-armed bandit algorithm. We evaluate our method by conducting a user study on Amazon Mechanical Turk. Experimental results on three datasets indicate that our approach discovers unknown unknowns more efficiently than state-of-the-art baselines.",unknown unknowns; crowdsourcing; multi-armed bandits,Colin,,Vandenhof,University of Waterloo,Edith,,Law,University of Waterloo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0089,Withdrawn (2/14/19) Investigating Curiosity for Multi-Prediction Learning,,,,Thomas,,Degris,X,Martha,,White,University of Alberta,Nadia,,Ady,University of Alberta,Cameron,,Linke,University of Alberta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0090,An Urgency-Dependent Quorum Sensing Algorithm for N-Site Selection in Autonomous Swarms,,"The ability for individuals and groups to trade off between decision time and decision accuracy when making decisions is commonly found in nature and essential for flexible decision-making responses. There has been little literature that models this ability in autonomous swarms, in which a large number of agents must come to a group consensus without a centralized controller. This paper successfully produces the first urgency-dependent model for discrete site selection by an autonomous swarm. It builds off of quorum sensing techniques found in natural swarms of ants and cockroaches as well as existing discrete site selection models for swarms to improve on previous work by adding the capability for agents to make a time-accuracy trade-off in decision making. The developed model will allow for future autonomous swarms to dynamically and effectively respond to a range of threats such as inclement weather and military attack.",Emergent behaviour; Analysis of agent-based simulations; Simulation of complex systems,Grace,,Cai,Montgomery Blair High School,Don,,Sofge,US Naval Research Laboratory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0092,General-Sum Cyber Deception Games under Partial Attacker Valuation Information,Extended Abstract,"The rapid increase in cybercrime, causing a reported annual economic loss of \$600 billion \cite{cybergdploss}, has prompted a critical need for effective cyber defense. Strategic criminals conduct network reconnaissance prior to executing attacks to avoid detection and establish situational awareness via scanning and fingerprinting tools.
Cyber deception attempts to foil these reconnaissance efforts; by disguising network and system attributes, among several other techniques.
Cyber Deception Games (CDG) is a game-theoretic model for optimizing strategic deception, and can apply to various deception methods.
Recently introduced initial model for CDGs assumes zero-sum payoffs, implying directly conflicting attacker motives, and perfect defender knowledge on attacker preferences.
These unrealistic assumptions are fundamental limitations of the initial zero-sum model, which we address by proposing a general-sum model that can also handle uncertainty in the defender's knowledge.",Game Theory; Cyber Deception; Cyber Security; Uncertainty,Omkar,,Thakoor,University of Southern California,Milind,,Tambe,University of Southern California,Phebe,,Vayanos,University of Southern California,Haifeng,,Xu,Harvard University,Christopher,,Kiekintveld,The University of Texas at El Paso,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0098,The Representational Capacity of Action-Value Networks for Multi-Agent Reinforcement Learning,,"Recent years have seen the application of deep reinforcement learning techniques to cooperative multi-agent systems, with great empirical success. In this work, we empirically investigate the representational power of various network architectures on a series of one-shot games. Despite their simplicity, these games capture many of the crucial problems that arise in the multi-agent setting, such as an exponential number of joint actions or the lack of an explicit coordination mechanism. Our results quantify how well various approaches can represent the requisite value functions, and help us identify issues that can impede good performance.",multi-agent systems; neural networks; decision-making; action-value representation; one-shot games,Jacopo,,Castellini,University of Liverpool,Frans,A.,Oliehoek,Delft University of Technology,Rahul,,Savani,University of Liverpool,Shimon,,Whiteson,University of Oxford,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0099,"WITHDRAWN Again (3/12/19, LT) A Constrained Optimization Solution to the Fair Load Shedding Problem in Developing Countries",,,,Olabambo,,Oluwasuji,University of Southampton,Malik,,Obaid,University of Southampton,Jie,,Zhang,University of Southampton,Sarvapali,,Ramchurn,University of Southampton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0103,WITHDRAWN - 2/22 -- Who Are We Playing Against? Follower Deception in Stackelberg Games,,,,Jiarui,,Gan,University of Oxford,Haifeng,,Xu,Harvard University,Qingyu,,Guo,Nanyang Technological University,Long,,Tran-Thanh,University of Southampton,Zinovi,,Rabinovich,Nanyang Technological University,Michael,,Wooldridge,University of Oxford,,,,,,,,,,,,,,,,,,,,,,,,
ea0104,Simple Contrapositive Assumption-Based Frameworks,,"We study the Dung semantics for extended forms of assumption-based argumentation frameworks (ABFs), based on any contrapositive propositional logic, and whose defeasible rules are expressed by arbitrary formulas in that logic. New results on the well-founded semantics for such ABFs are reported, the redundancy of the closure condition is shown, and the use of disjunctive attacks is investigated. Useful properties of the generalized frameworks are also considered.",ABA frameworks; Dung semantics; Defeasible reasoning; Assumption-Based Argumentation; Argumentation; Non-Monotonic Logic,Jesse,,Heyninck,Ruhr University Bochum,Ofer,,Arieli,The Academic College of Tel-Aviv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0105,Optimising Worlds to Evaluate and Influence Reinforcement Learning Agents,,"Training reinforcement learning agents on a distribution of procedurally generated environments has become an increasingly common method for obtaining more generalisable agents. However, this makes evaluation challenging, as the space of possible environment settings is large; simply looking at the average performance is insufficient for understanding how well - or how poorly - the agents perform. To address this, we introduce a method for strategically evaluating and influencing the behaviour of reinforcement learning agents. Using deep generative modelling to encode the environment, we propose a World Agent which efficiently generates and optimises worlds (i.e. environment settings) relative to the performance of the agents. Through the use of our method on two distinct environments, we demonstrate the existence of worlds which minimise and maximise agent reward beyond the typically reported average reward. Additionally, we show how our method can also be used to modify the distribution of worlds that agents train on, influencing their emergent behaviour to be more desirable.",Reinforcement Learning; Agent Simulation; Evaluation; Training; Procedurally Generated Environments,Richard,,Everett,University of Oxford,Adam,,Cobb,University of Oxford,Andrew,,Markham,University of Oxford,Stephen,,Roberts,University of Oxford,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0111,"WITHDRAWN (2/25, MB) -- Checking Multi-Agent Systems against Temporal-Epistemic Specifications",,,,A,,A,S,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0112,Broken Signals in Security Games: Coordinating Patrollers and Sensors in the Real World,,"Mobile sensors, e.g., unmanned aerial vehicles (UAVs), are becoming increasingly important in security domains and can be used for tasks such as searching for poachers in conservation areas. Such mobile sensors augment human patrollers by assisting in surveillance and in signaling potentially deceptive information to adversaries, and their coordinated deployment could be modeled via the well-known security games framework. Unfortunately, real-world uncertainty in the sensor's detection of adversaries and adversaries' observation of the sensor's signals present major challenges in the sensors' use. This leads to significant detriments in security performance. We first discuss the current shortcomings in more detail, and then propose a novel game model that incorporates uncertainty with sensors. The defender strategy in this game model will consist of three interdependent stages: an allocation stage, a signaling stage, and a reaction stage.",security games; computational sustainability; uncertainty; sensors,Elizabeth,,Bondi,University of Southern California,Hoon,,Oh,Carnegie Mellon University,Haifeng,,Xu,Harvard University,Fei,,Fang,Carnegie Mellon University,Bistra,,Dilkina,University of Southern California,Milind,,Tambe,University of Southern California,,,,,,,,,,,,,,,,,,,,,,,,
ea0128,Probabilistic Resource-bounded Alternating-time Temporal Logic,,"This paper extends resource-bounded alternating-time temporal logic (RB-ATL) with probabilistic reasoning and provides the syntax and semantics of the resulting logic, Probabilistic Resource-bounded ATL (pRB-ATL).",Logics for resource-bounded agents; Probabilistic reasoning.,Hoang Nga,,Nguyen,Coventry University,Abdur,,Rakib,University of the West of England,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0132,Fair Division of Indivisible Goods Among Strategic Agents,,"We study fair division of indivisible goods among strategic agents in a single-parameter environment. This work specifically considers fairness in terms of \emph{envy freeness up to one good} ($\EFone$) and \emph{maximin share guarantee} ($\MMS$).
We show that (in a single-parameter environment) the problem of maximizing welfare, subject to the constraint that the allocation of the indivisible goods is $\EFone$, admits a polynomial-time, $1/2$-approximate, truthful auction.
Under $\textsc{MMS}$ setup, we develop a truthful auction which efficiently finds an allocation wherein each agent gets a bundle of value at least $\left(1/2 - \varepsilon \right)$ times her maximin share and the welfare of the computed allocation is at least the optimal, here $\varepsilon >0$ is a fixed constant.
Our results for $\EFone$ and $\MMS$ are based on establishing interesting {majorization} inequalities.",Fair Division; Social Welfare; Approximation Algorithms; Auctions,Siddharth,,Barman,Indian Institute of Science,Ganesh,,Ghalme,Indian Institute of Science,Shweta,,Jain,"Indian Institute of Technology, Bhubhaneswar",Pooja,,Kulkarni,"University of Illinois, Urbana-Champaign",Shivika,,Narang,Indian Institute of Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0138,A Polynomial-time Fragment of Epistemic Probabilistic Argumentation,,"Probabilistic argumentation allows reasoning about argumentation problems in a way that is well-founded
by probability theory. However, in practice, this approach can be severely limited by the fact that probabilities are computed by adding an exponential number of terms.
We show that this exponential blowup can be avoided in an interesting fragment
of epistemic probabilistic argumentation and that
some computational problems that have been considered intractable
can be solved in polynomial time.",Probabilistic Argumentation; Complexity,Nico,,Potyka,University of Osnabrueck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0142,Bayesian-DPOP for Continuous Distributed Constraint Optimization Problems,,"In this work, the novel algorithm Bayesian Dynamic Programming Optimization Procedure (B-DPOP) is presented to solve multi-agent problems within the Distributed Constraint Optimization Problem framework.
The Bayesian optimization framework is used to prove convergence to the global optimum of the B-DPOP algorithm for Lipschitz-continuous objective functions.
The proposed algorithm is assessed based on the benchmark problem known as dynamic sensor placement.
Results show increased performance over related algorithms in terms of sample-efficiency.",DCOP; DPOP; Bayesian Optimization; Distributed Optimization,Jeroen,,Fransman,Delft University of Technology,Joris,,Sijs,Netherlands Organisation for Applied Scientific Research (TNO),Henry,,Dol,Netherlands Organisation for Applied Scientific Research (TNO),Erik,,Theunissen,Netherlands Defense Academy (NLDA),Bart,,De Schutter,Delft University of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0154,Polynomial-Time Multi-Agent Pathfinding with Heterogeneous and Self-Interested Agents,,"This paper proposes a polynomial-time strategyproof mechanism that solves multi-agent pathfinding (MAPF) problems with heterogeneous and self-interested agents. In MAPF, agents need to reach their goal destinations while avoiding collisions between them. In this paper, we consider heterogeneous and self-interested MAPF. Agents are heterogeneous if the costs of traversing a given path differ between agents. In particular, we assume each agent has a private linear cost function of travel time. The proposed strategyproof mechanisms aim to make agents truthfully declare the slope of the private linear cost function.",agent coordination; multi-agent pathfinding; path planning; mechanism design; self-interested agents,Manao,,Machida,NEC Corporation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0158,Strategyproof Facility Location for Three Agents on a Circle,Extended Abstract,"The paper presents two randomized facility location mechanisms for 3 agents on a circle, that are strategyproof and are strictly better than selecting a random dictator.  First lower bounds for the problem are also presented.",Facility location; Mechanism design; Randomized mechanisms,Reshef,,Meir,Technion -- Israel Institute if Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0162,Distributed Policy Iteration for Scalable Approximation of Cooperative Multi-Agent Policies,Extended Abstract,"We propose Strong Emergent Policy (STEP) approximation, a scalable approach to learn strong decentralized policies for cooperative MAS with a distributed variant of policy iteration. For that, we use function approximation to learn from action recommendations of a decentralized multi-agent planning algorithm. STEP combines decentralized multi-agent planning with centralized learning, only requiring a generative model for distributed black box optimization. We experimentally evaluate STEP in two challenging and stochastic domains with large state and joint action spaces and show that STEP is able to learn stronger policies than standard multi-agent reinforcement learning algorithms, when combining multi-agent open-loop planning with centralized function approximation. The learned policies can be reintegrated into the multi-agent planning process to further improve performance.",multi-agent planning; multi-agent learning; policy iteration,Thomy,,Phan,Ludwig-Maximilians-University,Kyrill,,Schmid,Ludwig-Maximilians-University,Lenz,,Belzner,MaibornWolff,Thomas,,Gabor,Ludwig-Maximilians-University,Sebastian,,Feld,Ludwig-Maximilians-University,Claudia,,Linnhoff-Popien,Ludwig-Maximilians-University,,,,,,,,,,,,,,,,,,,,,,,,
ea0163,A Reinforcement Learning Framework for Container Selection and Ship Load Sequencing in Ports,,"We describe a reinforcement learning (RL) framework for selecting and sequencing containers to load onto ships in ports. The goal is to minimize an approximation of the number of crane movements require to load a given ship, known as the shuffle count. It can be viewed as a version of the assignment problem in which the sequence of assignment is of importance and the task rewards are order dependent. The proposed methodology is developed specifically to be usable on ship and yard layouts of arbitrary scale, by dividing the full problem into fixed future horizon segments and through a redefinition of the action space into a binary choice framework. Using data from real-world yard and ship layouts, we show that our approach solves the single crane version of the loading problem for entire ships with better objective values than those computed using standard metaheuristics.",Reinforcement learning; Single agent planning & scheduling,Richa,,Verma,TCS Research,Sarmimala,,Saikia,TCS Research,Harshad,,Khadilkar,TCS Research,Gautam,,Shroff,TCS Research,Puneet,,Agarwal,TCS Research,Ashwin,,Srinivasan,Birla Institute of Technology and Science,,,,,,,,,,,,,,,,,,,,,,,,
ea0166,Learning Factored Markov Decision Processes with Unawareness,,"Methods for learning and planning in sequential decision problems often assume the learner is fully aware of all possible states and actions in advance. This assumption is sometimes untenable: evidence gathered via domain exploration or external advice may reveal not just information about which of the currently known states are probable, but that entirely new states or actions are possible. This paper provides a model-based method for learning factored markov decision problems from both domain exploration and contextually relevant expert corrections in a way which guarantees convergence to near-optimal behaviour, even when the agent is initially unaware of actions or belief variables that are critical to achieving success. Our experiments show that our agent converges quickly on the optimal policy for both large and small decision problems. We also explore how an expert's tolerance towards the agent's mistakes affects the agent's ability to achieve optimal behaviour.",Reasoning in agent-based systems; Reward structures for learning; Reinforcement Learning,Craig,,Innes,The University of Edinburgh,Alex,,Lascarides,University of Edinburgh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0178,On the Importance of Representations for Speech-Driven Gesture Generation,,"This paper presents a novel framework for automatic speech-driven gesture generation applicable to human-agent interaction, including both virtual agents and robots. Specifically, we extend recent deep-learning-based, data-driven methods for speech-driven gesture generation by incorporating representation learning.
Our model takes speech features as input and produces gestures in the form of sequences of 3D joint coordinates representing motion as output. The results of objective and subjective evaluations confirm the benefits of the representation learning.",Gesture generation; social robotics; representation learning; neural network; deep learning; virtual agents,Taras,,Kucherenko,KTH Royal Institute of Technology,Dai,,Hasegawa,Hokkai Gakuen University,Naoshi,,Kaneko,Aoyama Gakuin University,Gustav Eje,,Henter,KTH Royal Institute of Technology,Hedvig,,Kjellström,KTH Royal Institute of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0185,Reachability and Coverage Planning for Connected Agents,Extended Abstract,"Unmanned autonomous vehicle assisted information gathering missions have quickly picked up interest. Indeed, the advances on drones are making this type of missions possible. Thus, we study multi-agent path planning problems, namely reachability and coverage, for such missions with a connectivity constraint. This version of the multi-agent path planning asks to generate a plan, a sequence of steps, for a group of agents that are to stay connected during the missions while satisfying the specified goal.

In this paper, we study the complexity of the coverage and reachability problems for a cooperation of agents with a connectivity constraint which restrain their movement. We identify a class of topological graphs which allows one to reduce the complexity of the decision problems from PSPACE-complete to LOGSPACE. We show, on the other hand, that the bounded versions of the previous problems are NP-complete.","Multi-agent planning; Complexity theory and logic; Paths and connectivity
problems",Tristan,,Charrier,"Univ Rennes, CNRS, IRISA",Arthur,,Queffelec,"Univ Rennes, CNRS, IRISA",Ocan,,Sankur,"Univ Rennes, Inria, CNRS, IRISA",François,,Schwarzentruber,"Univ Rennes, CNRS, IRISA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0191,The Complexity of the Possible Winner Problem with Partitioned Preferences,Extended Abstract,"The Possible-Winner problem asks, given an election where the voters' preferences over the set of candidates is partially specified, whether a distinguished candidate can become a winner.
In this work, we consider the computational complexity of Possible-Winner under the assumption that the voter preferences are partitioned. That is, we assume that every voter provides a complete order over sets of incomparable candidates (e.g., candidates are ranked by their level of education). We consider elections with partitioned profiles over positional scoring rules.
Our first result is a polynomial time algorithm for voting rules with two distinct values, which include the common k-approval voting rule.
We then go on to prove NP-hardness for
the class of voting rules that produce scoring vectors with at least four distinct values, and a large class of voting rules that produce scoring vectors with three distinct values.",Possible-Winner; Preferences; Complexity analysis,Batya,,Kenig,University of Washington,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0201,"WITHDRAWN (2/25,MB) -- A Framework for Engineering Human/Agent Teaming Systems",,,,A,,A,S,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0202,Explicable Planning as Minimizing Distance from Expected Behavior,Extended Abstract,"In order to achieve effective human-AI collaboration, it is necessary for an AI agent to align its behavior with the human's expectations. When the agent generates a task plan without such considerations, it may often result in inexplicable behavior from the human's point of view. This may have serious implications for the human, from increased cognitive load to more serious concerns of safety around the physical agent. In this work, we present an approach to generate explicable behavior by minimizing the distance between the agent's plan and the plan expected by the human. To this end, we learn a mapping between plan distances (distances between expected and agent plans) and human's plan scoring scheme. The plan generation process uses this learned model as a heuristic. We demonstrate the effectiveness of our approach in a delivery robot domain.",Explicable planning; human's mental model of the robot; expected plans; plan distances,Anagha,,Kulkarni,Arizona State University,Yantian,,Zha,Arizona State University,Tathagata,,Chakraborti,IBM Research AI,Satya Gautam,,Vadlamudi,CreditVidya,Yu,,Zhang,Arizona State University,Subbarao,,Kambhampati,Arizona State University,,,,,,,,,,,,,,,,,,,,,,,,
ea0204,Avoiding Social Disappointment in Elections,,"Mechanism design is concerned with settings where a policy maker (or social
planner) faces the problem of aggregating the announced preferences of multiple agents into
a collective (or social), system-wide decision. One of the most important ways for aggregating
preference that has been used in multi-agent systems is election. In an election, the aim is to select the
candidate who reflects the common will of society. Despite the importance of this
subject, in real-world situations, under special circumstances, the result of the
election does not respect the purpose of those who execute it and the election leads
to dissatisfaction of a large amount of people and in some cases causes polarization in societies. To analyze these situations, we introduce new notion called social disappointment and we show which voting rules can prevent it in elections. In addition, we propose new protocols to prevent social disappointment in elections. A version of the impossibility theorem is proved
regarding social disappointment in elections, showing that there is no voting rule for four or more candidates that simultaneously satisfies avoiding social disappointment and Condorcet winner criteria.
We empirically compared our protocols with seven well-known voting protocols and we observed that our protocols are capable of preventing social disappointment and are more robust against manipulations.",mechanism design; social choice theory; voting procedures; impossibility theorem; social disappointment; manipulation,Mohammad Ali,,Javidian,University of South Carolina,Pooyan,,Jamshidi,University of South Carolina,Rasoul,,Ramezanian,Ferdowsi University Of Mashhad,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0208,Multinomial HMMs for Intent Recognition in Maritime Domains,Extended Abstract,"The need for increased maritime security has prompted research focus on intent recognition solutions for the naval domain. We consider the problem of early classification of the hostile behavior of agents in a dynamic maritime domain and propose a solution using multinomial Hidden Markov Models (HMMs). To enable early detection of hostile behaviors, the proposed approach encodes as observable symbols the rate of change (instead of static values) for parameters relevant to the task. We discuss our implementation of a one-versus-all intent classifier using multinomial HMMs and present the results of our system on three types of hostile behaviors (ram, herd, block) and a benign behavior.",Intent Recognition; Maritime; Multinomial HMM,Logan,,Carlson,"University of Nevada, Reno",Dalton,,Navalta,"University of Nevada, Reno",Monica,,Nicolescu,"University of Nevada, Reno",Mircea,,Nicolescu,"University of Nevada, Reno",Gail,,Woodward,NASA Jet Propulsion Laboratory,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0210,A Q-values Sharing Framework for Multiple Independent Q-learners,,"By using a multiagent reinforcement learning (MARL) framework, cooperative agents can communicate with one another to accelerate the joint learning. In the \emph{teacher-student} paradigm applied in MARL, a more experienced agent (advisor) can advise another agent (advisee) which action to take in a state. However, when agents need to cooperate with one another, the advisee may fail to cooperate well with others since their policies may have changed. It requires a long period for an advisee to learn the same best actions as an advisor has learned, especially when the amount of advice is limited. We propose a \emph{partaker-sharer} advising framework (PSAF) for independent Q-learners with limited communication in cooperative MARL. In PSAF, the overall learning process is shown to accelerate by multiple independent Q-learners' sharing their maximum Q-values with one another at every time step. We perform experiments in the Predator-Prey domain and HFO game. The results show that our approach significantly outperforms existing advising methods.",multiagent learning; Q-learning; reinforcement learning,Changxi,,Zhu,South China University of Technology,Ho-fung,,Leung,The Chinese University of Hong Kong,Shuyue,,Hu,The Chinese University of Hong Kong,Yi,,Cai,South China University of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0214,Multiagent Adversarial Inverse Reinforcement Learning,,"Learning to coordinate is a hard task for reinforcement learning due to a game-theoretic pathology known as relative overgeneralization. To help deal with this, we propose two methods which apply forms of imitation learning to the problem of learning coordinated behaviors. The proposed methods have a close connection to multiagent actor-critic models, and will avoid relative overgeneralization if the right demonstrations are given. We compare our algorithms with MADDPG, a state-of-the-art approach, and show that our methods achieve better coordination in multiagent cooperative tasks.",Multiagent; Adversarial Learning; Deep Reinforcement Learning,Ermo,,Wei,George Mason University,Drew,,Wicke,George Mason University,Sean,,Luke,George Mason University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0218,Landmark Based Reward Shaping in Reinforcement Learning with Hidden States,,"While most of the work on reward shaping focuses on fully observable problems, there are very few studies that couple reward shaping with partial observability. Moreover, for problems with hidden states, where there is no prior information about the underlying states, reward shaping opportunities are unexplored. In this paper, we show that landmarks can be used to shape the rewards in reinforcement learning with hidden states. Proposed approach is empirically shown to improve the  learning performance in terms of speed and quality.",reward shaping; landmarks; reinforcement learning; hidden states,Alper,,Demir,Middle East Technical University,Erkin,,Çilden,STM RF and Simulation Systems Directorate,Faruk,,Polat,Middle East Technical University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0220,Personality-Based Representations of Imperfect-Recall Games,,"Games with imperfect recall are a powerful model of strategic interactions that allows agents to forget less important details of the past. Nevertheless, the computational treatment of imperfect-recall games is largely unexplored so far, and no efficient strategy representation for this setting is known.
In this paper, we focus on general imperfect-recall games without absentmindedness, and we study how to produce a perfect-recall representation of these games using personalities. In particular, a valid personality assignment is a decomposition of an imperfect-recall player such that she does not exhibit memory losses within the same personality. Given a valid personality assignment, we can build an auxiliary team game where a team of perfect-recall players---sharing the same objectives---replaces a player with imperfect recall.
Our primary goal is the construction of a compact representation in terms of number of personalities. We study the (iterated) inflation operation as a way to simplify the information structure of a game with imperfect recall.
We show that the complete (i.e., maximal) inflation of a game can be found in polynomial time. We also show that finding the valid personality assignment minimizing the number of personalities is NP-hard, and also hard to approximate, unless P=NP, even in a completely inflated game.",Equilibrium computation; imperfect-recall games,Andrea,,Celli,Politecnico di Milano,Giulia,,Romano,Politecnico di Milano,Nicola,,Gatti,Politecnico di Milano,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0222,Generating Voting Rules from Random Relations,,"We consider a way of generating voting rules based on a random relation, the winners being alternatives that have the highest probability of being supported. We consider different notions of support, such as whether an alternative dominates the other alternatives, or whether an alternative is undominated, and we consider structural assumptions on the form of the random relation, such as being acyclic, asymmetric, connex or transitive. We give sufficient conditions on the supporting function for the associated voting rule to satisfy various properties such as Pareto and monotonicity. The random generation scheme involves a parameter p between zero and one. Further voting rules are obtained by tending p to zero, and by tending p to one, and these limiting rules satisfy a homogeneity property, and, in certain cases, Condorcet consistency. We define a language of supporting functions based on eight natural properties, and  categorise the different rules that can be generated for the limiting p cases.",Voting rules; random relations; limiting probabilities,Nic,,Wilson,University College Cork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0223,Multi-Agent Hierarchical Reinforcement Learning with Dynamic Termination,,"In a multi-agent system, an agent's optimal policy will typically depend on the policies of other agents. Predicting the behaviours of others, and responding promptly to changes in such behaviours, is therefore a key issue in multi-agent systems research. One obvious possibility is for each agent to broadcast their current intention, for example, the currently executed option in a hierarchical RL framework. However, this approach results in inflexible agents when options have an extended duration. While adjusting the executed option at each step improves flexibility from a single-agent perspective, frequent changes in options can induce inconsistency between an agent's actual behaviour and its broadcasted intention. In order to balance flexibility and predictability, we propose a dynamic termination Bellman equation that allows the agents to flexibly terminate their options.",Multi-agent Reinforcement Learning; Hierarchical Reinforcement Learning,Dongge,,Han,University of Oxford,Wendelin,,Boehmer,University of Oxford,Michael,,Wooldridge,University of Oxford,Alex,,Rogers,University of Oxford,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0225,Optimal Trip-Vehicle Dispatch with Multi-Type Requests,,"In recent years, traditional transportation platforms which mainly provide real-time ride-hailing services have started to accept rides scheduled in advance. The presence of both ride requests posted in real time and scheduled rides leads to new challenges to the service providers in deciding which requests to accept and how to dispatch the vehicles in a dynamic and optimal way, which, to the best of our knowledge, have not been addressed by existing works. To fill the gap, we provide the following contributions: (i) a novel two-stage decision-making model where in the first stage, the system decides whether to accept requests scheduled in advance in an online fashion, and in the second stage, dispatches vehicles to on-demand ride requests in real time given the accepted scheduled requests as constraints; (ii) novel algorithms for both stages that take an estimated distribution of on-demand ride requests into account to handle both on-demand and scheduled requests.",Trip-vehicle dispatch; Ride-hailing; On-demand request; Scheduled request,Taoan,,Huang,Tsinghua University,Bohui,,Fang,Shanghai Jiao Tong University,Hoon,,Oh,Carnegie Mellon University,Xiaohui,,Bei,Nanyang Technological University,Fei,,Fang,Carnegie Mellon University,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0232,Cooperating in Long-term Relationships with Time-Varying Structure,,"Extended interactions between agents have commonly been studied in the context of repeated games (RGs), in which the same players repeatedly interact in the same scenario. However, such interactions are uncommon in practice. Typically, the players' goals, action sets, and payoffs change from encounter to encounter, often in ways the players cannot easily model or control. These more realistic interactions, which we model as a form of stochastic game called interaction games (IGs), have attributes which prohibit the straightforward application of many often-used algorithms developed for RGs. In this paper, we generalize several algorithms previously designed for RGs, and explore their behavior and performance in IGs. Our results suggest that at least some of the methodologies designed for RGs can, with some modifications, be extended to IGs.
",Repeated interactions; learning in games; trigger strategies,Jacob,W.,Crandall,Brigham Young University,Huy,,Pham,Brigham Young University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0234,"WITHDRAWN (3/14, MB) Recognizing Plans by Learning Embeddings from Observed Action Distributions",,,,S,,S,S,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0242,Regular Decision Processes: Modelling Dynamic Systems without Using Hidden Variables,Extended Abstract,"We describe Regular Decision Processes (RDPs) a
model in between MDPs and POMDPs. Like in POMDPs, the effect of
an action may depend on the entire history of actions and observations,
but this dependence is restricted to regular functions only. This makes RDP
a tractable, yet rich model, that does not hypothesize hidden state,
and could possibly be useful for learning dynamic systems.",Planning under Uncertainty; MDPs; POMDPs; Latent State,Ronen,I.,Brafman,Ben Gurion University of the Negev,Giuseppe,,De Giacomo,Sapienza Università di Roma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0245,On Enactability of Agent Interaction Protocols: Towards a Unified Approach,Extended Abstract,"Interactions between agents are usually designed from a global viewpoint. However, the implementation of a multi-agent interaction is distributed. This difference can introduce problems. For instance, it is possible to specify protocols from a global viewpoint that cannot be implemented as a collection of individual agents. This leads naturally to the question of whether a given (global) protocol is enactable. We consider this question in a powerful setting (trace expressions), considering a range of message ordering interpretations (specifying what it means to say that an interaction step occurs before another), and a range of possible constraints on the semantics of message delivery, corresponding to different properties of the underlying communication middleware.",Agent Interaction Protocols; Enactability; Enforceability; Implementability; Realizability; Projectability; Trace Expressions,Angelo,,Ferrando,University of Liverpool,Michael,,Winikoff,University of Otago,Stephen,,Cranefield,University of Otago,Frank,,Dignum,Umeå University,Viviana,,Mascardi,University of Genova,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0246,MARL-PPS: Multi-agent Reinforcement Learning with Periodic Parameter Sharing,Extended Abstract,"We present a multi-agent reinforcement learning algorithm that is a simple, yet effective modification of a known algorithm. External agents are modeled as a time-varying environment, whose policy parameters are updated periodically at a slower rate than the planner to make learning stable and more efficient. Replay buffer, which is used to store the experiences, is also reset with the same large period to draw samples from a fixed environment. This enables us to address challenging cooperative control problems in highway navigation. The resulting Multi-agent Reinforcement Learning with Periodic Parameter Sharing (MARL-PPS) algorithm outperforms the baselines in multi-agent highway scenarios we tested.",Deep Reinforcement Learning; Multi-agent Learning,Safa,,Cicek,"University of California, Los Angeles",Alireza,,Nakhaei,Honda Research Institute,Stefano,,Soatto,"University of California, Los Angeles",Kikuo,,Fujimura,Honda Research Institute,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0250,A New Constraint Satisfaction Perspective on Multi-Agent Path Finding: Preliminary Results,,"In this paper, we adopt a new perspective on the Multi-Agent Path Finding (MAPF) problem and view it as a Constraint Satisfaction Problem (CSP). A variable corresponds to an agent, its domain is the set of all paths from the start vertex to the goal vertex of the agent, and the constraints allow only conflict-free paths for each pair of agents. Although the domains and constraints are only implicitly defined, this new CSP perspective allows us to use successful techniques from CSP search. With the concomitant idea of using matrix computations for calculating the size of the reduced domain of an uninstantiated variable, we apply Dynamic Variable Ordering and Rapid Random Restarts to the MAPF problem. In our experiments, the resulting simple polynomial-time MAPF solver, called Matrix MAPF solver, either outperforms or matches the performance of many state-of-the-art solvers for the MAPF problem and its variants.",Multi-Agent Path Finding; Constraint Satisfaction; Matrix Computations; Randomization,Jiangxing,,Wang,University of Southern California,Jiaoyang,,Li,University of Southern California,Hang,,Ma,University of Southern California,Sven,,Koenig,University of Southern California,T. K. Satish,,Kumar,University of Southern California,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0260,"WITHDRAWN (MB, 3/7) Empathic Listener Framework for Embodied Conversational Agents",,,,A,,A,S,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0271,Entailment Functions and Reasoning Under Inconsistency,,"This study proposes an intuitive and flexible framework for defining a large variety of paraconsistent entailment relations.
We first introduce a notion named entailment function (EF) that is used for associating a value called entailment degree to every pair of belief base and formula. Then, we introduce our EF-based framework for defining paraconsistent entailment relations. Finally, we discuss a connection between the notion of entailment function and that of inconsistency measure.",araconsistency; Entailment; Reasoning under inconsistency; Inconsistency Measures,Yakoub,,Salhi,"CRIL, U. Artois & CNRS",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0272,Vote For Me! Election Control via Social Influence in Arbitrary Scoring Rule Voting Systems,,"Online social networks are used to diffuse opinions and ideas among users, enabling a faster communication and a wider audience.
The way in which opinions are conditioned by social interactions is usually called social influence.
Social influence is extensively used during political campaigns to advertise and support candidates.

We consider the problem of exploiting social influence in a network of voters to change their opinion about a target candidate with the aim of increasing his chance to win or lose the election in a wide range of voting systems.

We introduce the Linear Threshold Ranking, a natural and powerful extension of the well-established Linear Threshold Model, which describes the change of opinions taking into account the amount of exercised influence.
We are able to maximize the score of a target candidate up to a factor of 1-1/e by showing submodularity.
We exploit such property to provide a 1/3(1-1/e)-approximation algorithm for the constructive election control problem and a 1/2(1-1/e)-approximation algorithm for the destructive control problem.
The algorithm can be used in arbitrary scoring rule voting systems, including plurality rule and borda count.",Election Control; Influence Maximization; Social Networks; Voting,Federico,,Corò,Gran Sasso Science Institute,Emilio,,Cruciani,Gran Sasso Science Institute,Gianlorenzo,,D'Angelo,Gran Sasso Science Institute,Stefano,,Ponziani,Gran Sasso Science Institute,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0273,Dynamic and Intelligent Control of Autonomous Vehicles for Highway On-ramp Merge,,"This work addresses the problem of predicting the intentions of drivers in a highway on-ramp situation using a dynamic Bayesian network. We present the proposed model and detail its use. Then, we report the simulation results that show good performances for predicting highway on-ramp merging intentions.",Connected car; Autonomous car; Collaborative driving; Dynamic Bayesian Network,Zine el abidine,,Kherroubi,Groupe Renault,Samir,,Aknine,University Claude Bernard Lyon 1 university,Rebiha,,Bacha,Groupe Renault,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0284,Coordinated Multiagent Reinforcement Learning for Teams of Mobile Sensing Robots,,"A mobile sensing robot team (MSRT) is a typical application of multi-agent systems. This paper investigates multiagent reinforcement learning in the MSRT problem. A naive coordinated learning approach is first proposed that uses a coordination graph to model interaction relationships among robots. To further reduce the computation complexity in the context of continuously changing topology caused by robots' movement, we then propose an on-line transfer learning method that is capable of transferring the past interaction experience and learned knowledge to a new context in a dynamic environment. Simulations verify that the method can achieve reasonable team performance by properly balancing robots' local selfish interests and global team performance.
",Mobile Sensing Robot Team; Coordination; Reinforcement Learning; Transfer Learning; Coordination Graph,Chao,,Yu,Chao Yu,Xin,,Wang,Dalian University of Technology,Zhanbo,,Feng,Dalian University of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0285,Towards Decentralized Reinforcement Learning Architectures for Social Dilemmas,,"Multi-agent reinforcement learning has received significant interest in recent years notably due to the advancements made in deep reinforcement learning which have allowed for the developments of new architectures and learning algorithms. In this extended abstract we present our initial efforts towards the development of decentralized architectures for multi-agent systems in order to understand and model societies. More specifically, using social dilemmas as the training ground, we present a novel learning architecture, Learning through Probing (LTP), where agents utilize a probing mechanism to incorporate how their opponent's behavior changes when an agent takes an action.",Multi-agent Systems; Reinforcement Learning; Cooperation,Nicolas,,Anastassacos,The Alan Turing Institute and University College London,Mirco,,Musolesi,The Alan Turing Institute and University College London,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0295,"WITHDRAWN (2/15/19, MB) Stochastic Activation Actor-Critic Methods",,,,A,,A,S,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0296,"WITHDRAWN (2/25, MB) -- One-Sided Markets with Neighbourhood Externalities",,,,S,,S,A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0299,"WITHDRAWN (2/15/19, MB) Probably Almost Stable Strategy Profiles in Simulation-Based Games",,,,A,,A,S,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0300,MCTS-based Automated Negotiation Agent,,"This paper introduces a new Negotiating Agent for automated negotiation  on continuous domains and without considering a specified deadline. The agent bidding strategy relies on Monte Carlo Tree Search, which is a trendy methods since it has been used with success on games with high branching factor such as Go. It uses two opponent modeling techniques that are used by its bidding strategy and its utility: Gaussian process regression and Bayesian learning. Evaluation is done by confronting the existing agents that are able to negotiate in such context: Random Walker, Tit-for-tat and Nice Tit-for-Tat. None of those agents succeeds in beating ours; however the modular and adaptive nature of our approach is a huge advantage when it comes to optimize it in specific applicative contexts.","Bargaining and negotiation; Learning agent-to-agent interactions (negotiation, trust, coordination)",Cédric,L. R.,Buron,Thales Research and Technology,Zahia,,Guessoum,"LIP6, Sorbonne Université",Sylvain,,Ductor,Universidade Estadual do Ceará,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0301,"Towards a ""Master Algorithm"" for Forming Faster Conventions On Various Networks",,"In this paper, we argue the importance of developing a ``Master Algorithm'' that forms faster social convention among software agents on any network topology. We hypothesize that one possible approach towards building this ``Master Algorithm'' is to embed network awareness in agent's decision-making process. As a first step towards this algorithm, we present a novel network aware convention formation (NACF) algorithm that equips agents with network awareness to select a suitable algorithm for creating faster conventions. The results obtained from an extensive set of simulations show that NACF successfully forms convention on various network scenarios. We also identify the limitations of NACF and provide insights for improvement.
",Scale-free network; Random network; Planar network; Small-world network; Ring network; Scale-free community network; Convention; Network Awareness,Mohammad,Rashedul,Hasan,University of Nebraska-Lincoln,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0303,Complexity of Additive Committee Selection with Outliers,,"We study the $\varphi_f$-{\sc{Outliers}} problem, where we are given an election and are asked whether there are at most~$\bar{n}$ votes whose removal leads to the existence of a $k$-committee of a desired quality under the voting rule~$\varphi_f$. We investigate the (parameterized) complexity of $\varphi_f$-{\sc{Outliers}} for additive $k$-committee selection rules, in both the general case and several special cases with respect to the incidence graphs of the given elections.",multiwinner voting; outliers; parameterized complexity; approval,Yongjie,,Yang,Central South University & Saarland University,Jianxin,,Wang,Central South University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0304,Maximin-Aware Allocations of Indivisible Goods,,"We study envy-free allocations of indivisible goods to agents in settings where each agent is unaware of the bundles (or allocated goods) of other agents. In particular, we propose maximin aware (MMA) fairness measure, which guarantees that every agent, given the bundle allocated to her, is aware that she does not get the worst bundle, even if she does not know how the other goods are distributed. We also introduce two of its relaxations, MMA1 and MMAX. We show that MMA1 and MMAX potentially have stronger egalitarian guarantees than EF1 and are easier to achieve than MMS and EFX. Finally, we present a polynomial-time algorithm, which computes an allocation such that every agent is either 1/2-approximate MMA or exactly MMAX. Interestingly, the returned allocation is also 1/2-approximate EFX when all agents have subadditive valuations, which answers an open question left in [Plaut and Roughgarden, SODA 2018].
",Fair allocation; maximin aware; envy-free,Hau,,Chan,University of Nebraska-Lincoln,Jing,,Chen,Stony Brook University,Bo,,Li,Stony Brook University,Xiaowei,,Wu,University of Vienna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0307,Advice Replay Approach for Richer Knowledge Transfer in Teacher Student Framework,,"One of the major drawbacks of RL is the low sample efficiency of the learning algorithms. In many cases domain expertise can help to mitigate this effect. Teacher-Student framework is one such paradigm, where a more experienced agent (teacher) upon being queried helps to accelerate the student's learning by providing advice on the action to take in a given state. Real world teachers not only provide the action to take in a given state but also provide a more informative signal using the synthesis of knowledge they may have gained with experience. With this motivation, we propose a richer advising framework where the teacher augments the student’s knowledge by also providing the expected long term reward of following that action. The student can then use this value to steadily guide its Q-Network in the correct direction which can lead to a quicker convergence. To help student relive the advices received throughout its learning, we introduce an additional memory called the Advice Replay Memory (ARM). Results show that a student following our approach (a) is able to exploit the environment better, and (b) has a steeper learning curve.
",Reinforcement Learning; Teacher-Student Framework; Transfer Learning; Deep Learning;,Vaibhav,,Gupta,"International Institute of Information Technology, Hyderabad",Daksh,,Anand,"International Institute of Information Technology, Hyderabad",Praveen,,Paruchuri,"International Institute of Information Technology, Hyderabad",Balaraman,,Ravindran,"Indian Institute of Technology, Madras",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0312,Proportional Representation in Elections: STV vs PAV,Extended Abstract,We consider proportionality in multiwinner elections and observe that PAV and STV are fundamentally different. We argue that the former is proportional and the latter is degressively proportional.,multiwinner voting; committee scoring rules; proportionality; Single Transferable Vote; Proportional Approval Voting,Piotr,,Faliszewski,AGH University of Science and Technology,Piotr,,Skowron,University of Warsaw,Stanislaw,,Szufa,Jagiellonian University,Nimrod,,Talmon,Ben-Gurion University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0316,Contest Manipulation for Improved Performance,Extended Abstract,"A contest is a situation in which participants compete with one
another for prizes by expending some resources - time, effort or
money. The contest mechanism can be used either to determine
supremacy (e.g., in a sport, activity or particular quality) or in
order to elicit effort and generate value (e.g., R&D competition,
crowdsourcing contest (as TopCoder) or even a contest for soliciting
transformative solutions for the benefit of humankind (as the Hult
prize)). As such, much research has been carried out in recent years
studying contest-based mechanism design [4–7, 15, 19, 27, 33].",contest design;mechanism design,Michal,,Habani,Bar-Ilan University,Priel,,Levy,Bar-Ilan University,David,,Sarne,Bar-Ilan University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0318,Selective Information Disclosure in Contests,Extended Abstract,"Contests are important mechanisms to elicit work (/effort/ideas)
from crowds. While contests have been used throughout history (e.g.
the British government’s 1714 Longitude Prize), they have gained
popularity in the current Internet era, and, in particular, in the
context of crowdsourcing [2, 7, 14, 41, 59, 60].Well known examples
include the Netflix prize (netflixprize.com), Darpa challenges [3,
57] and the Hult prize (hultprize.org), as well as various public
platforms that allow requesters to solicit contributions through
contests with monetary prizes, such as taskCN (www.taskcn.com),
TopCoder (www.topcoder.com) and Kaggle (www.kaggle.com). As
such, the study and analysis of contests have become prominent in
mechanism design and multi-agent systems literature [6, 14, 23, 36–
39, 43, 54]. These include both the analysis and determination of
optimal strategies - for the contestants, and methods for the design
of effective contests - for the contest’s organizer. In this work we
concentrate on the latter issue - that of contest design.",contest design;mechanism design,Priel,,Levy,Bar Ilan University,David,,Sarne,Bar-Ilan University,Yonatan,,Aumann,Bar-Ilan University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0325,Policy Networks: A Framework for Scalable Integration of Multiple Decision-Making Models,Extended Abstract,"Policy networks are graphical models that integrate decision-making models. They allow for multiple Markov decision processes (MDPs) that describe distinct focused aspects of a domain to work in harmony to solve a large-scale problem. This paper defines policy networks and shows how they are able to naturally generalize many previous models, such as options and constrained MDPs.",Markov Decision Processes; MDP; POMDP; Constrained MDP; Options,Kyle,Hollins,Wray,University of Massachusetts Amherst,Shlomo,,Zilberstein,University of Massachusetts Amherst,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0328,Actor Based Simulation for Closed Loop Control of Supply Chain using Reinforcement Learning,,"Reinforcement Learning (RL) has achieved a degree of success in control applications such as online gameplay and robotics, but has rarely been used to manage operations of business-critical systems such as supply chains. A key aspect of using RL in the real world is to train the agent before deployment, so as to minimise experimentation in live operation. While this is feasible for online gameplay (where the rules of the game are known) and robotics (where the dynamics are predictable), it is much more difficult for complex systems due to associated complexities, such as uncertainty, adaptability and emergent behaviour. In this paper, we describe a framework for effective integration of a reinforcement learning controller with an actor-based simulation of the complex networked system, in order to enable deployment of the RL agent in the real system with minimal further tuning.",Reinforcement learning; Simulation of complex systems; Model based simulation,Souvik,,Barat,Tata Consultancy Services Research,Harshad,,Khadilkar,Tata Consultancy Services Research,Hardik,,Meisheri,Tata Consultancy Services Research,Vinay,,Kulkarni,Tata Consultancy Services Research,Vinita,,Baniwal,Tata Consultancy Services Research,Prashant,,Kumar,Tata Consultancy Services Research,Monika,,Gajrani,Tata Consultancy Services Research,,,,,,,,,,,,,,,,,,,,
ea0332,Learning Self-Game-Play Agents for Combinatorial Optimization Problems,Extended Abstract,"Recent progress in reinforcement learning (RL) using self-game-play has shown remarkable performance on several board games as well as video games (e.g., Atari games and Dota2). DeepMind researchers have already implemented model-free RL to play Go and Chess at a superhuman level using neural Monte-Carlo-Tree-Search (neural MCTS). Therefore, it is plausible to consider that RL, starting from zero knowledge, can be applied to other problems which can be converted into games. We try to leverage the computational power of neural MCTS to solve a class of combinatorial optimization problems. Following the idea of Hintikka's Game-Theoretical Semantics, we propose the Zermelo Gamification (ZG) to transform specific combinatorial optimization problems into Zermelo games whose winning strategies correspond to the solutions of the original optimization problem.  The ZG also provides a specially designed neural MCTS. We use a combinatorial planning problem for which the ground-truth policy is efficiently computable to demonstrate that ZG is promising.",Reinforcement Learning; Neural MCTS; Self-game-play; Combinatorial Optimization; Model-free,Ruiyang,,Xu,Northeastern University,Karl,,Lieberherr,Northeastern University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0341,Multiagent Monte Carlo Tree Search,Extended Abstract,"Monte Carlo Tree Search (MCTS) is a best-first search which is efficient in large search spaces and is effective at balancing exploration versus exploitation. In this work, we introduce a novel extension for MCTS, called Multiagent Monte Carlo Tree Search (MAMCTS), which pairs MCTS with difference evaluations. We demonstrate the performance of MAMCTS in a cooperative, multiagent path-planning domain called Multiagent Gridworld. We show that MAMCTS using difference evaluations outperforms MAMCTS using local rewards by up to 31.4% and MAMCTS using the global reward by up to 88.9% for a system with 1,000 agents.",Multiagent Learning; Difference Evaluations; Monte Carlo Tree Search,Nicholas,,Zerbel,Oregon State University,Logan,,Yliniemi,Amazon Robotics Research and Development,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0344,Using Surrogate Models to Calibrate Agent-based Model Parameters Under Data Scarcity,Extended Abstract,"Social Simulation is one of the most prominent uses of Multiagent Systems, but it requires the costly task of fitting parameters to assure the credibility of the model. As, to date, there is no consensus on how to calibrate  parameters of agent-based models, we have investigated other knowledge domains to develop an efficient method for this task. Our proposal is based on the definition of a surrogate model, that reduces search space dimension. We have tested our method in the housing market scenario, using real data. We achieved satisfactory results, that corroborate the idea that it is important to reduce the search space for an efficient parameter calibration.",Social simulation; Simulation of complex systems; Modelling for agent based simulation,Priscilla,,Avegliano,IBM Research & University of São Paulo,Jaime,Simão,Sichman,University of São Paulo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0351,"WITHDRAWN (2/22/19, MB) -- Minimizing Movement for Fault-Tolerant Network Formation",,,,A,,A,S,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0357,Learning Simulation-Based Games from Data,,"We tackle a fundamental problem in empirical game-theoretic analysis (EGTA), that of learning equilibria of simulation-based games. Such games cannot be described in analytical form; instead, a black-box simulator can be queried to obtain noisy samples of utilities. Our approach to EGTA is in the spirit of probably approximately correct learning. We design algorithms that learn empirical games, which uniformly approximate the utilities of simulation-based games from finitely many samples. Our methodology learns all the equilibria of simulation-based games, as opposed to a single one.",Empirical Game-Theoretical Analysis; PAC Learning,Enrique,,Areyan Viqueira,Brown University,Amy,,Greenwald,Brown University,Cyrus,,Cousins,Brown University,Eli,,Upfal,Brown University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0371,Maxmin Share Fair Allocation of Indivisible Chores to Asymmetric Agents,,"We initiate the study of indivisible chore allocation for agents with asymmetric shares. The fairness concepts we focus on are natural generalizations of maxmin share: WMMS fairness and OWMMS fairness. We first highlight the fact that commonly-used algorithms that work well for allocation of goods to asymmetric agents, and even for chores to symmetric agents do not provide good approximations for allocation of chores to asymmetric agents under WMMS.  As a consequence, we present a novel polynomial-time constant-approximation algorithm, via linear program, for OWMMS. For two special cases: binary valuation case and 2-agent case, we provide exact or better constant-approximation algorithms.",Fair division; maxmin fair share; approximation algorithms; chores,Haris,,Aziz,UNSW Sydney and Data61,Hau,,Chan,University of Nebraska-Lincoln,Bo,,Li,Stony Brook University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0373,Modeling Random Guessing and Task Difficulty for Truth Inference in Crowdsourcing,Extended Abstract,"This paper addresses the challenge of truth inference in crowdsourcing applications. We propose a generative method that jointly models tasks' difficulties, workers' abilities and guessing behavior to estimate the truths of crowdsourced tasks, which leads to a more accurate estimation on the workers' abilities and tasks' truths. Experiments demonstrate that the proposed method is more effective for estimating truths of crowdsourced tasks compared with the state-of-art methods.",Crowdsourcing; Crowdsourcing Truth Inference,Yi,,Yang,Auckland University of Technilogy,Quan,,Bai,University of Tasmania,Qing,,Liu,CSIRO,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0374,Attention-based Deep Reinforcement Learning for Multi-view Environments,,"In reinforcement learning algorithms, it is a common practice to account for only a single view of the environment to make the desired decisions; however, utilizing multiple views of the environment can help to promote the learning of complicated policies. Since the views may frequently suffer from partial observability, their provided observation can have different levels of importance. In this paper, we present a novel attention-based deep reinforcement learning method in a multi-view environment in which each view can provide various representative information about the environment. Specifically, our method learns a policy to dynamically attend to views of the environment based on their importance in the decision-making process. We evaluate the performance of our method on TORCS racing car simulator and three other complex 3D environments with obstacles.",Reinforcement learning; Deep learning; Attention networks,Elaheh,,Barati,Wayne State University,Xuewen,,Chen,AIWAYS AUTO,Zichun,,Zhong,Wayne State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0390,Generating an Agent Taxonomy Using Topological Data Analysis,Extended Abstract,"One of the challenges with the interpretability of large and complex multiagent simulations is understanding the kinds of agents that emerge from the interactions in the simulation, in terms of agent states and behaviors. We address one aspect of this challenge, which is to generate an agent taxonomy by analyzing the simulation outputs. We show that topological data analysis (TDA) can be used for this problem by applying it to agent trajectories, and present some promising results from the analysis of a large-scale disaster simulation. The results show a taxonomy of multiple types of agents that emerge, and which can be tracked over time through this taxonomical description.",multiagent simulation; topology; taxonomy; simulation analytics,Samarth,,Swarup,University of Virginia,Reza,,Rezazadegan,University of Virginia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0392,Warning Time: Optimizing Strategic Signaling for Security Against Boundedly Rational Adversaries,,"Defender-attacker Stackelberg security games (SSGs) have been applied for solving many real-world security problems.
Recent work in SSGs has incorporated a deceptive signaling scheme into the SSG model, where the defender strategically reveals information about her defensive strategy to the attacker, in order to influence the attacker's decision making for the defender's own benefit.
In this work, we study the problem of signaling in security games against a \emph{boundedly rational} attacker.",Stackelberg security games; signaling schemes; bounded rationality; behavioral modeling; human subject experiments,Sarah,,Cooney,University of Southern California,Phebe,,Vayanos,University of Southern California,Thanh,H.,Nguyen,University of Oregon Computer and Information Science,Cleotilde,,Gonzalez,Carnegie Mellon University,Christian,,Lebiere,Carnegie Mellon University,Edward,A.,Cranford,Carnegie Mellon University,Milind,,Tambe,University of Southern California,,,,,,,,,,,,,,,,,,,,
ea0394,MaMiC : Macro and Micro Curriculum for Robotic Reinforcement Learning,,"Shaping in humans and animals has been shown to be a powerful tool for learning complex tasks as compared to learning in a randomized fashion. This makes the problem less complex and enables one to solve the easier sub task at hand first. Generating a curriculum for such guided learning involves subjecting the agent to easier goals first, and then gradually increasing their difficulty. This paper takes a similar direction and proposes a dual curriculum scheme for solving robotic manipulation tasks with sparse rewards, called MaMiC. It includes a macro curriculum scheme which divides the task into multiple sub-tasks followed by a micro curriculum scheme which enables the agent to learn between such discovered sub-tasks. We show how combining macro and micro curriculum strategies help in overcoming major exploratory constraints considered in robot manipulation tasks without having to engineer any complex rewards. The performance of such a dual curriculum scheme is analyzed on the Fetch environments.",Reinforcement Learning;Curriculum Learning,Manan,,Tomar,Indian Institute of Technology Madras,Akhil,,Sathuluri,Indian Institute of Technology Madras,Balaraman,,Ravindran,"Indian Institute of Technology Madras, Robert Bosch Center for Data Science and AI",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0395,Optimal Sequential Planning for Communicative Actions: A Bayesian Approach,,"We build on the Interactive POMDP (IPOMDP) framework, which extends POMDPs to multi-agent settings, and include communication which can take place between the agents. While IPOMDPs endow the agents with models of their environments and models of other agents, we supplement IPOMDPs with communicative acts available to the agents to formulate Communicative IPOMDPs (CIPOMDPs). We treat communication as a type of action; hence decisions regarding communicative acts should be based on decision-theoretic planning using Bellman optimality principle, just as they are for all other actions.  As in any form of planning, the results of actions need to be precisely specified. We use Bayes update to derive how agents update their beliefs in CIPOMDPs; updates are due to their actions, observations, messages they send to other agents, and messages they receive from others. Without communication, CIPOMDPs reduce to IPOMDPs.  Without other agents they all become classical POMDPs.",Single and multi-agent planning and scheduling; Speech act theory,Piotr,,Gmytrasiewicz,University of Illinois at Chicago,Sarit,,Adhikari,University of Illinois at Chicago,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0406,Incorporating Social Practices in BDI Agent Systems,,"When agents interact with humans, either through embodied agents
or because they are embedded in a robot, it would be easy if they
could use fixed interaction protocols as they do with other agents.
However, people do not keep fixed protocols in their day-to-day
interactions and the environments are often dynamic, making it
impossible to use fixed protocols. Deliberating about interactions
from fundamentals is not very scalable either, because in that case
all possible reactions of a user have to be considered in the plans. In
this paper we argue that social practices can be used as an inspiration
for designing flexible and scalable interaction mechanisms that are
also robust. However, using social practices requires extending the
traditional BDI deliberation cycle to monitor landmark states and
perform expected actions by leveraging existing plans. We define
and implement this mechanism in Jason using a periodically run
meta-deliberation plan, supported by a metainterpreter, and illustrate
its use in a realistic scenario.",Social practices; BDI agents; Jason; Metadeliberation,Stephen,,Cranefield,University of Otago,Frank,,Dignum,Umeå University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0414,Coordination Structures Generated by Deep Reinforcement Learning in Distributed Task Executions,,"We investigate the coordination structures generated by deep Q-network (DQN) in a distributed task execution. Cooperation and coordination are the crucial issues in multi-agent systems, and very sophisticated design or learning is required in order to achieve effective structures or regimes of coordination. In this paper, we show the results that agents establish the division of labor in a bottom-up manner by determining their implicit responsible area when input structure for DQN is constituted by their own observation and absolute location.",Multi-agent deep reinforcement learning; Coordination; Cooperation; Divisional cooperation,Yuki,,Miyashita,Waseda University,Toshiharu,,Sugawara,Waseda University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0421,Memory based Multiagent One Shot Learning,,"One shot learning is particularly difficult in multiagent systems where the relevant information is distributed across agents, and inter-agent interactions shape global emergent behavior. This paper introduces a distributed learning framework called Distributed Modular Memory Unit (DMMU) that creates a shared external memory to enable one shot adaptive learning in multiagent systems. In DMMU, a shared external memory is selectively accessed by agents acting asynchronously and in parallel. Each agent processes its own stream of sequential information independently while interacting with the shared external memory to identify, retain, and propagate salient information. This enables DMMU to rapidly assimilate task features from a group of distributed agents, consolidate it into a reconfigurable external memory, and use it for one shot multiagent learning. We compare the performance of the DMMU framework on a simulated cybersecurity task with traditional feedforward ensembles, LSTM based agents, and a centralized framework. Results demonstrate that DMMU significantly outperforms the other methods and exhibits distributed one shot learning.",memory; coordination; one-shot-learning; multiagent system; rnn,Shauharda,,Khadka,Oregon State University,Connor,,Yates,Oregon State University,Kagan,,Tumer,Oregon State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0436,Robustness against Agent Failure in Hedonic Games,,"In many real-world scenarios, stability is a key property in coalition formation to cope with uncertainty. In this paper, we propose a novel criterion that reshapes stability from robustness aspect. Specifically, we consider the problem of how stability can be maintained even after a small number of players leave the entire game, in the context of hedonic games. While one cannot guarantee the existence of robust outcomes with respect to most of the stability requirements, we identify several classes of friend-oriented and enemy-oriented games for which one can find a desired outcome efficiently. We also show that a symmetric additively hedonic game always admits an outcome that is individually stable and robust with respect to individual rationality.",Hedonic games; robustness; stability; agent failure,Ayumi,,Igarashi,Kyushu University,Kazunori,,Ota,Kyushu University,Yuko,,Sakurai,National Institute of Advanced Industrial Science and Technology (AIST),Makoto,,Yokoo,Kyushu University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0442,"Obvious Strategyproofness, Bounded Rationality and Approximation",Extended Abstract,"Obvious strategyproofness (OSP) has recently emerged as the solution concept of interest to study incentive compatibility in presence of agents with a specific form of bounded rationality, i.e., those who have \emph{no} contingent reasoning skill whatsoever. We here want to study the relationship between the approximation guarantee of incentive-compatible mechanisms and the \emph{degree} of rationality of the agents, intuitively measured in terms of the number of contingencies that they can handle in their reasoning. We weaken the definition of OSP to accommodate for cleverer agents and study the trade-off between approximation and agents' rationality for the paradigmatic machine scheduling problem. We prove that,
at least for the classical machine scheduling problem, ``good'' approximations are possible if and only if the agents' rationality allows for a significant number of contingencies to be considered, thus showing that OSP is not too restrictive a notion of bounded rationality from the point of view of approximation.",Mechanism Design; Bounded Rationality; Lookahead; Machine Scheduling;,Diodato,,Ferraioli,University of Salerno,Carmine,,Ventre,University of Essex,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0447,An Optimal Rewiring Strategy for Cooperative Multiagent Social Learning,Extended Abstract,"Multiagent coordination is a key problem in cooperative multiagent systems (MASs).
It has been widely studied in both fixed-agent repeated interaction setting and static social learning framework.
However, two aspects of dynamics in real-world MASs are currently neglected.
First, the network topologies can change during the course of interaction dynamically.
Second, the interaction utilities can be different among each pair of agents and usually unknown before interaction.
Both issues mentioned above increase the difficulty of coordination.
In this paper, we consider the multiagent social learning in a dynamic environment in which agents can alter their connections and interact with randomly chosen neighbors with unknown utilities beforehand.
We propose an optimal rewiring strategy to select most beneficial peers to maximize the accumulated payoffs in long-run interactions.
We empirically demonstrate the effects of our approach in a variety of large-scale MASs.","Learning agent-to-agent interactions (negotiation, trust, coordination); Multiagent learning; Social simulation",Hongyao,,Tang,Tianjin University,Jianye,,Hao,Tianjin University,Li,,Wang,Tianjin University,Zan,,Wang,Tianjin University,Tim,,Baarslag,Centrum Wiskunde & Informatica,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0451,Dynamic Aleatoric Reasoning in Games of Bluffing and Chance,Extended abstract," Games of chance and bluffing, such as bridge, The Resistance, and poker allow epistemic reasoning. Players know their own cards while being uncertain of opponents'. Success generally involves reducing your uncertainty without reducing that of your opponents. Reasoning in such games requires a mix of logical (deducing what is possible) and probabilistic (what is likely). We present a {\em dynamic aleatoric logic} for epistemic reasoning in such games.",Probabilistic reasoning; Dynamic epistemic logic,Tim,,French,The University of Western Australia,Andrew,,Gozzard,The University of Western Australia,Mark,,Reynolds,The University of Western Australia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0462,Improving Wind Power Forecasting through Cooperation: A Case-Study on Operating Farms,,"Concerns about climate change have never been so strong at the global level. One of the major challenges of the energy transition is dealing with the variability of renewable energies. Providing accurate production forecasts has become an important issue for the future, notably for wind energy. This paper proposes a method for wind power forecasting that focuses on interactions between neighboring wind turbines. The model is a multi-agent system based on a cooperative approach to improve an initial forecast. This work was carried out jointly with meteo*swift, a company specialized in wind power forecasting. The model was evaluated under real conditions on five wind farms currently operated by power producers. An improvement in forecast accuracy was observed compared to the model initially used by the company.",Multi-Agent System; Wind Power Forecasting; Cooperation; Wind Energy; Forecasting,Tanguy,,Esteoule,"IRIT, University of Toulouse & meteo*swift",Carole,,Bernon,"IRIT, University of Toulouse",Marie-Pierre,,Gleizes,"IRIT, University of Toulouse",Morgane,,Barthod,meteo*swift,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0467,Evaluation of Optimization for Pedestrian Route Guidance in Real-world Crowded Scene,,"In this paper, we proposed evaluation index considering safety of pedestrian.
This evaluation index, it is possible to evaluate all of pedestrian traveling time, unfairness and congestion degree.
We also confirm that the guidance control method optimized by CMA--ES can realized better than real guidance.",CMA-ES; RMSE; pedestrian simulation,Shusuke,,Shigenaka,Artificial Intelligence Research Center AIST,Shunki,,Takami,University of Tsukuba,Yoshihiko,,Ozaki,"Artificial Intelligence Research Center AIST &amp; GREE, Inc.",Masaki,,Onishi,Artificial Intelligence Research Center AIST,Tomohisa,,Yamashita,Hokkaido University,Itsuki,,Noda,Artificial Intelligence Research Center AIST,,,,,,,,,,,,,,,,,,,,,,,,
ea0472,"A Truthful, Privacy-Preserving, Approximately Efficient Combinatorial Auction For Single-minded Bidders",,"Combinatorial auctions are widely used to sell resources/items. The challenges in such auctions are multi-fold. We need to ensure that bidders, the strategic agents, bid their valuations truthfully to the auction mechanism. Besides, the agents may desire privacy of their identities as well as their bidding information. We consider three types of privacies: agent privacy, the identities of the losing bidders must not be revealed to any other agent except the auctioneer (AU), bid privacy, the bid values must be hidden from the other agents as well as the AU and bid-topology privacy, the items for which the agents are bidding must be hidden from the other agents as well as the AU. In this paper, we address whether can we solve the allocation and payment determination problems, which are NP-hard, approximately for single-minded bidders while preserving privacy. In the literature, $\sqrt{m}$-approximation, where $m$ is the number of items auctioned, and a strategy-proof mechanism is available for this, which we refer to as ICA-SM. To implement ICA-SM with privacy, we propose a novel cryptographic protocol TPACAS. We show that TPACAS achieves these privacy guarantees with high probability. To accomplish this, we use notaries who are semi-trusted third parties. We show that, in TPACAS, notaries do not learn any information about the agents and their bidding information.",Combinatorial Auctions; Privacy and Security; Truthful Mechanism Design,Sankarshan,,Damle,"International Institute of Information Technology, Hyderabad",Boi,,Faltings,Ecole Polytechnique Fede?alé de Lausanne,Sujit,,Gujar,"International Institute of Information Technology, Hyderabad",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0477,Cooperative Routing with Heterogeneous Vehicles,,"Cooperation among different vehicles is a promising application for Mobility as a Service (MaaS).
A primary problem is optimizing the vehicle routes.
In this paper, we propose a new concept, named delegation, where heterogeneous vehicles cooperate to reduce the total travel cost.
Our study models a case in logistics, where a large truck for long-distance delivery carries small self-driving cargoes for the last mile delivery, and the travel cost of the small ones is discounted.
We define an optimization problem enabling delegation, propose its integer programming~(IP) instance, and discuss our concept through numerical experiments using a modern IP solver.",Cooperative routing; Heterogeneous vehicles; Integer programming,Keisuke,,Otaki,"Toyota Central R&D Labs., Inc.",Satoshi,,Koide,"Toyota Central R&D Labs., Inc.",Ayano,,Okoso,"Toyota Central R&D Labs., Inc.",Tomoki,,Nishi,"Toyota Central R&D Labs., Inc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0478,On the Maximization of Influence Over an Unknown Social Network,Extended Abstract,"Influence maximization is a well-investigated problem which asks for key individuals who have significant influence in a given social network. This paper addresses this problem when the social network structure is hidden. We adopt the framework of influence learning from samples and build a neural network model to represent the information diffusion process. Based on the model, we propose two new algorithms $\NeuGreedy$ and $\NeuMax$. $\NeuGreedy$ simulates the traditional greedy algorithm whilst $\NeuMax$ utilizes the weights of connections between neurons. We test the algorithms on both synthetic and real-world datasets. The results verify the effectiveness of the proposed methods as compared to existing algorithms with or without the network structure.",Social network; social influence; influence maximization; neural network; hidden network structure; machine learning,Bo,,Yan,Beijing Institute of Technology,Kexiu,,Song,Beijing Institute of Technology,Jiamou,,Liu,The University of Auckland,Fanku,,Meng,Beijing Institute of Technology,Yiping,,Liu,Beijing Institute of Technology,Hongyi,,Su,Beijing Institute of Technology,,,,,,,,,,,,,,,,,,,,,,,,
ea0482,Preference-Based Fault Estimation in Autonomous Robots : Incompleteness and Meta-Diagnosis,,"In autonomous systems, planning and decision making rely on the estimation of the system state across time. In this work, we use a preference model to implement a fault management strategy that selects a unique estimated state at each time point. If this strategy is not carefully designed, it can lead to incomplete estimators that meet a dead-end in some scenarios. Our goal is to detect such scenarios at design time and to be able to blame a subset of preferences causing them; those can be proposed to the designer for revision. To do so, we propose a method for detecting dead-end scenarios, introduce preference relaxation, and apply a consistency-based meta-diagnosis approach for identifying the sets of ``faulty'' preferences for a given dead-end scenario. We build upon SAT solvers for checking estimator incompleteness, and for
consistency checking during meta-diagnosis.",Diagnosis and Abductive Reasoning; Preference Modelling and Preference-Based Reasoning; Dependable Robots; SAT: Applications,Valentin,,Bouziat,ONERA,Xavier,,Pucel,ONERA,Stéphanie,,Roussel,ONERA,Louise,,Travé-Massuyès,LAAS-CNRS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0485,Summary: Distributed Task Assignment and Path Planning with Limited Communication for Robot Teams,,"We consider multi-robot service scenarios, where tasks appear at any time and in any location of the working area.
A solution to such a service task problem requires finding a suitable task assignment and a collision-free trajectory for each robot of a multi-robot team.
In cluttered environments, such as indoor spaces with hallways, those two problems are tightly coupled.
We propose a decentralized algorithm for simultaneously solving both problems, called Hierarchical Task Assignment and Path Finding (HTAPF).
HTAPF extends a previous bio-inspired Multi-Robot Task Allocation (MRTA) framework [1].
In this work, task allocation is performed on an arbitrarily deep hierarchy of work areas and is tightly coupled with a fully distributed version of the priority-based planning paradigm [12], using only broadcast communication.
Specifically, priorities are assigned implicitly by the order in which data is received from nearby robots.
No token passing procedure or specific schedule is in place ensuring robust execution also in the presence of limited probabilistic communication and robot failures.",Multi-Robot Task Allocation; Multi-Agent Path Finding; Swarm-Robotics,Dario,,Albani,La Sapienza University of Rome,Wolfgang,,Höenig,University of Southern California,Nora,,Ayanian,University of Southern California,Daniele,,Nardi,La Sapienza University of Rome,Vito,,Trianni,ISTC - CNR,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0487,How to Get the Most from Goods Donated to Charities,Extended abstract,"The charity sector is assuming a central role in many countries, due to a generalized increase in wealth inequalities and the restructuring of the welfare state. This market, however, exhibits inefficiencies. In this work, we empirically test the adoption of a centralized truthful allocation mechanism without money to charities bidding for donations. Our results show that it is indeed possible to improve the income of the sector by at least 50% on average. We further show how the application of proxy bidding allows to maintain a significant portion of the welfare
improvements without the need of many bids. Our results pave the way for a novel and more profitable model of distribution of donated goods.",Combinatorial Auctions; Charities; Proxy bidding,Christopher,,Culley,University of Southampton,Ji,,Qi,University of Essex,Carmine,,Ventre,University of Essex,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0488,Actor-Critic Algorithms for Constrained Multi-agent Reinforcement Learning,Extended Abstract,"Multi-agent reinforcement learning has gained lot of popularity primarily owing to the success of deep function approximation architectures. However, many real-life multi-agent applications often impose constraints on the joint action sequence that can be taken by the agents. In this work, we formulate such problems in the framework of constrained cooperative stochastic games. Under this setting, the goal of the agents is to obtain joint action sequence that minimizes a total cost objective criterion subject to total cost penalty/budget functional constraints. To this end, we utilize the Lagrangian formulation and propose actor-critic algorithms.
Through experiments on a constrained multi-agent grid world task, we demonstrate that our algorithms converge to near-optimal joint action sequences satisfying the given constraints.","Constrained Reinforcement Learning; Multi-agent Learning; Actor-Critic Algorithms;
Cooperative Stochastic Game.",Raghuram Bharadwaj,,Diddigi,Indian Institute of Science,Sai Koti Reddy,,Danda,IBM Research,Prabuchandran,,Krithivasan Jayachandran,Amazon-IISc Postdoctoral Fellow,Shalabh,,Bhatnagar,Indian Institute of Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0492,"WITHDRAWN (3/8, MB) Emergent Privacy Norms for Collaborative Systems",,,,S,,S,S,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0496,Meta-Strategy for Multi-Time Negotiation: A Multi-Armed Bandit Approach,,"Multi-time negotiation, which repeats negotiations many times under the same conditions, is an important class of automated negotiation. We propose a meta-strategy that selects an agent's individual negotiation strategy for multi-time negotiation. We model the meta-strategy as a multi-armed bandit problem that regards an individual negotiation strategy as a slot machine and utility of the agent as a reward. Our meta-strategy takes an individual negotiation strategy according to the opponent's strategy, its own profile, and the opponent's profile. The experimental results demonstrate the effectiveness of our meta-strategy under various negotiation conditions.",automated negotiation; meta-strategy; multi-time negotiation; multi-armed bandit problem,Ryohei,,Kawata,Tokyo University of Agriculture and Technology,Katsuhide,,Fujita,Tokyo University of Agriculture and Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0501,Stackelberg Equilibrium Approximation in General-Sum Extensive-Form Games with Double-Oracle Sampling Method,Extended Abstract,"The paper presents a new method for approximating Strong Stackelberg Equilibrium in general-sum sequential games with imperfect information and perfect recall. The proposed approach is generic, i.e. does not rely on any specific properties of a particular game model. The method is based on iterative interleaving of the two following phases: (1) guided Monte Carlo Tree Search sampling of the Follower's strategy space and (2) building the Leader's behavior strategy tree for which the sampled Follower's strategy is an optimal response.
The above solution scheme is evaluated on interception games played on graphs with respect to expected Leader's utility and time requirements. A comparison with two state-of-the-art exact methods for this genre of games shows that in vast majority of test cases our simulation-based approach leads to optimal Leader's strategies, while excelling both exact methods in terms of time scalability and much lower memory usage.",Game Theory; Noncooperative games: computation; UCT; Guided Monte Carlo Tree Search,Jan,,Karwowski,Warsaw University of Technology,Jacek,,Ma&#324;dziuk,Warsaw University of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0502,Thompson Sampling Based Multi-Armed-Bandit Mechanism Using Neural Networks,Extended Abstract,"In many practical applications such as crowd-sourcing and online advertisement, use of mechanism design (auction-based mechanisms) depends upon inherent stochastic parameters which are unknown. These parameters are learnt using multi-armed bandit (MAB) algorithms. The mechanisms which incorporate MAB are referred to as Multi-Armed-Bandit Mechanisms. While most of the MAB mechanisms focus on frequentist approaches like upper confidence bound algorithms, recent work has shown that using Bayesian approaches like Thompson sampling results in mechanisms with better regret bounds; although lower regret is obtained at the cost of the mechanism ending up with a weaker game theoretic property i.e. Within-Period Dominant Strategy Incentive Compatibility (WP-DSIC). The existing payment rules used in the Thompson sampling based mechanisms may cause negative utility to the auctioneer. In addition, if we wish to minimize the cost to the auctioneer, it is very challenging to design payment rules that satisfy WP-DSIC while learning through Thompson sampling.

In our work, we propose a data-driven approach for designing MAB-mechanisms. Specifically, we use neural networks for designing the payment rule which is WP-DSIC, while the allocation rule is modeled using Thompson sampling. Our results, in the setting of crowd-sourcing for recruiting quality workers, indicate that the learned payment rule guarantees better cost while maximizing the social welfare and also ensuring reduced variance in the utilities to the agents.",Mechanism Design;MAB;Neural Networks,Padala,,Manisha,"International Institute of Information Technology, Hyderabad Machine Learning",Sujit,,Gujar,"International Institute of Information Technology, Hyderabad Machine Learning Laboratory",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0506,Computing Stable Solutions in Threshold Network Flow Games With Bounded Treewidth,,"Network flow games are a prominent model for team formation, where a commodity can flow through a network whose edges are controlled by selfish agents. In Threshold Network Flow Games (TNFGs), an agent team is successful if the flow it can achieve between a source and target vertices meets or exceeds a certain threshold. Cooperative game theory allows predicting how agents are likely to share the the joint reward in such settings, by applying solution concepts such as the core, which characterizes stable reward distributions. When TNFGs have empty cores, every reward distribution is somewhat unstable, which requires using a relaxed solution such as the least-core to find the most stable distribution. Earlier work showed that computing the least-core in TNFGs is computationally hard, but tractable for very restricted graphs, such as layer graphs. We extend these results, presenting polynomial algorithms for the much larger class of bounded-treewidth graphs.",Network Flow Games; Least Core,Aldo,,Pacchiano,"University of California, Berkeley",Yoram,,Bachrach,Deepmind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0522,Hybrid BiLSTM-Siamese Network for Relation Extraction,,"Relation extraction is an important processing task in knowledge graph completion. In previous approaches, it is considered to be a multi-class classification problem. In this paper, we propose a novel approach called hybrid BiLSTM-Siamese network which combines two word-level bidirectional LSTMs by a Siamese model architecture. It learns a similarity metric between two sentences and predicts the relation of a new sentence by k-nearest neighbors’ algorithm. In experiments, we use the SemEval-2010 Task8 dataset and achieve an F1-score of 81.8%.",Siamese Network; Relation Extraction; Knowledge Graph,Zeyuan,,Cui,Shandong University,Li,,Pan,Shandong University,Shijun,,Liu,Shandong University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0526,Efficient City-Scale Patrolling Using Decomposition and Grafting,,"This paper uses an integer program (IP) to formulate the city-scale patrolling (CSP) problem, with the objective of maximizing the police visibility rate (PVR) and the constraint of incident response time guarantee. We decompose the original CSP into two subproblems: minimizing police problem (MinP) and maximizing PVR (MaxP) problem. A polynomial time approximation algorithm is proposed for MinP, and a polynomial time optimal algorithm is proposed for MaxP.  We conduct experiments to demonstrate the efficiency of the proposed algorithm.",Police Patrolling; multi-objective; Approximation; Decomposition,Wanyuan,,Wang,Southeast University,Zichen,,Dong,Southeast University,Bo,,An,Nanyang Technological University,Yichuan,,Jiang,Southeast University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0529,ViTALiSE: Virtual to Augmented Loop in Smart Environments,,Future workplaces will be smart environments providing human users with features and functionalities augmenting their capabilities while lowering their cognitive/physical efforts. \vitalise{} is a vision of future smart environments integrating Human-Agent Collectives (HAC) with Digital Twins (DTs) fostering the synergistic interplay between the physical and digital reality.,"ViTALiSE; Smart environments; Mixed reality; Virtual reality, Augmented reality; Mirror Worlds; Augmented Worlds; Multiagent Systems",Stefano,,Mariani,Università di Modena e Reggio Emilia,Angelo,,Croatti,Università di Bologna,Alessandro,,Ricci,Università di Bologna,Andrea,,Prati,Università degli Studi di Parma,Giuseppe,,Vizzari,Università di Milano-Bicocca,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0531,Large-Scale Home Energy Management Using Entropy-Based Collective Multiagent Reinforcement Learning Framework,Extended Abstract,"Smart grids are contributing to the demand-side management by integrating electronic equipment, distributed energy generation and storage, and advanced meters and controllers. With the increasing adoption of distributed energy generation and storage systems, residential energy management is drawing more and more attention, which is regarded as being critical to demand-supply balancing and peak load reduction. In this paper, we focus on a microgrid in which a large-scale modern homes interact together to optimize their electricity cost. We present an Entropy-Based Collective Multiagent Deep Reinforcement Learning (EB-C-MADRL) framework to address it. Experiments demonstrate that EB-C-MADRL can reduce both the long-term group power consumption cost and daily peak demand effectively compared with existing approaches.",Energy and emissions; Agent solutions of significant social and economic impact; Other innovative application areas,Yaodong,,Yang,Tianjin University,Jianye,,Hao,Tianjin University,Yan,,Zheng,Tianjin University,Xiaotian,,Hao,University of Tianjin,Bofeng,,Fu,Tianjin University,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0534,Risk Averse Reinforcement Learning for Mixed Multi-agent Environments,,"Most real world applications of multi-agent systems, need to keep a balance between maximizing the rewards and minimizing the risks.  In this work we consider a popular risk measure, variance of return (VOR), as a constraint in the agent's policy learning algorithm in the mixed cooperative and competitive environments. We present a multi-timescale actor critic method for risk sensitive Markov games where the risk is modeled as a VOR constraint. We also show that the risk-averse policies satisfy the desired risk constraint without compromising much on the overall reward for a popular task.",Risk-Sensitive Reinforcement Learning; Mixed Multi-Agent Environments;  Actor-Critic Algorithms,D. Sai Koti,,Reddy,IBM Research,Amrita,,Saha,IBM Research,Srikanth,G.,Tamilselvam,IBM Research,Priyanka,,Agrawal,IBM Research,Pankaj,,Dayama,IBM Research,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0540,Evidence Propagation and Consensus Formation in Noisy Environments,Extended Abstract,"We study the effectiveness of consensus formation in multi-agent systems where belief updating is an iterative two-part process, consisting of both belief updating based on direct evidence and also belief combination between agents, within the context of a best-of-n problem. Agents' beliefs are represented within Dempster-Shafer theory by mass functions and we investigate the macro-level properties of four well-known belief combination operators: Dempster's rule, Yager's rule, Dubois & Prade's operator and the averaging operator. Simulation experiments are conducted for different evidence rates and noise levels. Broadly, Dubois & Prade's operator results in better convergence to the best state, and is more robust to noisy evidence.",Consensus formation; evidence propagation; noisy decision-making; emergent behaviour; distributed problem solving,Michael,,Crosscombe,University of Bristol,Jonathan,,Lawry,University of Bristol,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0545,DeepFlow: Detecting Optimal User Experience From Physiological Data Using Deep Neural Networks,Extended Abstract,"The affective state called flow is described as a state of optimal experience, total immersion and high productivity. As an important metric for various scenarios ranging from (professional) sports to work environments to user experience evaluations, it is extensively studied using traditional questionnaires. In order to make flow measurement accessible for online, real-time environments, in this work, we present our preliminary findings towards automatically estimating a user's flow state based on physiological signals measured with a wearable device. We conducted a study of subjects playing the game Tetris in varying difficulty levels, leading to boredom, stress, and flow. Using a convolutional neural network, we achieve an accuracy of 70% in recognizing flow-inducing levels.
In the future, we expect flow to be a potential reward signal for human-in-the-loop reinforcement learning systems.",affective computing; flow; socially intelligent agents; deep learning,Marco,,Maier,TAWNY,Chadly,,Marouane,TAWNY,Daniel,,Elsner,TAWNY,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0560,(WITHDRAWN 3/20) Asymptotically Optimal VCG Redistribution Mechanism for Public Project Problem,,,,s,,s,s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0564,Emergence of Scenario-Appropriate Collaborative Behaviors for Teams of Robotic Bodyguards,,"We are considering the problem of controlling a team of robotic bodyguards protecting a VIP from physical assault in the presence of neutral and/or adversarial bystanders in a variety of scenarios. This problem is challenging due to the large number of active entities with different agendas and dynamic movement patterns, the need of cooperation between the robots as well as the requirement to take into consideration criteria such as social norms in addition to the main goal of VIP safety. We show how a multi-agent reinforcement learning approach can evolve behavior policies that outperform hand-engineered approaches. Furthermore, we propose a novel multi-agent reinforcement learning algorithm inspired by universal value function approximators that can learn policies that exhibit appropriate, distinct behavior in environments with different requirements.
",Multi-Agent Reinforcement Learning; Robot Team Formation; Multi-Robot Systems,Hassam Ullah,,Sheikh,University of Central Florida,Ladislau,,Bölöni,University of Central Florida,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0565,The Imitation Game: Learned Reciprocity in Markov games,,"Reciprocity is an important feature of human social interaction and underpins our cooperative nature. What is more, simple forms of reciprocity have proved remarkably resilient in matrix game social dilemmas. Most famously, the tit-for-tat strategy performs very well in tournaments of Prisoner's Dilemma. Unfortunately this strategy is not readily applicable to the real world, in which options to cooperate or defect are temporally and spatially extended. Here, we present a general online reinforcement learning algorithm that displays reciprocal behavior towards its co-players. We show that it can induce pro-social outcomes for the wider group when learning alongside selfish agents, both in a $2$-player Markov game, and in $5$-player intertemporal social dilemmas. We analyse the resulting policies to show that the reciprocating agents are strongly influenced by their co-players' behavior.",Reciprocity;reinforcement learning;sequential social dilemmas,Tom,,Eccles,DeepMind,Edward,,Hughes,DeepMind,János,,Kramár,DeepMind,Steven,,Wheelwright,DeepMind,Joel,Z.,Leibo,DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0567,From Hotelling to Load Balancing: Approximation and the Principle of Minimum Differentiation,,"Competing firms tend to select similar locations for their stores. This phenomenon, called the principle of minimum differentiation, was captured by Hotelling with a landmark model of spatial competition but is still the object of an ongoing scientific debate. Although consistently observed in practice, many more realistic variants of Hotelling's model fail to support minimum differentiation or do not have pure equilibria at all.
In particular, it was recently proven for a generalized model which incorporates negative network externalities and which contains Hotelling's model and classical selfish load balancing as special cases, that the unique equilibria do not adhere to minimum differentiation. Furthermore, it was shown that for a significant parameter range pure equilibria do not exist.

We derive a sharp contrast to these previous results by investigating Hotelling's model with negative network externalities from an entirely new angle: approximate pure subgame perfect equilibria. This approach allows us to prove analytically and via agent-based simulations that approximate equilibria having good approximation guarantees and that adhere to minimum differentiation exist for the full parameter range of the model.
Moreover, we show that the obtained approximate equilibria have high social welfare.",Location Analysis; Facility Location Games; Approximate Pure Subgame Perfect Equilibria; Agent-based Simulation,Matthias,,Feldotto,Paderborn University,Pascal,,Lenzner,Hasso Plattner Institute & University of Potsdam,Louise,,Molitor,Hasso Plattner Institute & University of Potsdam,Alexander,,Skopalik,University of Twente,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0577,Online Motion Concept Learning: A Novel Algorithm for Sample-Efficient Learning and Recognition of Human Actions,,"Humans have the remarkable ability to interact with their environment
through rich and diverse actions. In order to actuate on
shared environments with humans, artificial agents should have
the ability to recognize the human’s actions. However the creation
of an artificial agent that is capable of recognizing all possible human
actions from prior training is unrealistic, given the diversity of
potential actions and ways to perform them. If we aim at deploying
competent artificial agents in such scenarios, this hurdle should be
overcome.","Learning agent capabilities (agent models, communication, obser-vation); Other",Miguel,,Vasco,"INESC-ID, Instituto Superior Técnico, Universidade de Lisboa",Francisco,,Melo,"INESC-ID, Instituto Superior Técnico, Universidade de Lisboa",David,,Martins de Matos,"INESC-ID, Instituto Superior Técnico, Universidade de Lisboa",Ana,,Paiva,"INESC-ID, Instituto Superior Técnico, Universidade de Lisboa",Tetsunari,,Inamura,National Institute of Informatics,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0586,Delayed and Time-Variant Patrolling Strategies against Attackers with Local Observation Capabilities,,"Surveillance of graph-represented environments is an application of autonomous patrolling robots that received remarkable attention during the last years. In this problem setting, computing a patrolling strategy is a central task to guarantee an effective protection level. Literature provides a vast set of methods where the patrolling strategies explicitly consider the presence of a rational adversary and fully informed attacker, which is characterized by worst-case (for the patroller) observation capabilities. In this work, we  consider an attacker that does not have any prior knowledge on the environment and the patrolling strategy. Instead, we assume that the attacker can only access local observations on the vertex potentially under attack. We study the definition of patrolling strategies under the assumption that the attacker, when planning an attack on a particular location, tries to forecast the arrivals of the patroller on that particular location. We model our patrolling strategies with Markov chains where we seek the generation of arrivals that are difficult to forecast. To this end we introduce time-variance in the transition matrix used to determine the patrollers movements on the graph-represented environment.",Adversarial Patrolling; Limited Observation; Mobile Robots,Carlos,,Diaz Alvarenga,"University of California, Merced",Nicola,,Basilico,University of Milan,Stefano,,Carpin,"University of California, Merced",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0590,"Deriving Norms from Actions, Values and Context",Extended Abstract,"Personal technology such as electronic partners (e-partners) play an increasing role in our daily lives, and can make an important difference by supporting us in various ways. However, when they offer this support, it is important that they do so with an understanding of our choices and what is important to us. To allow an e-partner to flexibly do this, we propose a formal framework to automatically derive norms which describe how to perform a certain behavior. These norms are directly derived from the user's actions, values and the context they are in. In this way, the e-partner can take into account the user's values and offer more flexible personalized support.",Formal methods; Values; Normative systems,Myrthe,L.,Tielman,Delft University of Technology,Catholijn,M.,Jonker,Delft University of Technology,M. Birna,,van Riemsdijk,Delft University of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0600,Automatic Feature Engineering by Deep Reinforcement Learning,Extended Abstract,"We present a framework called \textit{Learning Automatic Feature Engineering Machine} (LAFEM), which formalizes the \textit{Feature Engineering} (FE) problem as an optimization problem over a \textit{Heterogeneous Transformation Graph} (HTG). We propose a Deep Q-learning on HTG to support efficient learning of fine-grained and generalized FE policies that can transfer knowledge of engineering ""good"" features from a collection of datasets to other unseen datasets.",Innovative agents and multiagent applications; Deep learning; Feature generation,Jianyu,,Zhang,Tianjin University,Jianye,,Hao,Tianjin University,Françoise,,Fogelman-Soulié,Tianjin University,Zan,,Wang,Tianjin University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0603,Exploiting Inaccurate A Priori Knowledge in Robot Exploration,Extended Abstract,"Exploration is a task in which autonomous mobile robots incrementally discover features of interest in initially unknown environments.
Most of the current exploration approaches ignore prior knowledge about the environments that have to be explored. However, in some practical cases, such knowledge could be available. In this paper, we present a method that includes a priori knowledge in an exploration strategy that selects the next best locations the robot should reach in partially explored indoor environments by exploiting the (possibly inaccurate) knowledge of their floor plans.",robot exploration; exploration strategies; robot mapping,Matteo,,Luperto,Università degli Studi di Milano,Danilo,,Fusi,Politecnico di Milano,N. Alberto,,Borghese,Università degli Studi di Milano,Francesco,,Amigoni,Politecnico di Milano,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0605,Rethinking the Neutrality Axiom in Judgment Aggregation,,"When aggregating the judgments of a group of agents, an important consideration concerns the fairness of the aggregation process. This is the fundamental idea behind the neutrality axiom in social choice theory: if two judgments enjoy the same support amongst the agents, either both or neither of them should be part of  the collective decision. This is a reasonable requirement in many scenarios, but we argue that for scenarios in which agents are asked to judge very diverse kinds of propositions, the classical neutrality axiom is much too strong. We thus propose a family of weaker  neutrality axioms, parametrised by binary relations between the propositions.",Judgment Aggregation; Social Choice Theory; Fairness,Zoi,,Terzopoulou,University of Amsterdam,Ulle,,Endriss,University of Amsterdam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0606,Explaining Failures Propagations in the Execution of Multi-Agent Temporal Plans,,"This paper addresses the problem of explaining execution failures in the context of Temporal Multiagent Plans (TMAPs). Each diagnosis identifies the actions that executed in a faulty mode (primary failures, and those that failed  because affected by the propagation of previous failures (secondary failures).Diagnoses are then integrated with temporal explanations, that detail what happened during the plan execution.",Temporal Multi-agent Plans; Diagnosis; Explanation; SMT,Gianluca,,Torta,Università di Torino,Roberto,,Micalizio,Università di Torino,Samuele,,Sormano,Università di Torino,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0612,Logically-Constrained Neural Fitted Q-iteration,,"We propose a method for efficient training of Q-functions for continuous-state Markov Decision Processes (MDPs), such that the traces of the resulting policies satisfy a given Linear Temporal Logic (LTL) property. LTL, a modal logic, can express a wide range of time-dependent logical properties (including ""safety"") that are quite similar to patterns in natural language. We convert the LTL property into a limit deterministic Buchi automaton and construct an on-the-fly synchronised product MDP. The control policy is then synthesised by defining an adaptive reward function and by applying a modified neural fitted Q-iteration algorithm to the synchronised structure, assuming that no prior knowledge is available from the original MDP (namely, the method is model-free). The proposed method is evaluated in a numerical study to test the quality of the generated control policy and is compared with conventional methods for policy synthesis, such as MDP abstraction (Voronoi quantizer) and approximate dynamic programming (fitted value iteration).",Reinforcement Learning; Safety; Formal Methods; Neural Networks,Mohammadhosein,,Hasanbeig,University of Oxford,Alessandro,,Abate,University of Oxford,Daniel,,Kroening,University of Oxford,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0613,A Homophily-Free Community Detection Framework for Trajectories with Delayed Responses,,"Community detection has been widely studied in the areas of social network analysis and recommendation system. However, most existing research focus on cases where relationships are explicit or depend on simultaneous appearance. In this paper, we propose to study the community detection problem where the relationships are not based on simultaneous appearance, but time-delayed appearances. In other words, we aim to capture the relationship where one individual physically follows another individual. In our attempt to capture such relationships, the major challenge is the presence of spatial homophily, i.e., individuals are attracted to locations due to their popularities and not because of communications.

In tackling the community detection problem with spatial homophily and delayed responses, we make the following key contributions: (1) We introduce a four-phase framework, which by way of using quantified impacts excludes homophily. (2) To validate the framework, we generate a synthetic dataset based on a known community structure and then infer that community structure. (3) Finally, we execute this framework on a real-world dataset with more than 6,000 taxis in Singapore. Our results are also compared to those of a baseline approach without homophily-elimination.",Spatial homophily; Community detection; Trajectory simulation; Hotspot detection,Chung-Kyun,,Han,Singapore Management University,Shih-Fen,,Cheng,Singapore Management University,Pradeep,,Varakantham,Singapore Management University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0615,Stability of Human-Inspired Agent Societies,,"Models of emotion, particularly those based on the Ortony, Clore,
and Collins (OCC) account of emotions, have been used as part
of agents’ decision making processes to explore their effects on
cooperation within social dilemmas [7, 19, 22]. We analyse two
different interpretations of OCC agents. Firstly, Emotional agents
that decide their action using only a model of emotions. To analyse
the possibility of evolutionary stability of these agents we use the
Prisoner’s Dilemma game. We contrast the results with the second
interpretation of an OCC agent, the Moody agent [7], which additionally
uses a psychology-grounded model of mood. Our analysis
highlights the different strategies that are needed to achieve success
as a society in terms of both stability and cooperation, in the iterated
Prisoner’s Dilemma. The Emotional agents are better suited
playing against a mixed group of agents with differing strategies
than the Moody agents are. The Moody agents are more successful
than the Emotional agents when only one strategy exists in the
society.",Agents; OCC; Emotions; Mood; Evolutionarily Stable Strategy,Joe,,Collenette,University of Liverpool,Katie,,Atkinson,University of Liverpool,Daan,,Bloembergen,Centrum Wiskunde & Informatica (CWI),Karl,,Tuyls,University of Liverpool,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0616,Deep Generative and Discriminative Domain Adaptation,,"The ability to adapt to and learn from different domains and environments is crucial for agents to generalize. In this paper we propose a probabilistic framework for domain adaptation that blends both generative and discriminative modeling in a principled way. Under this framework, generative and discriminative models correspond to specific choices of the prior over parameters. By maximizing both the marginal and the conditional log-likelihoods, our models can use both labeled instances from the source domain as well as unlabeled instances from \emph{both} source and target domains. We show that the popular reconstruction loss of autoencoder corresponds to an upper bound of the negative marginal log-likelihoods of unlabeled instances, and give a generalization bound that explicitly incorporates it into the analysis. We instantiate our framework using neural networks, and build a concrete model, DAuto.
",Deep Learning; Adversarial Machine Learning; Domain Adaptation; Generative Models; Discriminative Models,Han,,Zhao,Carnegie Mellon University,Junjie,,Hu,Carnegie Mellon University,Zhenyao,,Zhu,Google,Adam,,Coates,Apple,Geoff,,Gordon,Carnegie Mellon University & Microsoft Research Montreal,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0619,An Adaptable Self-Monitoring Framework for Complex Machines,,"Diagnostic systems for complex machines are
highly specialized and cannot be applied in other domains without significant effort. Our goal is to improve the robustness of diagnostics with an adaptable monitoring framework for identifying and explaining anomalous behavior that can be easily modified for different domains or systems. We define a vocabulary for reasonable data---to precisely identify contradictions between expected and anomalous behavior and a language---to express rules, policies, and constraints/preferences of the user. We combine this framework with explanation mechanisms to describe the core reasons and support for a reasonableness judgment made by running the reasoner over the reasonable data, rules and the state of the related components.",Explainability; Knowledge Representation and Reasoning,Leilani,H.,Gilpin,Massachusetts Institute of Technology,Lalana,,Kagal,Massachusetts Institute of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0621,Role of Emotions in Perception of Humanness of Virtual Agents,,"We analyzed the performance of an agent based on an appraisal theory of human emotion with respect to how it modulates play in a social dilemma game. An experiment with 117 participants showed how the agent was rated on dimensions of Human-Uniqueness (HU), separating humans from animals, and Human-Nature (HN), separating humans from machines. We showed that our appraisal theoretic agent significantly improved on both HN and HU ratings, compared to the baselines. We also showed that perception of humanness positively affects cooperation and enjoyment.",Emotions; human likeness; OCC; prisoner's dilemma; virtual agents,Moojan,,Ghafurian,University of Waterloo,Neil,,Budnarain,University of Waterloo,Jesse,,Hoey,University of Waterloo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0626,Exploration in the Face of Parametric and Intrinsic Uncertainties,,"In distributional reinforcement learning (RL), the estimated distribution of the value functions model both the parametric and intrinsic uncertainties. We propose a novel, efficient exploration method for Deep RL that has two components. The first is a decaying schedule to suppress the intrinsic uncertainty. The second is an exploration bonus calculated from the upper quantiles of the learned distribution. In Atari 2600 games, our method achieves 483% average gain in cumulative rewards over QR-DQN.",distributional reinforcement learning; exploration,Borislav,,Mavrin,Huawei Noah's Ark Lab & University of Alberta,Shangtong,,Zhang,University of Oxford,Hengshuai,,Yao,Huawei Noah's Ark Lab,Linglong,,Kong,University of Alberta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0637,Towards Predictive Execution Monitoring in BDI Recipes,,"Agents do not only generate and choose plans for execution, they also monitor the execution of plans and handle contingencies [1, 5, 6]. The capacity for execution monitoring allows agents to assess the execution of plans, determine the need for re-planning, identify opportunities, and re-evaluate goal selection.</par><par>In practice, many BDI plan execution systems focus only on the current plan step. They do not project ahead the current knowledge of the agent to determine implications on future steps. Thus a failure of a future plan-step, which may already be predictable with given the current knowledge of the agent, is not detected until the last possible moment.",Execution Monitoring; BDI,Mika,,Barkan,Bar-Ilan University,Gal,A.,Kaminka,Bar-Ilan University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0638,Priority driven Local Optimization for Crowd Simulation,Extended Abstract,"We provide an initial model and preliminary findings of a lookahead based local optimization scheme for collision resolution between agents in large goal-directed crowd simulations. Considering crowd simulation to be a global optimization problem, we break down this large problem into smaller problems where each potential collision resolution step is independently optimized in terms of a criticality measure. Agents resolved earlier in order of criticality, maintain the optimized velocity obtained, for the resolution of agents that come later in that order. Hence, the problem is converted to a low dimensional optimization problem of one or two agents where all other obstacles are static or deterministically dynamic. We illustrate the performance of our method on four well known test scenarios.",agent-based crowd navigation; collision avoidance; optimization,Himangshu,,Saikia,KTH Royal Institute of Technology in Stockholm,Fangkai,,Yang,KTH Royal Institute of Technology,Christopher,,Peters,KTH Royal Institute of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0639,Aggregating Citizen Preferences for Public Projects Through Civic Crowdfunding,,"We focus on the aggregation of citizen preferences for public projects through civic crowdfunding. Existing civic crowdfunding mechanisms consider only agents with positive valuation towards the public project. Moreover, these mechanisms assume that each agent has a symmetric belief about the project getting provisioned. As public projects aim to cater to the majority, they should be provisioned only if the majority prefers it. To incorporate negative valuations, we propose a methodology to convert existing civic crowdfunding mechanisms for positive preferences to cater to markets having both types of agents. Specifically, we adapt existing PPR and PPS mechanisms to design PPRN and PPSN, that incentivize agents to contribute towards or against the project's provision: based on their preference. Besides, to address asymmetric beliefs, we propose a novel reward scheme,  Belief Based Reward (BBR) based on Robust Bayesian Truth Serum (RBTS) mechanism. BBR rewards agents based on their belief towards the project's provision. Using this reward scheme, we introduce a general mechanism for civic crowdfunding which allows for agents having asymmetric beliefs towards the project getting provisioned and incentivizes them to contribute towards the project's provision. We illustrate the general mechanism by designing two novel mechanisms, namely PPRx and PPSx, adapting PPR and PPS respectively, and prove that in both the mechanisms the project is provisioned at equilibrium.",Mechanism Design; Civic Crowdfunding; Preference Aggregation; Nash Equilibrium; Sub-game Perfect Equilibrium,Sankarshan,,Damle,"International Institute of Information Technology, Hyderabad",Moin Hussain,,Moti,"International Institute of Information Technology, Hyderabad",Praphul,,Chandra,KoineArth,Sujit,,Gujar,"International Institute of Information Technology, Hyderabad",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0640,Adaptive Multi-agent System for Situated Task Allocation,,"Multi-agent scheduling has received significant attention in tackling the problem of load balancing and task allocation in distributed systems. Apart from dividing the work through decentralization, we consider dynamicity because allocation of tasks must be concurrent with their execution, and adaptation because tasks must be reallocated when a disruptive event is performed. We will assume that agents are fully distributed and cooperative in order to optimize the global runtime, i.e a system-centric metric rather than user-centric metrics. We will also assume that a task can be performed by any single agent without preemption and precedence order. Moreover, tasks have no deadlines, are indivisible and not shareable.

We follow a market-based approach to tackle the multi-agent situated task allocation problem. In order to improve load balancing, agents adopt a locality-based strategy in concurrent one-to-many negotiations for task delegations. The task reallocation is dynamic since the negotiation process is iterated and concurrent with the tasks processing. Moreover, the system is adaptive to disruptive events, e.g. task consumptions. As a practical application, we consider the distributed deployment of the MapReduce design pattern for processing large datasets. Our preliminary empirical results show that, for such an application, the locality-based strategy improves the runtime.
",Distributed problem solving; Bargaining and negotiation,Quentin,,Baert,Université de Lille,Anne-Cécile,,Caron,Université de Lille,Maxime,,Morge,Université de Lille,Jean-Christophe,,Routier,Université de Lille,Kostas,,Stathis,"Royal Holloway, University of London",,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0648,The Gift Exchange Game: Managing Opponent Actions,,"Interacting with an opponent is a fundamental concern in multi-agent systems. In this work, we consider ways in which an agent can manipulate an opponent to adopt a preferred strategy. This difficult problem is often further complicated by the difficulty of analyzing the game. We have developed the Gift Exchange game, a sequential game that is deliberately simplified to focus on how to interact with an opponent. In this paper we describe the game and discuss different methods an agent might use to influence its opponent to select a preferred action. We show results from using simulated annealing to find optimal strategies to use against a learning opponent.",Repeated sequential games; cooperation; non-stationary opponents,Steven,,Damer,University of Minnesota,Maria,,Gini,University of Minnesota,Jeffrey,S.,Rosenschein,Hebrew University of Jerusalem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0652,DeepAggregation: A New Approach for Aggregating Incomplete Ranked Lists using Multi-Layer Graph Embedding,,"Preference aggregation, and specifically rank aggregation, is a well known problem in the fields of computational social choice and preference handling with broad application including web search and recommendation systems.
Inspired by the recent advances in the area of deep neural representation learning, for the first time in the literature, in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called {\em DeepAggregation}. It takes as input a set of incomplete rank lists and constructs a multi-layer graph wherein the nodes are the alternatives that are ranked and the edges capture information contained in the incomplete rank lists. We then compute deep neural representation vectors (i.e. embeddings) for the nodes and then derive the aggregated order using these representation vectors. Our proposed algorithm can handle incomplete rank lists with or without ties. We conduct thorough empirical analysis of the proposed DeepAggregation algorithm using various real life data sets such as TripAdvisor reviews data. We empirically observe that DeepAggregation generates impressive results in comparison with a number of well-known state-of-the-art preference aggregation methods.",Incomplete rank lists; Top-k rank lists; Partial rank lists; Rank aggregation; multi-layer graphs; deep neural networks; graph representation learning; graph embedding;,Rohith,Dwarakanath,Vallam,IBM Research - India,Ramasuri,,Narayanam,IBM Research - India,Srikanth,G.,Tamilselvam,IBM Research - India,Nicholas,,Mattei,Tulane University,Sudhanshu,S.,Singh,IBM Research - India,Shweta,,Garg,IBM Research - India,Gyana,R.,Parija,IBM Research - India,,,,,,,,,,,,,,,,,,,,
ea0653,A Social Choice Theoretic Perspective on Database Aggregation,,"Aggregating information coming from multiple sources is a longstanding problem in both knowledge representation and multiagent systems (see, e.g., [28]). Depending on the chosen representation for the incoming pieces of knowledge or information, a number of competing approaches has seen the light in these literatures. Belief merging [21–23] studies the problem of aggregating propositional formulas coming from different agents into a set of models, subject to some integrity constraint. Judgment and binary aggregation [11, 12, 17] asks individual agents to report yes/no opinions on a set of logically-related binary issues – the agenda – in order to take a collective decision. Social welfare functions, the cornerstone problem in social choice theory (see, e.g., [2]), can also be viewed as mechanisms to merge conflicting information, namely the individual preferences of voters expressed in the form of linear orders over a set of alternatives. Other examples include graph aggregation [13], multi-agent argumentation [6–8], ontology merging [26], and clustering aggregation [15].",Integrity constraints; Axioms; Collective Rationality,Francesco,,Belardinelli,Imperial College London & Université d’Evry,Umberto,,Grandi,"IRIT, University of Toulouse",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0663,A Privacy Preserving Multiagent System for Load Balancing in the Smart Grid,,"To improve system economics and reliability, microgrids (viz. power consumers equipped with local generators) can cooperatively utilize their local energy to facilitate load balancing on the power grid (balancing the regional supply and demand) via a multiagent system. However, due to the privacy concerns on continuously revealing each microgrid's local data (e.g., demand and supply at different times) for deriving real-time optimal balancing decisions, the application of such multiagent cooperation is still limited. In this paper, we design a novel privacy preserving multiagent system via an efficient cryptographic protocol for cooperatively balancing the regional supply and demand, as well as each microgrid's local supply and demand without disclosing their local data.","Privacy, Smart Grid, Multi-agent Systems, Secure Computation",Shangyu,,Xie,Illinois Institute of Technology,Yuan,,Hong,Illinois Institute of Technology,Peng-Jun,,Wan,Illinois Institute of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0674,Collaborative Reinforcement Learning Model for Sustainability of Cooperation in Sequential Social Dilemmas,,"Learning the emergence of cooperation in conflicting scenarios such as social dilemmas is a centerpiece of research. Many reinforcement learning based theories exist in the literature to address this problem. The well-known fact about RL based model's very slow learning capabilities coupled with large state space exhibit significant negative effects especially in repeated version of social dilemma settings such as repeated Public Goods Game (PGG) and thereby making them ineffective to model sustainability of cooperation. In this paper, we address this research challenge by augmenting the reinforcement learning based models with a notion of collaboration among the agents, motivated by the fact that humans learn not only through their own actions but also by following the actions of other agents who also continuously learn about the environment. In particular, we propose a novel model, which we refer to as Collaborative Reinforcement Learning (CRL), wherein we define collaboration among the agents as the ability of agents to fully follow other agent's actions/decisions. This is also termed as social learning. The proposed CRL model significantly influences the speed of individual learning, which eventually has a large effect on the collective behavior as compared to that of RL only models and thereby effectively explaining the sustainability of cooperation in repeated PGG settings. We also extend the CRL model for PGGs over different generations where agents die out and new agents are born following a birth-death process.",Multi-agent learning; Learning agent capabilities; Reinforcement learning,Ritwik,,Chaudhuri,"IBM Research, India",Kushal,,Mukherjee,"IBM Research, India",Ramasuri,,Narayanam,"IBM Research, India",Rohith,Dwarakanath,Vallam,"IBM Research, India",Ayush,,Kumar,IIT Delhi,Antriksh,,Mathur,IIT Delhi,Shweta,,Garg,"IBM Research, India",Sudhanshu,,Singh,"IBM Research, India",Gyana,,Parija,"IBM Research, India",,,,,,,,,,,,
ea0679,A Truthful Online Mechanism for Allocating Fog Computing Resources,,"The Internet of Things (IoT) is developing rapidly, and it is estimated
that by 2025 22 billion active devices will be in the IoT [13]. Since
it is impossible to let the often low-powered IoT devices perform
all computing tasks, some of which are highly computationally
demanding, fog computing, which extends the cloud to be closer
IoT devices, has been proposed as a solution [3].To make the most of
the fog resources and maximise the efficiency, good fog computing
resource allocation mechanisms are needed.",Mechanism Design; Fog Computing; IoT; Resource Allocation,Fan,,Bi,University of Southampton,Sebastian,,Stein,University of Southampton,Enrico,,Gerding,University of Southampton,Nick,,Jennings,Imperial College London,Tom,,La Porta,Pennsylvania State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0681,Reinforcement Learning with Derivative-Free Exploration,Extended Abstract,"Effective exploration is key to sample-efficient reinforcement learning. While the most popular general approaches (e.g., $\epsilon$-greedy) for exploration are still of low efficiency, derivative-free optimization also invents efficient ways of exploration for better global search, which reinforcement learning usually desires for. In this paper, we introduce a derivative-free based exploration called DFE as a general efficient exploration method for early-stage reinforcement learning. DFE overcomes the disadvantage of optimization inefficiency and pool scalability in pure derivative-free optimization based reinforcement learning methods. Our experiments show DFE is an efficient and general exploration method through exploring trajectories with DFE in deterministic off-policy method DDPG and stochastic off-policy method ACER algorithms, and applying in Atari and Mujoco, which represent a high-dimensional discrete-action environment and a continuous control environment.
",reinforcement learning;derivative-free optimization;exploration,Xiong-Hui,,Chen,Nanjing University,Yang,,Yu,Nanjing University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0682,Strategic Majoritarian Voting with Propositional Goals,(Extended Abstract),"We study strategic behaviour in goal-based voting, where agents take a collective decision over multiple binary issues based on their individual goals (expressed as propositional formulas). We focus on three generalizations of the issue-wise majority rule, and study their resistance to manipulability in the general case, as well as for restricted languages for goals. We also study how computationally hard it is for an agent to know if they can profitably manipulate.",Computational Social Choice; Strategic Voting; Knowledge Representation; Preference Modeling,Arianna,,Novaro,"IRIT, University of Toulouse",Umberto,,Grandi,University of Toulouse IRIT,Dominique,,Longin,"CNRS, Universite Paul Sabatier Toulouse III Institut de Recherche en Informatique de Toulouse (IRIT)",Emiliano,,Lorini,IRIT-CNRS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0687,(WITHDRAWN 4/1)Integrating Task-Motion Planning with Reinforcement Learning for Robust Decision Making in Mobile Robots,,,,s,,s,s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0694,Interpretable Automated Machine Learning in Maana™ Knowledge Platform,,"Machine learning is becoming an essential part of developing solutions for many industrial applications, but the lack of interpretability hinders wide industry adoption to rapidly build, test, deploy and validate machine learning models, in the sense that the insight of developing machine learning solutions are not structurally encoded, justified and transferred. In this paper we describe the Maana Meta-learning Service, an interpretable and interactive automated machine learning service residing in XYZ Knowledge Platform that performs machine-guided, user assisted pipeline search and hyper-parameter tuning and generates structured knowledge about decisions for pipeline profiling and selection. The service is shipped with the Maana Knowledge Platform and is validated using benchmark dataset. Furthermore, its capability of deriving knowledge from pipeline search facilitates various inference tasks and transferring to similar data science projects.",Auto{ML}; Machine learning; Knowledge representation,Alexander,,Elkholy,Maana Inc.,Fangkai,,Yang,Maana Inc.,Steven,,Gustafson,Maana Inc.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0696,Teaching Social Behavior through Human Reinforcement for Ad hoc Teamwork - The STAR Framework,Extended Abstract,"As AI technology continues to develop, more and more agents will become capable of long term autonomy alongside people. Thus, a recent line of research has studied the problem of teaching autonomous agents the concept of ethics and human social norms. Most existing work considers the case of an individual agent attempting to learn a predefined set of rules. In reality however, social norms are not always pre-defined and are very difficult to represent algorithmically. Moreover, the basic idea behind the social norms concept is ensuring that one's actions do not negatively influence others' utilities, which is inherently a multiagent concept. Thus, here we investigate a way to teach agents, as a team, how to act according to human social norms. In this research, we introduce the STAR framework used to teach an ad hoc team of agents to act in accordance with human social norms. Using a hybrid team (agents and people), when taking an action considered to be socially unacceptable, the agents receive negative feedback from the human teammate(s) who has(have) an awareness of the team's norms. We view STAR as an important step towards teaching agents to act more consistently with respect to human morality.",Ad hoc; Reinforcement Learning; Social Norms,Shani,,Alkoby,The University of Texas at Austin,Avilash,,Rath,The University of Texas at Austin,Peter,,Stone,The University of Texas at Austin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0700,Classification of Contractual Conflicts via Learning of Semantic Representations,,"Contracts are the main medium through which parties formalize their trade relations, be they the exchange of goods or the specification of mutual obligations. While electronic contracts allow automated processes to verify their correctness, most agreements in the real world are still written in natural language, which need substantial human revision effort to eliminate possible conflicting statements in long and complex contracts. In this paper, we formalize a typology of conflict types between clauses suitable for machine learning and develop techniques to review contracts by learning to identify and classify such conflicts, facilitating the task of contract revision. We evaluate the effectiveness of our techniques using a manually annotated contract conflict corpus with results close to the current state-of-the-art for conflict identification, while introducing a more complex classification task of such conflicts for which our method surpasses the state-of-the art method.",Natural Language Processing; Norms; Norm Conflicts; Semantic Representation,João Paulo,,Aires,Pontifical Catholic University of Rio Grande do Sul,Roger,,Granada,Pontifical Catholic University of Rio Grande do Sul,Juarez,,Monteiro,Pontifical Catholic University of Rio Grande do Sul,Rodrigo,Coelho,Barros,Pontifical Catholic University of Rio Grande do Sul,Felipe,,Meneguzzi,Pontifical Catholic University of Rio Grande do Sul,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0702,High-Level Path Planning in Hostile Dynamic Environments,,"In this paper, we introduce and study a graph-based variant of the path planning problem arising in hostile environments. Here, the robot must reach a given destination while avoiding being intercepted by probabilistic entities which exist in the graph with a given probability and move according to a probabilistic motion pattern. Given a deadline to reach its goal, the robot must compute a path that maximizes its chances of survival. To solve this problem, which is proven to be NP-hard, we present a convex Mixed-Integer Nonlinear Program to compute optimal solutions and a more scalable heuristic algorithm.",Robot control; Path planning; Hostile environments,Jacopo,,Banfi,Cornell University,Mark,,Campbell,Cornell University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0706,Deep Fictitious Play for Games with Continuous Action Spaces,Extended Abstract,"Fictitious play has been a classic algorithm to solve two-player adversarial games with discrete action spaces. In this work we develop an approximate extension of fictitious play to two-player games with high-dimensional continuous action spaces. We use generative neural networks to approximate players' best responses while also learning a differentiable approximate model to the players' rewards given their actions. Both these networks are trained jointly with gradient-based optimization to emulate fictitious play. We explore our approach in zero-sum games, non zero-sum games and security game domains.",Multi-agent learning; Two-player games; Nash Equilibrium; Fictitious Play; Stackelberg Security Games; Deep Learning,Nitin,,Kamra,University of Southern California,Umang,,Gupta,University of Southern California,Kai,,Wang,University of Southern California,Fei,,Fang,Carnegie Mellon University,Yan,,Liu,University of Southern California,Milind,,Tambe,University of Southern California,,,,,,,,,,,,,,,,,,,,,,,,
ea0709,Power Indices for Team Reformation Planning Under Uncertainty,Extended Abstract,"This work is an attempt at solving the problem of decentralized team formation and reformation under uncertainty with partial observability. We describe a model coined Team-POMDP, derived from the standard Dec-POMDP model, and we propose an approach based on the computation of team power indices using the Elo rating system to determine the most fitting team of agents in every situation. We couple this to a Monte-Carlo Tree Search algorithm to efficiently compute joint policies.","Coalition formation; Teamwork, team formation; Multi-agent planning",Jonathan,,Cohen,"GREYC-CNRS Lab, University of Caen Normandy",Abdel-Illah,,Mouaddib,"GREYC-CNRS Lab, University of Caen Normandy",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0713,Learning Behaviors from a Single Video Demonstration Using Human Feedback,,"In this paper we present a method for learning from video demonstrations by using human feedback to construct a mapping between the internal state representation of the agent and the visual representation from the video. In this way, we leverage the advantages of both these representations, i.e., we learn the policy using agent centered state representations, but are able to specify the expected behavior using video demonstrations. We show the effectiveness of our method by teaching a hopper agent in the MuJoCo simulator to perform a backflip using a single video demonstration generated in MuJoCo as well as from a real-world YouTube video of a person performing a backflip.","Deep learning; Reinforcement learning, Human-Robot Interaction",Sunil,,Gandhi,University of Maryland Baltimore County,Tim,,Oates,University of Maryland Baltimore County,Tinoosh,,Mohsenin,University of Maryland Baltimore County,Nicholas,R.,Waytowich,US Army Research Laboratory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0715,The StarCraft Multi-Agent Challenge,,"In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems.
Standardised environments such as the ALE and MuJoCo have allowed single-agent RL to move beyond toy domains, such as grid worlds. However, there is no comparable benchmark for cooperative multi-agent RL. As a result, most papers in this field use one-off toy problems, making it difficult to measure real progress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC) as a benchmark problem to fill this gap. SMAC is based on the popular real-time strategy game StarCraft II and focuses on micromanagement challenges where each unit is controlled by an independent agent that must act based on local observations. We offer a diverse set of challenge maps and recommendations for best practices in benchmarking and evaluations. We also open-source a deep multi-agent RL learning framework including state-of-the-art algorithms. We believe that SMAC can provide a standard benchmark environment for years to come.
Videos of our best agents for several SMAC scenarios are available at: https://youtu.be/VZ7zmQ_obZ0.
",StarCraft; Reinforcement Learning; Multi-Agent Learning,Mikayel,,Samvelyan,Russian-Armenian University,Tabish,,Rashid,University of Oxford,Christian,,Schroeder de Witt,University of Oxford,Gregory,,Farquhar,University of Oxford,Nantas,,Nardelli,University of Oxford,Tim,G. J.,Rudner,University of Oxford,Chia-Man,,Hung,University of Oxford,Philip,H. S.,Torr,University of Oxford,Jakob,,Foerster,University of Oxford,Shimon,,Whiteson,University of Oxford,,,,,,,,
ea0722,Adversarial Imitation Learning from State-only Demonstrations,,"Imitation from observation (IfO) is the problem of learning directly from state-only demonstrations without having access to the demonstrator's actions. The lack of action information both distinguishes IfO from most of the literature in imitation learning, and also sets it apart as a method that may enable agents to learn from a large set of previously inapplicable resources such as internet videos. In this paper, we propose a new IfO approach based on generative adversarial networks called generative adversarial imitation from observation (GAIfO). We demonstrate that our approach performs comparably to classical imitation learning approaches (which have access to the demonstrator's actions) and significantly outperforms existing imitation from observation methods in high-dimensional simulation environments.",Reinforcement Learning; Imitation Learning; Control,Faraz,,Torabi,The University of Texas at Austin,Garrett,,Warnell,Army Research Laboratory,Peter,,Stone,The University of Texas at Austin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0726,Verifying Strategic Abilities in Multi-agent Systems with Private Data-Sharing,,"Most formalisms for multi-agent systems (MAS) are not adept at explicitly expressing of data sharing between agents. Yet, disclosure and hiding of data amongst agents impacts on their strategic abilities, and so has a strong baring on non-classical logics that formally capture agents' coalitions, e.g., Alternating-time Temporal Logic (ATL).
To this end, we devise concurrent game structures with propositional control for atom-visibility (vCGS). In vCGS, agents a and b have an explicit endowment to see some of each others' variables, without other agents partaking in this. Second, we ascertain that the model checking problem for ATL with imperfect information and perfect recall on vCGS is undecidable. Third, we put forward a methodology to model check a formula varphi in ATL* on a vCGS M, by verifying a suitable translation of varphi in a submodel of M.","[Agent Theories and Models] Logic and Game Theory; Logics for agents and multi-agents systems; [Verification and Validation of Agent-based Systems] Verification techniques for multi-agents systems, including model checking",Francesco,,Belardinelli,Imperial College London & Université d’Evry,Ioana,,Boureanu,University of Surrey,Catalin,,Dima,Université Paris-Est Créteil,Vadim,,Malvone,University of Evry,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0728,Masquerade Attack Detection Through Observation Planning for Multi-Robot Systems,Extended Abstract,"The increasing adoption of autonomous mobile robots comes with a rising concern
over the security of these systems. In this work, we
examine the dangers that an adversary could pose in a multi-agent robot system.
We show that conventional multi-agent plans are vulnerable to strong attackers
masquerading as a properly functioning agent. We propose a novel technique to
incorporate attack detection into the multi-agent path-finding problem through
the simultaneous synthesis of observation plans. We show that by specially
crafting the multi-agent plan, the induced inter-agent observations can provide
introspective monitoring guarantees; we achieve guarantees that any adversarial
agent that plans to break the system-wide security specification must necessarily
violate the induced observation plan.",Multi-robot systems; Multi-agent pathfinding; Masquerade attacks; Observation planning,Kacper,,Wardega,Boston University,Roberto,,Tron,Boston University,Wenchao,,Li,Boston University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0729,Meta-learning of Bidding Agent with Knowledge Gradient in a Fully Agent-based Sponsored Search Auction Simulator,,"We take a practical approach on learning how to bid in sponsored search auctions, and model the problem of improving real world profit of advertisers in sponsored search auction as a meta-learning problem of configuring adaptive bidding agents.
We construct a fully agent-based sponsored search auction simulator that 1) captures the dynamic nature of sponsored search auctions, 2) emulates the interface of Google AdWords platforms, and 3) can be customized and extended by modules.
We then present Meta-LQKG algorithm, an agent-based meta-learning algorithm using knowledge gradient, and show the effect of meta-learning with Meta-LQKG on the performance of adaptive bidding agents.",Meta-learning; Learning to Bid; Sponsored Search Auction; Agent-Based Simulation; Knowledge Gradient,Donghun,,Lee,Princeton University,Warren,B.,Powell,Princeton University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0732,Curriculum Learning for Tightly Coupled Multiagent Systems,Extended Abstract,"In this
paper, we leverage curriculum learning (CL) to improve the performance
of multiagent systems (MAS) that are trained with the cooperative
coevolution of artificial neural networks.
We design curricula to progressively change two dimensions: scale (i.e.
domain size) and coupling (i.e. the number of agents required to
complete a subtask).
We demonstrate that CL can successfully mitigate the challenge of
learning on a sparse reward signal resulting from a high degree of
coupling in complex MAS.
We also show that, in most cases, the combination of difference reward
shaping with CL can improve performance by up to 56\%.
We evaluate our CL methods on the tightly coupled multi-rover domain.
CL increased converged system performance on all tasks presented.
Furthermore, agents were only able to learn when trained with CL for
most tasks.",Curriculum learning; multiagent coordination; difference rewards,Golden,,Rockefeller,Oregon State University,Patrick,,Mannion,Galway-Mayo Institute of Technology,Kagan,,Tumer,Oregon State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0737,A Compression-Inspired Framework for Macro Discovery,,"We consider the problem of how a reinforcement learning agent, tasked with solving a set of related Markov decision processes, can use knowledge acquired early on in its lifetime to improve its ability to more rapidly solve novel tasks. We propose a three-step framework that generates a diverse set of macros that lead to high rewards when solving a set of related tasks.
Our experiments show that augmenting the original action-set of the agent with the identified macros allows it to more rapidly learn optimal policies in novel MDPs.",Reinforcement Learning; Hierarchical RL; Exploration,Francisco,M.,Garcia,University of Massachusetts Amherst,Bruno,C.,da Silva,Federal University of Rio Grande do Sul,Philip,S.,Thomas,University of Massachusetts Amherst,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0741,When to Stop for Safe Manipulation in Unstructured Environments?,Robotics Track Extended Abstract,"Since robots are getting used in domestic environments,  relevant safety measures become crucial for safely interacting with the environment. However, these measures should not interrupt the robot execution persistently or reduce the capabilities of task execution. This can be achieved by a proper decision mechanism that is selective to what to react considering the relevancy to the current task. In this study, we propose an execution monitoring architecture that addresses these issues. This architecture provides the  reactions depending on the task in execution, the faced failures and the properties of interactions between the robot and the environment. In particular, we are interested in when the robot should choose to stop the execution as a reaction. We model the problem as a classification problem, and use a neural network based approach. We evaluate the accuracy of the reactions of our humanoid robot in real-world tabletop manipulation scenarios. The results indicate that our architecture can make a decision with 98\% accuracy.",Cognitive Architecture/System; Robot Learning; Robot Manipulation; Safety;  Execution Monitoring,Abdullah Cihan,,Ak,Istanbul Technical University,Arda,,Inceoglu,Istanbul Technical University,Sanem,,Sariel,Istanbul Technical University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0743,A Meta-MDP Approach to Exploration for Lifelong Reinforcement Learning,,"In this paper we consider the problem of how a reinforcement learning agent that is tasked with solving a sequence of reinforcement learning problems (Markov decision processes) can use knowledge acquired early in its lifetime to improve its ability to solve new problems. Specifically, we focus on the question of how the agent should explore when faced with a new environment. We show that the search for an optimal exploration strategy can be formulated as a reinforcement learning problem itself, albeit with a different timescale.
We conclude with experiments that show the benefits of optimizing an exploration strategy using our proposed approach.",Reinforcement Learning; Hierarchical RL; Exploration,Francisco,M.,Garcia,University of Massachusetts Amherst,Philip,S.,Thomas,University of Massachusetts Amherst,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0744,An Open MAS Services Architecture for the V2G/G2V Problem,,"In this paper we propose an original and open multi-agent system architecture for the important and challenging to engineer vehicle-to-grid (V2G) and grid-to-vehicle (G2V) energy transfer problem domain. To address the features required, we define two novel design patterns that can be used with statecharts in many real-world situations. The first one is based on the well-known factory design pattern, and the second on the class generalization relationship. These patterns can be coupled with ASEME, an agent-oriented software engineering methodology that uses statecharts for the inter- and intra-agent control models. The latter also fits well with the FIPA standards-compliant JADE agent platform that we used for implementation.",open multi-agent systems; smart grid; design patterns,Nikolaos,,Spanoudakis,Technical University of Crete,Charilaos,,Akasiadis,Technical University of Crete,Georgios,,Kechagias,Technical University of Crete,Georgios,,Chalkiadakis,Technical University of Crete,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0750,X*: Anytime Multiagent Planning With Bounded Search,,"Multi-agent planning in dynamic domains is a challenging problem: the size of the configuration space increases exponentially in the number of agents, and plans need to be re-evaluated periodically to account for moving obstacles. However, we have two key insights that hold in several domains: 1) conflicts between multi-agent plans often have geometrically local resolutions within a small repair window, even if such local resolutions are not globally optimal; and 2) the partial search tree for such local resolutions can then be iteratively improved over successively larger windows to eventually compute the global optimal plan. Building upon these two insights, we introduce 1) a class of anytime multiagent planning solvers, 2) a naive solver in this class, and 3) an efficient solver in this class which reuses prior search information when improving a solution.
",multiagent planning; anytime planning; bounded search; search reuse; anytime multiagent planning,Kyle,,Vedder,University of Massachusetts Amherst,Joydeep,,Biswas,University of Massachusetts Amherst,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0752,Towards a Prototypical Approach to Tool-Use Improvisation,Extended Abstract,"When a robot is operating in a dynamic environment, it cannot be assumed that a tool required to solve a given task will always be available. In case of a missing tool, an ideal response would be to find a substitute to complete the task. In this paper, we present a proof of concept of a grounded knowledge-based approach to tool substitution where knowledge is generated in an unsupervised manner from robot's sensory data about objects. Such robot-centric grounded knowledge is then used to identify a substitute from the available objects in the environment.
",Tool Substitution; Symbol Grounding; Affordances,Madhura,,Thosar,Otto-von-Guericke University of Magdeburg,Christian,A.,Mueller,Jacobs University,Sebastian,,Zug,Technische Universitaet Bergakademie Freiberg,Max,,Pfingsthorn,OFFIS Institute for Information Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0763,Report-Sensitive Spot-checking in Peer Grading,,"Peer grading systems make large courses more scalable, provide
students with faster and more detailed feedback, and help teach
students to think critically about the work of others. Various recent
implementations of peer grading mechanisms make such systems
relatively easy to deploy in practice [2, 11, 24]. The broader adoption
of such systems faces a common, critical obstacle: motivating
students to provide accurate grades. A natural solution is asking
multiple students to grade the same assignment and rewarding them
based on their behavior (e.g., based on the extent to which their
grades agree with the grades given by other students). Such solutions
have been explored in detail in a large literature on peer prediction,
which considers how to incentivize agents to truthfully disclose
unverifiable private information [4, 7–10, 12–17, 22, 23]. Unfortunately,
almost all known peer prediction mechanisms also give rise
to uninformative equilibria in which agents do not reveal their private
information; e.g., all students grading an assignment favorably
regardless of its quality [1, 8, 10, 17, 22]. Human experiments show
that such strategic behavior does arise in practice [5].",Peer Grading; Peer Prediction; Mechanism Design,Hedayat,,Zarkoob,University of British Columbia,Hu,,Fu,University of British Columbia,Kevin,,Leyton-Brown,University of British Columbia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0771,Withdrawn (2/14/19) ~ Coupling Agent Motivations and Spatial Behaviors for Authoring Multi-Agent Narratives,,,,Xun,,Zhang,Rutgers University,Davide,,Schaumann,Rutgers University,Brandon,,Haworth,X,Petros,,Faloutsos,X,Mubbasir,,Kapadia,Rutgers University,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0773,Training Cooperative Agents for Multi-Agent Reinforcement Learning,Extended Abstract,"Deep Learning and back-propagation has been successfully used to perform centralized training with communication protocols among multiple agents in a cooperative environment. In this paper we present techniques for centralized training of Multi-Agent (Deep) Reinforcement Learning (MARL) using the model-free Deep Q-Network as the baseline model and message sharing between agents. We present a novel, scalable, centralized MARL training technique, which separates the message learning module from the policy module. The separation of these modules helps in faster convergence in complex domains like autonomous driving simulators. A second contribution uses the centrally trained model to bootstrap training of  distributed, independent, cooperative agent policies for execution and thus addresses the challenges of noise and communication bottlenecks in real-time communication channels. This paper theoretically and empirically compares our centralized training algorithms to current research in the field of MARL. We also present and release a new OpenAI-Gym environment which can be used for multi-agent research as it simulates multiple autonomous cars driving cooperatively on a highway.",MARL; Multi-Agent Reinforcement Learning; Reinforcement Learning; MultiAgent Systems; Autonomous Driving,Sushrut,,Bhalla,University of Waterloo,Sriram,G.,Subramanian,University of Waterloo,Mark,,Crowley,University of Waterloo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0778,(WITHDRAWN 3/27) Decoupling Dynamics and Reward for Transfer in Multi-Agent Systems,,,,s,,s,s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0787,Long-term Autonomous Mobile Manipulation under Uncertainty,Extended Abstract,"We describe a hierarchical belief space planning framework to achieve robust long-term autonomous mobile manipulation behavior under uncertainty. The approach relies on condensing belief distributions across different abstractions to simultaneously suppress uncertainty and mitigate risk at run-time. We evaluate this system in an experimental domain that requires a robot to monitor and clean a dynamic unstructured environment, executing hundreds of physical actions without failures that require human intervention. Results indicate that this framework provides a sound basis for cognitive robots in uncertain and dynamic environments.",Long-term Autonomy; Robot Manipulation; Failure Recovery; Cognitive Architecture/System,Michael,W.,Lanighan,University of Massachusetts Amherst,Roderic,A.,Grupen,University of Massachusetts Amherst,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0788,Toward Robust Policy Summarization,,"AI agents are being developed to help people with high stakes decision-making processes from driving cars to prescribing drugs. It is therefore becoming increasingly important to develop ""explainable AI"" methods that help people understand the behavior of such agents. Summaries of agent policies can help human users anticipate agent behavior and facilitate more effective collaboration. Prior work has framed agent summarization as a machine teaching problem where examples of agent behavior are chosen to maximize reconstruction quality under the assumption that people do inverse reinforcement learning to infer an agent's policy from demonstrations. We compare summaries generated under this assumption to summaries generated under the assumption that people use imitation learning. We show through simulations that in some domains, there exist summaries that produce high-quality reconstructions under different models, but in other domains, only matching the summary extraction model to the reconstruction model produces high-quality reconstructions. These results highlight the importance of assuming correct computational models for how humans extrapolate from a summary, suggesting human-in-the-loop approaches to summary extraction.",Explainable AI; Policy Summarization,Isaac,,Lage,Harvard University,Daphna,,Lifschitz,Technion -- Israel Institute of Technology,Finale,,Doshi-Velez,Harvard University,Ofra,,Amir,Technion -- Israel Institute of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0789,Is Agent Software More Complex than Other Software?,,"We empirically investigate agent software repositories using com-monly used software metrics, which are used in software engineer-ing literature to quantify meaningful characteristics of softwarebased on its source code. We contrast the measurements with thoseof software in other categories. Analyzing hundreds of softwareprojects, we find that agent software is clearly and significantlydifferent from other types of software of comparable size.","Agent-Oriented Software Engineering; Software Metrics; AI and
Software Engineering",Alon,,Zanbar,Bar-Ilan University,Gal,A.,Kaminka,Bar-Ilan University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0790,"WITHDRAWN (3/16 & 29, LT) A Multi-agent Architecture to Integrate Deep Learning Methods for Object Recognition in Mobile Robot Systems",,,,A,,A,Instituto Militar de Engenharia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0797,A Property-based Testing Framework for Multi-Agent Systems,,"In this article we describe a framework that we have developed for testing multi-agent systems written in the Jason agent programming language, using the testing technique known as property-based testing, a form of randomised automatic model-based testing.",Testing; multi-agent systems; Jason,Clara,,Benac Earle,Universidad Politecnica de Madrid,Lars-Åke,,Fredlund,Universidad Politecnica de Madrid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0804,Manipulative Design of Scoring Systems,Extended Abstract,"Scoring systems give points to candidates according to their positions and are, for example, used in elections and sports tournaments. We study the influence that the design of such systems has on the outcome by introducing two related decision problems. The problem Scoring System Existence asks whether for a given set of profiles there exists a scoring system that makes some distinguished candidate win, whereas Closest Scoring System bounds the choice of an alternative scoring system by some given distance.
",Scoring System; Manipulation; Distances,Dorothea,,Baumeister,Heinrich-Heine-Universitaet Duesseldorf,Tobias,,Hogrebe,Heinrich-Heine-Universität Düsseldorf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0808,Removing the Target Network from Deep Q-Networks with the Mellowmax Operator,,"Deep Q-Network (DQN) is a learning algorithm that achieves human-level performance in high-dimensional domains like Atari games. We propose that using an softmax operator, Mellowmax, in DQN reduces its need for a separate target network, which is otherwise necessary to stabilize learning. We empirically show that, in the absence of a target network, the combination of Mellowmax and DQN outperforms DQN alone.",Reinforcement Learning; Sequential Decision Making,Seungchan,,Kim,Brown University,Kavosh,,Asadi,Brown University,Michael,,Littman,Brown University,George,,Konidaris,Brown University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0816,Modeling Human Decision-Making during Hurricanes: From Model to Data Collection to Prediction,Extended abstract,"Hurricanes are devastating natural disasters. To effectively plan to help people at risk during a hurricane, a model of human decision-making is needed to predict people's decisions and to potentially identify ways to influence those decisions. In this work, we propose a generative model of human decision making based on a Markov Decision Process where we explicitly model concerns, risk perception, and information. As a first step toward evaluating the model, the work presented here focuses on one step of the decision part of the model. We created a questionnaire based on the model and collect data from 2018 Hurricanes, Florence and Michael. The results show that, across hurricane data-sets that we collected, the features of the models correlate well with evacuation decisions and our model outperforms existing methods in most cases, demonstrating the validity of the proposed model.",Modelling for agent-based simulation; Validation of simulation systems; Decision-making during hurricanes,Nutchanon,,Yongsatianchot,Northeastern University,Stacy,,Marsella,Northeastern University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0823,"Towards Modeling the Interplay of Personality, Motivation, Emotion, and Mood in Social Agents",,"Creating social agents that interact in believable ways is a challenging task. The agent's emotional state must be faithfully modeled and should bear an influence on its behavior. In this paper, we introduce a computational model of affect which incorporates an empirically-based interplay between its various affective components - personality, motivation, emotion, and mood. These affective components as well as the relations between them capture a number of important mechanisms that are observable in human beings (e.g., motivation driven planning, emotional reactions, or coping) and influence the agent's decision making. Further, these mechanisms, reflected in the agent's behavior, are integral to human-human interaction and are therefore likely to contribute to improved human-agent interaction.  In a preliminary evaluation of our approach, we demonstrate the impact of the various components in the model and their interaction with one another on the agent's decision making and behavior, by showing that the agent displays disparate behavior with and without the inclusion of specific components in our model.",Emotion; Mood; Personality; Motivation; Social Agents,Maayan,,Shvo,University of Toronto,Jakob,,Buhmann,Disney Research,Mubbasir,,Kapadia,Rutgers University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0824,Preference Learning in Automated Negotiation Using Gaussian Uncertainty Models,,"In this paper, we propose a general two-objective Markov Decision Process (MDP) modeling paradigm for automated negotiation with incomplete information, in which preference elicitation alternates with negotiation actions, with the objective to optimize negotiation outcomes. The key ingredient in our MDP framework is a stochastic utility model governed by a Gaussian law, formalizing the agent's belief (uncertainty) over the user's preferences. Our belief model is fairly general and can be updated in real time as new data becomes available, which makes it a fundamental modeling tool.",Automated Negotiation; Preference Elicitation; Gaussian Processes.,Haralambie,,Leahu,Centrum Wiskunde & Informatica (CWI) IAS,Michael,,Kaisers,Centrum Wiskunde & Informatica (CWI) IAS,Tim,,Baarslag,Centrum Wiskunde & Informatica (CWI) IAS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0825,Social Power in Human-Robot Interaction: Towards More Persuasive Robots,,"Social power is defined as one's ability to influence another to do something which s/he would not do without the presence of such power. Different theories classify alternative ways to achieve social power, such as providing a reward, using coercion, or acting as an expert. In this work, we explored two types of persuasive strategies that are based on social power  (specifically Reward and Expertise) and created two social robots that would employ such strategies. To examine the effectiveness of these strategies we performed a user study with 51 participants using two social robots in an adversarial setting in which both robots try to persuade the user on a concrete choice. The results show that even though each of the strategies caused the robots to be perceived differently in terms of their competence and warmth, both were similarly persuasive.",Persuasion; Social Power; Social Reward; Human-Robot Interaction; HRI; Persuasive Robot; Social Robots,Mojgan,,Hashemian,INESC-ID & Universidade de Lisboa,Ana,,Paiva,INESC-ID & Universidade de Lisboa,Samuel,,Mascarenhas,INESC-ID & Universidade de Lisboa,Pedro,A.,Santos,INESC-ID & Universidade de Lisboa,Rui,,Prada,INESC-ID & Universidade de Lisboa,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0828,Designing Emergent Swarm Behaviors using Behavior Trees and Grammatical Evolution,Extended Abstract,"Bio-inspired collectives like honeybee, ant, and termite colonies
provide elegant distributed solutions to complex collective problems
like finding food sources, selecting a new site, and allocating
tasks. Effective collective behaviors emerge from biological swarms
through local interactions (see, for example, [7, 17, 19]).",Behavior Trees; Swarms; Grammatical Evolution,Aadesh,,Neupane,Brigham Young University,Michael,A.,Goodrich,Brigham Young University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0836,Multiagent Learning and Coordination with Clustered Deep Q-Network,Extended Abstract,"Existing decentralized learning methods entail scalability issues due to the number of agents involved. Independent Q-Learning approach proposes that each agent learns its own action-values. One drawback of this method is that the non-stationarity introduced by Independent Q-Learning limits the use of experience replay memory, needed in deep reinforcement learning methods such as Deep Q-Network. This paper presents a multiagent, multi-level solution named Clustered Deep Q-Network (CDQN) to overcome this issue.",Multiagent Reinforcement Learning; Deep Reinforcement Learning,Simon,,Pageaud,Université de Lyon - Université Claude Bernard Lyon 1 LIRIS CNRS UMR 5205 & NAVER LABS Europe,Véronique,,Deslandres,Université de Lyon - Université Claude Bernard Lyon 1 LIRIS CNRS UMR 5205,Vassilissa,,Lehoux,NAVER LABS Europe,Salima,,Hassas,Université de Lyon - Université Claude Bernard Lyon 1 LIRIS CNRS UMR 5205,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0849,"WITHDRAWN (2/25,MB) -- Enhanced Delta-tolling: Traffic Optimization via Link-Dependent Optimization",,,,A,,A,S,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0852,Applying Norms and Sanctions to Promote Cybersecurity Hygiene,,"Cybersecurity breaches cause enormous harm to the safety, privacy, and prosperity of individuals and organizations. Many security breaches occur due to people not following security regulations such as applying software patches, updating software applications, and so on. We term these regulations as cybersecurity hygiene. This paper investigates different sanctioning mechanisms with respect to the success in establishing these regulations for cybersecurity hygiene. Our findings have implications for workforce training to promote cybersecurity.",Cybersecurity; Normative systems; Sanctions; Agent societies,Shubham,,Goyal,Amazon,Nirav,,Ajmeri,North Carolina State University,Munindar,P.,Singh,North Carolina State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0858,Robust Peer-Monitoring on Graphs with an Application to Suicide Prevention in Social Networks,Extended Abstract,"We consider the problem of selecting a subset of nodes (individuals) in a (social) network that can act as monitors capable of ``watching-out'' for their neighbors (friends) when the availability or performance of the chosen monitors is uncertain. Such problems arise for example in the context of ``Gatekeeper Trainings'' for suicide prevention. We formulate this problem as a two-stage robust optimization problem that aims to maximize the worst-case number of covered nodes. Our model is capable of incorporating domain specific constraints, e.g., fairness constraints. We propose a practically tractable approximation scheme and we provide empirical results that demonstrate the effectiveness of our approach.",Multiagent Systems; Social Network; Robust Optimization,Aida,,Rahmattalabi,University of Southern California,Phebe,,Vayanos,University of Southern California,Anthony,,Fulginiti,University of Denver,Milind,,Tambe,University of Southern California,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0862,Learn a Robust Policy in Adversarial Games via Playing with an Expert Opponent,,"Reinforcement learning methods such as AlphaZero have achieved super-human performance in adversarial games by training in a self-play manner. However, they generally require a large amount of computational resources to search for an (approximately) optimal policy in the joint state-action space involving both players and the environment. To accelerate the exploration process, we propose a new paradigm of ``learning by playing'' by considering the scenarios where expert opponents are accessible. By observing the opponent actions, the agent accelerates exploration by assigning more searching sources in these actions. To alleviate the sparse reward issue when facing the expert opponent at the beginning, we technically propose a novel method called Ladder Opponent Modeling (LOM), which builds a ladder opponent to facilitate the learning process. The agent plays with both the expert and ladder alternatively with its competence improved gradually. The online manner of the ladder opponent generates auxiliary tasks gradually, yielding a tractable improvement for the agent.",Reinforcement Learning; Extensive games; Sparse Reward,Jialian,,Li,Tsinghua University,Tongzheng,,Ren,Tsinghua University,Hang,,Su,Tsinghua University,Jun,,Zhu,Tsinghua University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0865,Smart Targets to Avoid Observation in CTO Problem,,"Security is one of the values essential to the common good of society. With population growth and modernity, new challenges have emerged to ensure the safety of a large number of people in circulation with limited resources for observation. The Cooperative Targets Observation (CTO) problem consists of two groups of agents, observers and targets, in which observer agents seek to maximize the Average Number of Observed Targets (ANOT) in environments where there are more targets than observers. In most of the approaches to this problem the behavior of the target agents was modeled very simply, out of reality in competitive multi-agent environments. The objective of this work is to propose and validate four strategies for the team of target agents in the CTO problem, three involving grouping algorithms and two organizational paradigms, and one using neural networks. The approaches were implemented and tested on the NetLogo agent-based simulation platform. Test results showed that target team performance increased considerably when they were modeled as rational agents in an organization.",Cooperative Target Observation; Extension of Target's Strategy; Organization; Artificial Neural Network,Thayanne,,França da Silva,Universidade Estadual do Ceará,José Luis,,Alves Leite,Universidade Estadual do Ceará,Raimundo Juracy,,Campos Ferro Junior,Universidade Estadual do Ceará,Leonardo,,Ferreira da Costa,Universidade Estadual do Ceará,Raphael,,Pinheiro de Souza,Universidade Estadual do Ceará,João Pedro,,Bernardino Andrade,Universidade Estadual do Ceará,Gustavo Augusto,,Lima de Campos,Universidade Estadual do Ceará,,,,,,,,,,,,,,,,,,,,
ea0872,"WITHDRAWN (3/18, MB) HC-Dyna: A model-based incremental deep reinforcement learning architecture",,,,S,,S,S,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0873,"The Rise and Fall of Complex Family Structures: Coalition Formation, Stability, and Power Struggle",,"A complex family is a particular case of animal family structures that arises from cooperative breeding. This paper studies the problem of complex family formation from a game-theoretical perspective and proposes a characteristic function coalitional game. We investigate the stability of coalitions and provide theoretical bounds on the existence of complex families and the size of coalitions. Furthermore, we empirically examine the proposed framework  and show that our results are consistent with the observed coalition formations, shedding light on the family compositions in the past.",Coalition formation; cooperative games; biologically-inspired applications,Angelina,,Brilliantova,Rochester Institute of Technology,Anton,,Pletenev,Lomonosov Moscow State University,Hadi,,Hosseini,Rochester Institute of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea088,Invincible Strategies of Iterated Prisoner's Dilemma,,"The Iterated Prisoner's Dilemma (IPD) is a well-known benchmark for studying rational agents' long term behaviour such as how cooperation can emerge among selfish and unrelated agents that need to co-exist over long term. Many well-known strategies have been studied, from the simple tit-for-tat (TFT) made famous by Axelrod after his influential tournaments to more involved ones like zero determinant and extortionate strategies studied recently by Press and Dyson. In this paper, we consider what we call invincible strategies. These are ones that will never lose against any other strategy in terms of average payoff in the limit. We provide a simple characterization of this class of strategies, and discuss its relationship with some other classes of strategies.",Evolution of cooperation; Repeated games; Memory-one strategies; Invincible strategies;,Shiheng,,Wang,The Hong Kong University of Science and Technology (HKUST),Fangzhen,,Lin,The Hong Kong University of Science and Technology (HKUST),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0894,The Unbroken Telephone Game: Keeping Swarms Connected,,"Connectivity maintenance plays a key role in achieving a desired global behaviour among a swarm of robots. Yet, lack of computation resources, low communication bandwidth, robot failures, and unstable links are tough challenges for connectivity maintenance in realistic environments. In this paper, we propose a novel decentralized connectivity-preserving algorithm that can be deployed on top of other behaviours to enforce connectivity constraints. The algorithm takes a set of targets to be reached while keeping a minimum number of redundant links between robots, with the goal of guaranteeing bandwidth and reliability. We empirically study the performance of the algorithm, analyzing its time to convergence and robustness to failure.",Swarm robotics; Connectivity Maintenance; Fault-Tolerance,Vivek Shankar,,Varadharajan,École polytechnique de Montréal,Bram,,Adams,École polytechnique de Montréal,Giovanni,,Beltrame,École polytechnique de Montréal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0901,Optimal Risk in Multiagent Blind Tournaments,,"In multiagent blind tournaments, many agents compete at an individual game, unaware of the performance of the other agents. When all agents have completed their games, the agent with the best performance--for example, the highest score, or greatest distance, or fastest time--wins the tournament. In stochastic games, an obvious and time honoured strategy is to maximize expected performance. In tournaments with many agents, however, the top scores may be far above the expected score. As a result, maximizing expected score is not the same as maximizing the chance of winning the tournament. Rather, a ""riskier"" strategy, which increases the chance of obtaining a top score while possibly sacrificing some expected score, may offer a better chance of winning. In this paper, we study how an agent should optimally adapt its strategy based on the size of the tournament in which it is competing. Our solution involves first approximating the agent's pool of opponents as a collection of known or estimated strategies. Second, score distributions are computed for those strategies, and the distributions are convolved to obtain a distribution for the maximum score of the agent's opponents. Finally, a strategy that maximizes the chance of exceeding the opponents' scores is computed. As a demonstration, we study optimal tournament-size adaptation in online Yahtzee tournaments involving as few as one and as many as ten thousand opponents. We find that strategies change dramatically over that range of tournament sizes, and that in large tournaments, an agent adopting the optimally risky strategy can nearly double its chance of winning.
",multiagent; tournaments; adaptation; optimality; risk; Yahtzee,Theodore,J.,Perkins,Ottawa Hospital Research Institute & University of Ottawa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0907,Can a Virtual Human Facilitate Language Learning in a Young Baby?,Extended Abstract,"There is a significant paucity of work on language learning systems
for young infants [2, 5, 19] despite the widely understood critical
importance that this developmental period has for healthy language
and cognitive growth, and related reading and academic success
[6, 14]. Deaf babies constitute one vulnerable population as they can
experience dramatically reduced or no access to usable linguistic
input during this period [18]. This causes potentially devastating
impact on children’s linguistic, cognitive, and social skills [9, 10, 15,
16, 20]. We introduced an AI system, called RAVE (Robot, AVatar,
thermal Enhanced language learning tool), designed specifically for
babies within the age range of 6-12 months [8, 17]. RAVE consists
of two agents: a virtual human (provides language and socially
contingent interactions) and an embodied robot (provides socially
engaging physical cues to babies and directs babies’ attention to
the virtual human). Detailed description of the system’s constituent
components and dialogue algorithms are presented in [17] and [8].",Empirical studies on social agents/robots; Social impact; Multi-user/multi-agent/robot interaction,Setareh,,Nasihati Gilani,University of Southern California,David,,Traum,University of Southern California,Rachel,,Sortino,Gallaudet University,Grady,,Gallagher,Gallaudet University,Kailyn,,Aaron-lozano,Gallaudet University,Cryss,,Padilla,Gallaudet University,Ari,,Shapiro,University of Southern California,Jason,,Lamberton,Gallaudet University,Laura-ann,,Petitto,Gallaudet University,,,,,,,,,,,,
ea0914,To be Big Picture Thinker or Detail-Oriented? Utilizing Perceived Gist Information to Achieve Efficient Convention Emergence with Bilateralism and Multilateralism,,"Recently, the study of social conventions (or norms) has attracted much attention. In this paper, we study the emergence of conventions from agents' repeated coordination games via bilateralism and multilateralism. We assume that agents can perceive the gist information, i.e., a big picture of how popular each action is in their neighbourhood. A novel reinforcement learning approach which utilizes the gist information is proposed. Experiment verifies that the proposed approach significantly outperforms the baseline and the state-of-the-art approaches, in terms of the speed of convention emergence.",Norm; Convention Emergence; Fuzzy Trace Theory,Shuyue,,Hu,The Chinese University of Hong Kong,Chin-wing,,Leung,The Chinese University of Hong Kong,Ho-fung,,Leung,The Chinese University of Hong Kong,Jiamou,,Liu,The University of Auckland,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0925,The DARPA SocialSim Challenge: Massive Multi-Agent Simulations of the Github Ecosystem,,"We model the evolution of GitHub, a large collaborative software-development ecosystem, using massive multi-agent simulations as a part of DARPA's SocialSim program. Our best performing models and our agent-based simulation framework are described here. Six different agent models were tested based on a variety of machine learning and statistical methods. The most successful models are based on sampling from a stationary probability distribution of actions and repositories for each agent.",massive scale simulations; collaborative platforms; GitHub,James,,Blythe,University of Southern California,Emilio,,Ferrara,University of Southern California,Di,,Huang,University of Southern California,Kristina,,Lerman,University of Southern California,Goran,,Muric,University of Southern California,Anna,,Sapienza,University of Southern California,Alexey,,Tregubov,University of Southern California,Diogo,,Pacheco,Indiana University,John,,Bollenbacher,Indiana University,Alessandro,,Flammini,Indiana University,Pik-Mai,,Hui,Indiana University,Filippo,,Menczer,Indiana University
ea0927,Meta-learning for Predictive Knowledge Architectures: A Case Study Using TIDBD on a Sensor-rich Robotic Arm,,"Predictive approaches to modelling the environment have seen recent successes in robotics and other long-lived applications. These predictive knowledge architectures are learned incrementally and online, through interaction with the environment. One challenge for applications of predictive knowledge is the necessity of tuning feature representations and parameter values: no single step size will be appropriate for every prediction. Furthermore, as sensor signals might be subject to change in a non-stationary world, predefined step sizes cannot be sufficient for an autonomous agent.
In this paper, we explore Temporal-Difference Incremental Delta-Bar-Delta (TIDBD)—a meta-learning method for temporal-difference (TD) learning which adapts a vector of many step sizes, allowing for simultaneous step size tuning and representation learning. We demonstrate that, for a predictive knowledge application, TIDBD is a viable alternative to tuning step-size parameters, by showing that the performance of TIDBD is comparable to that of TD with an exhaustive parameter search. Performance here is measured in terms of root mean squared difference from the true value, calculated offline. Moreover, TIDBD can perform representation learning, potentially supporting robust learning in the face of failing sensors. The ability for an autonomous agent to adapt its own learning and adjust its representation based on interactions with its environment is a key capability. With its potential to fulfill these desiderata, meta-learning is a promising component for future systems.
",Continual learning; Reinforcement learning; Robot learning; Long-term autonomy,Johannes,,G&#252;enther,University of Alberta,Alex,,Kearney,University of Alberta,Nadia,M.,Ady,University of Alberta,Michael,R.,Dawson,University of Alberta,Patrick,M.,Pilarski,University of Alberta,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0930,Towards Accurate Deep-Sea Localization in Structured Environments based on Perception Quality Cues,Extended Abstract,"In recent years, the number of maritime exploration and exploitation activities has rapidly increased, and with it the necessity to perform more complex tasks underwater, e.g., floating manipulation and mapping with Remote Operated Vehicles (ROVs). The first step to perform these activities in a reliable manner, is to obtain an accurate robot localization estimate. Localization approaches based on multi-robot systems or complex acoustic infrastructures have been favored in the literature, but alternatively visual modalities are pursued when these options are not feasible. In this work, we present a two-stage navigation scheme that initially generates a coarse probabilistic map of the workspace that is used to refine localization accuracy and filter noise in the second stage. Additionally, an adaptive decision-making approach is introduced that determines which perception cues to incorporate into the localization filter, i.e., tracked 2D features or plane representations, to ensure high accuracy and reduce computation times. Our approach is thoroughly investigated in simulation and validated with deep-sea field trial data originated from oil \& gas commercial operations.",Localization;Navigation;Marine robotics;Field robotics;Multimodal perception;Adaptive behavior;Long-term autonomy,Arturo,,Gomez Chavez,Jacobs University Bremen gGmbH,Qingwen,,Xu,ShanghaiTech University,Christian,A.,Mueller,Jacobs University Bremen gGmbH,Sören,,Schwertfeger,ShanghaiTech University,Andreas,,Birk,Jacobs University Bremen gGmbH,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0936,Active Learning with Gaussian Processes for High Throughput Phenotyping,,"A looming question that must be solved before robotic plant phenotyping capabilities can have significant impact to crop improvement programs is scalability. High Throughput Phenotyping (HTP) uses robotic technologies to analyze crops in order to determine species with favorable traits, however, the current practices rely on exhaustive coverage and data collection from the entire crop field being monitored under the breeding experiment. This works well in relatively small agricultural fields but can not be scaled to the larger ones, thus limiting the progress of genetics research. In this work, we propose an active learning algorithm to enable an autonomous system to collect the most informative samples in order to accurately learn the distribution of phenotypes in the field with the help of a Gaussian Process model. We demonstrate the superior performance of our proposed algorithm compared to the current practices on sorghum phenotype data collection.",Robot Learning; Gaussian Process; Active Learning; Adaptive Sampling; Crop Phenotyping,Sumit,,Kumar,Carnegie Mellon University,Wenhao,,Luo,Carnegie Mellon University,George,,Kantor,Carnegie Mellon University,Katia,,Sycara,Carnegie Mellon University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0942,"WITHDRAWN (3/14, TP) Crowdsourced PAC Learning under Classification Noise",,,,A,,A,S,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0946,Object Exchangability in Reinforcement Learning,,"Although deep reinforcement learning has advanced significantly over the past several years, sample efficiency remains a major challenge.
Careful choice of input representations can help improve efficiency depending on the structure present in the problem. In this work, we present an attention-based method to project inputs into an efficient representation space that is invariant under changes to input ordering.
We show that our proposed representation results in a search space that is a factor of m! smaller for inputs of m objects.
Our experiments demonstrate improvements in sample efficiency for policy gradient methods on a variety of tasks.
We show that our representation allows us to solve problems that are otherwise intractable when using naive approaches.",Knowledge Representation; Reasoning; Reinforcement Learning,John,,Mern,Stanford University,Dorsa,,Sadigh,Stanford University,Mykel,,Kochenderfer,Stanford University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0954,The Effect of First- and Third-person POVs on Different Cultural Communication: How Japanese People Understand Social Conversation at Thai Night Flea Markets,Extended Abstracts,"Social and communication in culturally different condition is a challenging topic because people from many countries have to work, live, and communicate together but everyone has their own background that affects their perception. Herein, we create a cultural agent for an assistance system that can help people learn social conversational and improve their communication skills. Interaction with cultural agent from first-and third-person points of view (POVs) is novel method for human agent communication. This experiment was conducted with Japanese participants, who were asked to observe customer agents and interact with shopkeeper avatar at the Thai night market. The behavior of the agent and avatar were designed based on the Hofstede cultural dimension with Thai cultural values. A significant different result showed that most Japanese participants in the third-person POV group understood the Thai culture IDV from our simulation better than those in the first-person POV group. Furthermore, participants from both first- and third-person POV groups gave a similar score for Thai culture in the masculine and uncertainty avoidance dimensions.",Simulated crowd; different POV; cultural communication; social interaction agent,Sutasinee,,Thovuttikul,Kyoto University & RIKEN Center for Advanced Intelligence Project,Yoshimasa,,Ohmoto,Kyoto University,Toyoaki,,Nishida,Kyoto University & RIKEN Center for Advanced Intelligence Project,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0955,Escape Room: A Configurable Testbed for Hierarchical Reinforcement Learning,Extended Abstract,"Recent successes in Reinforcement Learning have encouraged a fastgrowing
network of RL researchers and a number of breakthroughs
in RL research. As the RL community and body of work grows, so
does the need for widely applicable benchmarks that can fairly and
effectively evaluate a variety of RL algorithms.
In this paper we present the Escape Room Domain (ERD), a
new flexible, scalable, and fully implemented testing domain for
Hierarchical RL that bridges the “moderate complexity"" gap left
behind by existing alternatives. ERD is open-source and freely
available through GitHub, and conforms to widely-used public
testing interfaces for simple integration and testing with a variety
of public RL agent implementations.","Reinforcement Learning; Simulation Techniques, Tools, and Platforms",Jacob,,Menashe,The University of Texas at Austin,Peter,,Stone,The University of Texas at Austin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0956,Effects of Task Similarity on Policy Transfer with Selective Exploration in Reinforcement Learning,Extended Abstract,"The SEAPoT algorithm [9] is a knowledge transfer mechanism in
model-based reinforcement learning. By constructing subspaces
around the changed regions, and selectively and efficiently exploring
the target task, the transfer is most effective when the source
and target tasks share similar objectives but differ in the transition
dynamics. In this work, we identify the similarity between tasks
using a new light-weight metric, based on the Jensen-Shannon
distance, and show how the degree of similarity affects the transfer
efficacy. We also empirically show that SEAPoT performs better
in terms of jump starts and average rewards, as compared to the
state-of-the-art policy reuse methods.",Reinforcement learning; Policy transfer; Transfer in RL; Similarity metric,Akshay,,Narayan,National University of Singapore,Tze Yun,,Leong,National University of Singapore,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0970,Two-stage N-person Prisoner's Dilemma with Social Preferences,,"We examine two-stage games where all players choose the parameters of social preferences at the first stage and play the $n$-person prisoner's dilemma at the second stage with perfect and imperfect information. This model expresses situations where players can choose how much they depend on the other players' payoffs. In this model, we get the following results. If the game has perfect information, cooperation among all players can be attained in an equilibrium by punishing a deviating player. If each player plays the $n$-person prisoner's dilemma without knowing the choices of the other players at the first stage, cooperation among a constant number of players can be attained in an equilibrium. In addition, we study two-stage games where all players choose how much they are concerned with the social welfare at the first stage and play the $n$-person prisoner's dilemma at the second stage. We show that when the players are more concerned with the minimum payoff, the number of players who cooperate at the second stage in an equilibrium weakly decreases.",Non-cooperative game; prisoner's dilemma; social preference,Seji,,Takanashi,Kyushu University,Makoto,,Yokoo,Kyushu University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0976,Bribery in Balanced Knockout Tournaments,Extended Abstract,"Balanced knockout tournaments comprise a common format for sporting competitions and pairwise decision-making. In this paper, we investigate the computational complexity of arranging the tournament's initial seeding and bribing players to guarantee one player's victory. We give a model of bribery in which the organizer can both arrange the seeding and bribe players to decrease their probability of beating other players at a cost, without exceeding a budget. We also show that it is \NP-hard to determine a bribery and a seeding under which a given player wins the tournament with probability 1, even when the pre-bribery matrix is monotonic, and the post-bribery matrix is $\epsilon$-monotonic and very close to the initial one. We also show that for almost all $n$ player inputs generated by a well known deterministic model due to Condorcet, one can always bribe the ""top"" $O(\log{n})$ players so that there is an efficiently constructible seeding for which \textit{any} player wins.",Social choice theory; Other,Christine,,Konicki,Massachusetts Institute of Technology,Virginia,,Vassilevska Williams,Massachusetts Institute of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0987,Fairness Through the Lens of Proportional Equality,,"Today, automated algorithms, such as machine learning classifiers, are playing an increasingly pivotal role in important societal decisions such as hiring, loan allocation, and criminal risk assessment. This motivates the need to probe the outcomes of a prediction model for discriminatory traits towards specific groups of individuals. In this context, one of the crucial challenges is to formally define a satisfactory notion of fairness. Our contribution in this paper is to formalize Proportional Equality (PE) as a fairness notion. We additionally show that it is a more appropriate criterion than the existing popular notion called Disparate Impact (DI), which is used for evaluating the fairness of a classifier's outcomes.",Classification; Discrimination; Racial bias; Gender bias; Prior probability shifts; Fairness concepts,Arpita,,Biswas,Indian Institute of Science,Suvam,,Mukherjee,Microsoft Research,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0997,Recognising and Explaining Bidding Strategies in Negotiation Support Systems,,"To improve a negotiator's ability to recognise bidding strategies, we pro-actively provide explanations that are based on the opponent's bids and the negotiator's guesses about the opponent's strategy. We introduce an aberration detection mechanism for recognising strategies and the notion of an explanation matrix. The aberration detection mechanism identifies when a bid falls outside the range of expected behaviour for a specific strategy. The explanation matrix is used to decide when to provide what explanations. We evaluated our work experimentally in a task in which participants are asked to identify their opponent's strategy in the environment of a negotiation support system, namely the Pocket Negotiator (PN). We implemented our explanation mechanism in the PN and experimented with different explanation matrices. As the number of correct guesses increases with explanations, indirectly, these experiments show the effectiveness of our aberration detection mechanism. Our experiments with over 100 participants show that suggesting consistent strategies is more effective than explaining why observed behaviour is inconsistent.  ",strategy recognition; aberration detection; explanation matrix; bidding strategies; negotiation support system,Vincent,J.,Koeman,Delft University of Technology,Koen,V.,Hindriks,Delft University of Technology,Jonathan,,Gratch,University of Southern California,Catholijn,M.,Jonker,Delft University of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea0998,Cooperative Multi-Agent Deep Reinforcement Learning in Soccer Domains,,"In multi-robot reinforcement learning the goal is to enable a group of robots to learn coordinated behaviors from direct interaction with the environment. Here, we provide a comparison of two main approaches designed for tackling this challenge; namely, independent learners (IL) and joint-action learners (JAL). We evaluate these methods in a multi-robot cooperative and adversarial soccer scenario, called 2 versus 2 free-kick task, with simulated NAO humanoid robots as players. Our findings show that both approaches can achieve satisfying solutions, with JAL outperforming IL.",Multi-Robot; Deep Reinforcement Learning; Robot Soccer,Jim Martin,,Catacora Ocana,Sapienza University of Rome,Francesco,,Riccio,Sapienza University of Rome,Roberto,,Capobianco,Sapienza University of Rome,Daniele,,Nardi,Sapienza University of Rome,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea1001,Domain Adaptation for Reinforcement Learning on the Atari,Extended Abstract,"Deep Reinforcement learning is a powerful machine learning paradigm that has had significant success across a wide range of control problems.  This success often requires long training times to achieve.  Observing that many problems share similarities, it is likely that much of the training done could be redundant if knowledge could be efficiently and appropriately shared across tasks. In this paper we demonstrate a novel adversarial domain adaptation approach to transfer state knowledge between domains and tasks on the Atari game suite.  We show how this approach can successfully transfer across very different visual domains of the Atari platform.  We focus on semantically related games that involve returning a ball with the user controlled agent.  Our experiments demonstrate that our method reduces the number of samples required to successfully train an agent to play an Atari game.",Deep Learning; Reinforcement Learning; Domain Adapatation,Thomas,,Carr,Aston University,Maria,,Chli,Aston University,George,,Vogiatzis,Aston University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0016,Modelling the Dynamic Joint Policy of Teammates with Attention Multi-agent DDPG,,"Modelling teammates' policies in cooperative multi-agent systems has long been an interest and also a big challenge for the reinforcement learning (RL) community. The interest lies in the fact that if the agent knows the teammates' policies, it can adjust its own policy accordingly to arrive at proper cooperations; while the challenge is that the agents' policies are changing continuously because they are learning concurrently to adapt to each other. In this paper, we present ATTention Multi-Agent Deep Deterministic Policy Gradient (ATT-MADDPG) to address this challenge. ATT-MADDPG extends DDPG, a single-agent actor-critic RL method, with two special designs. First, as a necessary step to model the teammates' policies, the agent should get access to the observations and actions of teammates. ATT-MADDPG adopts a centralized critic to collect such information. Second, ATT-MADDPG further enhances the centralized critic with an attention mechanism in a principled way. This attention mechanism introduces a special structure to explicitly model the dynamic joint policy of teammates in an adaptive manner, making sure that the collected information can be processed in an effective way. As a result, all agents will cooperate with each other efficiently. We evaluate our method on both benchmark tasks and the real-world packet routing tasks. Results show that ATT-MADDPG not only outperforms the state-of-the-art RL-based and rule-based methods by a large margin, but also achieves better scalability and robustness.",Teammates Modelling; Multi-agent Reinforcement Learning; Deep Reinforcement Learning; Agent Modelling,Hangyu,,Mao,Peking University,Zhengchao,,Zhang,Peking University,Zhen,,Xiao,Peking University,Zhibo,,Gong,"Huawei Technologies Co., Ltd.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0023,The Impact of Agent Definitions and Interactions on Multiagent Learning for Coordination,,"The state-action space of an individual agent in a multiagent team fundamentally dictates how the individual interacts with the rest of the team. Thus, how an agent is defined in the context of its domain has a significant effect on team performance when learning to coordinate. In this work we explore the trade-offs associated with these design choices, for example, having fewer agents in the team that individually are able to process and act on a wider scope of information about the world versus a larger team of agents where each agent observes and acts in a more local region of the domain. We focus our study on a traffic management domain and highlight the trends in learning performance when applying different agent definitions.",Agent definition; Agent interaction; Multiagent learning; Multiagent coordination,Jen Jen,,Chung,Eidgenössische Technische Hochschule Zürich,Damjan,,Mikli?,RoMb Technologies d.o.o.,Lorenzo,,Sabattini,University of Modena and Reggio Emilia,Kagan,,Tumer,Oregon State University,Roland,,Siegwart,Eidgenössische Technische Hochschule Zürich,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0041,Testing Preferential Domains using Sampling,,"A preferential domain is a collection of sets of preferences which are linear orders over a set of alternatives. These domains have been studied extensively in social choice theory due to both its practical importance and theoretical elegance. Examples of some extensively studied preferential domains include single peaked, single crossing, Euclidean, etc. In this paper, we study the sample complexity of testing whether a given preference profile is close to some specific domain. We consider two notions of closeness: (a) closeness via preferences, and (b) closeness via alternatives. We further explore the effect of assuming that the outlier preferences/alternatives to be random (instead of arbitrary) on the sample complexity of the testing problem. In most cases, we show that the above testing problem can be solved with high probability for all commonly used domains by observing only a small number of samples (independent of the number of preferences, n, and often the number of alternatives, m). In the remaining few cases, we prove either impossibility results or \Omega(n) lower bound on the sample complexity. We complement our theoretical findings with extensive simulations to figure out the actual constant factors of our asymptotic sample complexity bounds.",Computational social choice; preferential domain; sampling; algorithms; testing; almost single peak; almost single crossing,Palash,,Dey,Indian Institute of Technology Kharagpur,Swaprava,,Nath,Indian Institute of Technology Kanpur,Garima,,Shakya,Indian Institute of Technology Kanpur,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0044,Bootstrapped Policy Gradient for Difficulty Adaptation in Intelligent Tutoring Systems,,"One challenge for an intelligent interactive tutoring agent is to autonomously determine the difficulty levels of the questions presented to the users. This difficulty adaptation problem can be formulated as a sequential decision making problem which can be solved by Reinforcement learning (RL) methods. However, the cost of taking an action is an important consideration when applying RL in real-time responsive application involving human in the loop. Sample efficient algorithms are therefore critical for such applications, especially when the action space is large. This paper proposes a bootstrapped policy gradient (BPG) framework, which can incorporate prior knowledge into policy gradient to enhance sample efficiency. The core idea is to update the summed probability of a set of related actions rather than a single action at gradient estimation sample. A sufficient condition for unbiased convergence is provided and proved. We apply the BPG to solve the difficulty adaptation problem in a challenging environment with large action space and short horizon, and it achieves fast and unbiased convergence both in theory and in practice. We also generalize BPG to multi-dimensional continuous action domain in general actor-critic reinforcement learning algorithms with no prior knowledge required.",Reinforcement Learning; Multi-arm Bandit; Policy Gradient; Intelligent Tutoring Systems; Bootstrapped Policy Gradient,Yaqian,,Zhang,Nanyang Technological University,Wooi-Boon,,Goh,Nanyang Technological University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0054,Adversarial Coordination on Social Networks,,"Extensive literature exists studying decentralized coordination and consensus, with considerable attention devoted to ensuring robustness to faults and attacks.
However, most of the latter literature assumes that non-malicious agents follow simple stylized rules.
In reality, decentralized protocols often involve humans, and understanding how people coordinate in adversarial settings is an open problem.
We initiate a study of this problem, starting with a human subjects investigation of human coordination on networks in the presence of adversarial agents, and subsequently using the resulting data to bootstrap the development of a credible agent-based model of adversarial decentralized coordination.
In human subjects experiments, we observe that while adversarial nodes can successfully prevent consensus, the ability to communicate can significantly improve robustness, with the impact particularly significant in scale-free networks.
On the other hand, and contrary to typical stylized models of behavior, we show that the existence of trusted nodes has limited utility.
Next, we use the data collected in human subject experiments to develop a data-driven agent-based model of adversarial coordination. We show that this model successfully reproduces observed behavior in experiments, is robust to small errors in individual agent models, and illustrate its utility by using it to explore the impact of optimizing network location of trusted and adversarial nodes.",Decentralized coordination; social networks; robust consensus,Chen,,Hajaj,Ariel University,Sixie,,Yu,Washington University,Zlatko,,Joveski,Vanderbilt University,Yifan,,Guo,Capital One,Yevgeniy,,Vorobeychik,Washington University,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0055,On an Argument-centric Persuasion Framework,,"In this paper, we propose an argument-centric persuasion framework. We first introduce a decision problem, called persuasion satisfiability, which is defined as the problem of determining whether there exists a sequence of arguments that  starts from a given initial state, such as beliefs or wishes of the persuadee, and allows for achieving a given purpose of the persuader. This sequence should satisfy different constraints, including particularly upper bound constraints on the weight as well as on the length. We show that this decision problem is NP-complete and  propose an encoding in partial weighted MaxSAT framework for solving it. Then, we show that the proposed  encoding offers flexibility for dealing with different variants of the persuasion satisfiability problem. Finally, to avoid the explicit  use of  upper bound constraints on the weight and the length, we consider the notion of Pareto optimality by proposing an approach based on the use of partial weighted MaxSAT, which allows for finding non dominated (optimal) solutions.",Computational Persuasion; Argumentation; Knowledge Representation,Yakoub,,Salhi,"CRIL, U. Artois & CNRS",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0061,"Many-to-Many Stable Matchings with Ties, Master Preference Lists, and Matroid Constraints",,"In this paper, we consider a matroid generalization of the hospitals/residents problem with ties. Especially, we focus on the situation in which we are given a master list and the preference list of each hospital over residents is derived from this master list. In this setting, Kamiyama proved that if hospitals have matroid constraints and each resident is assigned to at most one hospital, then we can solve the super-stable  matching problem and the strongly stable matching problem in polynomial time. In this paper, we generalize these results to the many-to-many setting. More specifically, we consider the setting where each resident can be assigned to multiple hospitals, and the set of hospitals that this resident is assigned to must form an independent set of a matroid. In this paper, we prove that the super-stable  matching problem and the strongly stable matching problem in this setting can be solved in polynomial time.",Stable matching; Matroid; Tie,Naoyuki,,Kamiyama,Kyushu University & JST PRESTO,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0073,Multiagent Disjunctive Temporal Networks,,"Temporal network formalisms allow us to encode a set of constraints relating distinct events in time, and by deploying algorithms over these networks, we can determine whether schedules for these networks exist that satisfy all constraints. By augmenting simple temporal networks, we can consider the effects that disjunctive constraints, temporal uncertainty, and coordinating agents have on modeling fidelity and the algorithmic efficiency of schedule construction. In this paper, we introduce Partially Observable Disjunctive Temporal Networks with Uncertainty (PODTNUs) and Multiagent Disjunctive Temporal Networks with Uncertainty (MaDTNUs), generalizing previously studied multi-agent variants of temporal networks. We provide the first theoretical completeness results for the controllability of multiagent temporal network structures and discuss the importance of these results for modelers.",Single and multiagent planning and scheduling; Coordination and control models for multiagent systems,Nikhil,,Bhargava,Massachusetts Institution of Technology,Brian,,Williams,Massachusetts Institution of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0075,PT-ISABB: A Hybrid Tree-based Complete Algorithm to Solve Asymmetric Distributed Constraint Optimization Problems,,"Asymmetric Distributed Constraint Optimization Problems (ADCOPs) have emerged as an important formalism in multi-agent community due to their ability to capture personal preferences.  However, the existing search-based complete algorithms for ADCOPs can only use local knowledge to compute lower bounds, which leads to inefficient pruning and prohibits them from solving large scale problems. On the other hand, inference-based complete algorithms (e.g., DPOP) for Distributed Constraint Optimization Problems (DCOPs) require only a linear number of messages, but they cannot be directly applied into ADCOPs due to a privacy concern. Therefore, in the paper, we consider the possibility of combining inference and search to effectively solve ADCOPs at an acceptable loss of privacy. Specifically, we propose a hybrid complete algorithm called PT-ISABB which uses a tailored inference algorithm to provide tight lower bounds and a tree-based complete search algorithm to exhaust the search space. We prove the correctness of our algorithm and the experimental results demonstrate its superiority over other state-of-the-art complete algorithms.",ADCOP; Complete algorithms; Search; Inference,Yanchen,,Deng,Chongqing University,Ziyu,,Chen,Chongqing University,Dingding,,Chen,Chongqing University,Xingqiong,,Jiang,Chongqing University,Qiang,,Li,Chongqing University,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0087,Evolving Intrinsic Motivations for Altruistic Behavior,,"Multi-agent cooperation is an important feature of the natural world. Many tasks involve individual incentives that are misaligned with the common good, yet a wide range of organisms from bacteria to insects and humans are able to overcome their differences and collaborate. Therefore, the emergence of cooperative behavior amongst self-interested individuals is an important question for the fields of multi-agent reinforcement learning (MARL) and evolutionary theory. Here, we study a particular class of multi-agent problems called intertemporal social dilemmas (ISDs), where the conflict between the individual and the group is particularly sharp. By combining MARL with appropriately structured natural selection, we demonstrate that individual inductive biases for cooperation can be learned in a model-free way. To achieve this, we introduce an innovative modular architecture for deep reinforcement learning agents which supports multi-level selection. We present results in two challenging environments, and interpret these in the context of cultural and ecological evolution.",multi-agent; evolution; altruism; social dilemmas,Jane,X.,Wang,DeepMind,Edward,,Hughes,DeepMind,Chrisantha,,Fernando,DeepMind,Wojciech,M.,Czarnecki,DeepMind,Edgar,A.,Duéñez-Guzmán,DeepMind,Joel,Z.,Leibo,DeepMind,,,,,,,,,,,,,,,,,,,,,,,,
fp0107,From Matching with Diversity Constraints to Matching with Regional Quotas,,"In the past few years, several new matching models have been proposed and studied that take into account complex distributional constraints. Relevant lines of work include (1) school choice with diversity constraints where students have (possibly overlapping) types and (2) hospital-doctor matching where various regional quotas are imposed.
In this paper, we present a polynomial-time reduction to transform an instance of (1) to an instance of (2) and we show how the feasibility and stability of corresponding matchings are preserved under the reduction. Our reduction provides a formal connection between two important strands of work on matching with distributional constraints.
We then apply the reduction in two ways. Firstly, we show that it is NP-complete to check whether a feasible and stable outcome for (1) exists. Due to our reduction, these NP-completeness results carry over to setting (2). In view of this, we help unify some of the results that have been presented in the literature. Secondly, if we have positive results for (2), then we have corresponding results for (1). One key conclusion of our results is that further developments on axiomatic and algorithmic aspects of hospital-doctor matching with regional quotas will result in corresponding results for school choice with diversity constraints.",Two-sided Matching; Distributional Constraints; Diversity Constraints; Regional Quotas,Haris,,Aziz,"University of New South Wales, Sydney & Data61, CSIRO",Serge,,Gaspers,"University of New South Wales, Sydney & Data61, CSIRO",Zhaohong,,Sun,"University of New South Wales, Sydney & Data61, CSIRO",Toby,,Walsh,"University of New South Wales, Sydney & Data61, CSIRO",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0110,Reinforcement Learning for Cooperative Overtaking,,"This paper solves the cooperative overtaking problem in autonomous driving using reinforcement learning techniques. Learning in such a situation is challenging due to vehicular mobility, which renders a continuously changing environment for each learning vehicle. Without no explicit coordination mechanisms, inefficient behaviors among vehicles might cause fatal uncoordinated outcomes. To solve this issue, we propose two basic coordination models to enable distributed learning of cooperative overtaking maneuvers in a group of vehicles. Extension mechanisms are then presented to make these models workable in more complex and realistic settings with any number of vehicles. Experiments verify that, by capturing the underlying consistency of identities or positions during vehicles' movement, efficient coordinated behaviors can be achieved simply through vehicles' local learning interactions.
",Reinforcement Learning; Autonomous Driving; Coordination Graph; Cooperative Overtaking; Multiagent Learning,Chao,,Yu,Dalian University of Technology,Xin,,Wang,Dalian University of Technology,Jianye,,Hao,Tianjin University,Zhanbo,,Feng,Dalian University of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0120,Obtaining Costly Unverifiable Valuations from a Single Agent,,"We consider the problem of a principal who needs to elicit the true worth of an object she owns from an agent who has a unique ability to compute this information.
The correctness of the information cannot be verified by the principal, so it is important to incentivize the agent to report truthfully.
Previous works coped with this unverifiability by employing two or more information agents and awarding them according to the correlation between their reports.
In this paper we show that even with only one information agent truthful information can be elicited, as long as the object is valuable for the agent too. In particular the paper introduces a mechanism that, under mild realistic assumptions, is proved to elicit the information truthfully, even when computing the information is costly for the agent.
Moreover, using this mechanism, the principal obtains the truthful information incurring an arbitrarily small expense beyond whatever unavoidable costs the setting dictates.
",Information elicitation; principal-agent; truthful mechanism,Erel,,Segal-Halevi,Ariel University,Shani,,Alkoby,University of Texas at Austin,Tomer,,Sharbaf,Israel Ministry of Finance,David,,Sarne,Bar-Ilan University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0124,Complexity Results and Algorithms for Bipolar Argumentation,,"Bipolar Argumentation Frameworks (BAFs) admit several interpretations of the support relation and diverging definitions of semantics. Recently, several classes of BAFs have been captured as instances of bipolar Assumption-Based Argumentation, a class of Assumption-Based Argumentation (ABA). In this paper, we establish the complexity of bipolar ABA, and consequently of several classes of BAFs. In addition to the standard five complexity problems, we analyse the rarely-addressed extension enumeration problem too. We also advance backtracking-driven algorithms for enumerating extensions of bipolar ABA frameworks, and consequently of BAFs under several interpretations. We prove soundness and completeness of our algorithms, describe their implementation and provide a scalability evaluation. We thus contribute to the study of the as yet uninvestigated complexity problems of (variously interpreted) BAFs as well as of bipolar ABA,
and provide the lacking implementations thereof.
",Complexity; Structured Argumentation; Bipolar Argumentation,Amin,,Karamlou,Imperial College London,Kristijonas,,&#268;yras,Imperial College London,Francesca,,Toni,Imperial College London,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0125,Resolving Conflicts in Clinical Guidelines using Argumentation,,"Automatically reasoning with conflicting generic clinical guidelines is a burning issue in patient-centric medical reasoning where patient-specific conditions and goals need to be taken into account. It is even more challenging in the presence of preferences such as patient's wishes and clinician's priorities over goals. We advance a structured argumentation formalism for reasoning with conflicting clinical guidelines, patient-specific information and preferences. Our formalism integrates assumption-based reasoning and goal-driven selection among reasoning outcomes. Specifically, we assume applicability of guideline recommendations concerning the generic goal of patient well-being, resolve conflicts among recommendations using patient's conditions and preferences, and then consider prioritised patient-centered goals to yield non-conflicting, goal-maximising and preference-respecting recommendations. We rely on the state-of-the-art Transition-based Medical Recommendation model for representing guideline recommendations and augment it with context
given by the patient's conditions, goals, as well as preferences over recommendations and goals. We establish desirable properties of our approach in terms of sensitivity to recommendation conflicts and patient context.",Medical reasoning; Structured argumentation; Ariadne principles,Kristijonas,,&#269;yras,Imperial College London,Tiago,,Oliveira,National Institute of Informatics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0127,NoRML: No-reward Meta Learning,,"Efficiently adapting to new environments and changes in dynamics is critical for agents to successfully operate in the real world. Reinforcement learning (RL) based approaches typically rely on external reward feedback for adaptation. However, in many scenarios this reward signal might not be readily available for the target task, or the difference between the environments can be implicit and only observable from the dynamics. To this end, we introduce a method that allows for self-adaptation of learned policies: No-Reward Meta Learning (NoRML). NoRML extends Model Agnostic Meta Learning (MAML) for RL and uses observable dynamics of the environment instead of an explicit reward function in MAML’s finetune step. Our method has a more expressive update step than MAML, while maintaining MAML’s gradient based foundation. Additionally, in order to allow more targeted exploration, we implement an extension to MAML that effectively disconnects the meta-policy parameters from the fine-tuned policies’ parameters. We first study our method on a number of synthetic control problems and then validate our method on common benchmark environments, showing that NoRML outperforms MAML when the dynamics change between tasks.",Deep Learning; Reinforcement Learning; Meta Learning,Yuxiang,,Yang,Robotics at Google,Ken,,Caluwaerts,Robotics at Google,Atil,,Iscen,Robotics at Google,Jie,,Tan,Robotics at Google,Chelsea,,Finn,Robotics at Google,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0131,Obviously Strategyproof Mechanisms without Money for Scheduling,,"We consider the scheduling problem when no payments are allowed and the machines are bound by their declarations. We are interested in a stronger notion of truthfulness termed obvious strategyproofness (OSP) and explore its possibilities and its limitations. OSP formalizes the concept of truthfulness for agents/machines with a certain kind of bounded rationality, by making an agent's incentives to act truthfully obvious in some sense: roughly speaking, the worst possible outcome after selecting her true type is at least as good as the best possible outcome after misreporting her type. Under the weaker constraint of truthfulness, Koutsoupias [2011] proves a tight approximation ratio of $\frac{n+1}{2}$ for one task. We wish to examine how this guarantee is affected by the strengthening of the incentive compatibility constraint. The main message of our work is that there is essentially no worsening of the approximation guarantee corresponding to the significant strengthening of the guarantee of incentive-compatibility from truthfulness to OSP. To achieve this, we introduce the notion of strict monitoring and prove that such a monitoring framework is essential, thus providing a complete picture of OSP with monitoring in the context of scheduling a task without money.",Obvious strategyproofness; Extensive-form mechanisms without money; Monitoring; Machine scheduling,Maria,,Kyropoulou,University of Essex,Carmine,,Ventre,University of Essex,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0137,Extending Modular Semantics for Bipolar Weighted Argumentation,,"Weighted argumentation offers a tool for
decision support and social media analysis.
Arguments are evaluated by an iterative procedure that takes
initial weights and attack and support relations into account.
Mossakowski and Neuhaus recently unified different approaches
and proved first convergence results in cyclic graphs.
We build up on this work, simplify and generalize convergence results and
add runtime guarantees.
As it turns out, there is a tradeoff between convergence guarantees
and the ability to move strength values away from the initial weights.
We demonstrate that continuizing semantics can avoid divergence without this tradeoff.
Semantically, we extend the framework with a Duality property that assures
a symmetric impact of attack and support.
We also present a Java implementation of modular semantics and explain
the practical usefulness of the theoretical ideas.",Abstract Argumentation; Bipolar Argumentation; Weighted Argumentation,Nico,,Potyka,University of Osnabrueck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0140,Removing Malicious Nodes from Networks,,"A fundamental challenge in networked systems is detection and removal of suspected
malicious nodes.
In reality, detection is always imperfect, and the decision about
which potentially malicious
nodes to remove must trade off false positives (erroneously removing
benign nodes) and false negatives (mistakenly failing to remove
malicious nodes).
However, in network settings this conventional tradeoff must now
account for node connectivity.
In particular, malicious
nodes may exert malicious influence, so that mistakenly leaving some of
these in the network may cause damage to spread.
On the other hand, removing benign nodes causes direct harm to these,
and indirect harm to their benign neighbors who would wish
to communicate with them.
We formalize the problem of removing potentially malicious nodes from
a network under uncertainty through an objective that takes connectivity into account.
We show that optimally solving the resulting problem is NP-Hard.
We then propose a tractable solution approach based on a convex relaxation of the objective.
Finally, we experimentally demonstrate that our approach significantly
outperforms both a simple baseline that ignores network structure, as well as a state-of-the-art approach for a related problem, on both synthetic and real-world datasets.",network security; relational learning; social network,Sixie,,Yu,Washington University in St. Louis,Yevgeniy,,Vorobeychik,Washington University in St. Louis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0143,Attacking Similarity-Based Link Prediction in Social Networks,,"Link prediction is one of the fundamental problems in computational social science. A particularly common means to predict existence of unobserved links is via structural similarity metrics, such as the number of common neighbors; node pairs with higher similarity are thus deemed more likely to be linked. However, a number of applications of link prediction, such as predicting links in gang or terrorist networks, are adversarial, with another party incentivized to minimize its effectiveness by manipulating observed information about the network. We offer a comprehensive algorithmic investigation of the problem of attacking similarity-based link prediction through link deletion, focusing on two broad classes of such approaches, one which uses only local information about target links, and another which uses global network information. While we show several variations of the general problem to be NP-Hard for both local and global metrics, we exhibit a number of well-motivated special cases which are tractable. Additionally, we provide principled and empirically effective algorithms for the intractable cases, in some cases proving worst-case approximation guarantees.",Computational social science; link prediction; security and privacy; adversarial attacks,Kai,,Zhou,Washington University in St. Louis,Tomasz,P.,Michalak,University of Warsaw,Marcin,,Waniek,Khalifa University of Science and Technology,Talal,,Rahwan,Khalifa University of Science and Technology,Yevgeniy,,Vorobeychik,Washington University in St. Louis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0145,"Your 2 is My 1, Your 3 is My 9: Handling Arbitrary Miscalibrations in Ratings",,"A key step in building multi-agent systems is to gather data reported by the agents (people), in either cardinal (numeric ratings) or ordinal (rankings) form. Cardinal scores collected from people are well known to suffer from miscalibrations. A popular approach to address this issue is to assume simplistic models of miscalibration (such as linear biases) to de-bias the scores. This approach, however, often fares poorly because people's miscalibrations are typically far more complex and not well understood. It is widely believed that in the absence of simplifying assumptions on the miscalibration, the only useful information in practice from the cardinal scores is the induced ranking. In this paper we address the fundamental question of whether this widespread folklore belief is actually true. We consider cardinal scores with arbitrary (or even adversarially chosen) miscalibrations that is only required to be consistent with the induced ranking. We design rating-based estimators and prove that despite making no assumptions on the ratings, they strictly and uniformly outperform all possible estimators that rely on only the ranking. These estimators can be used as a plug-in to show the superiority of cardinal scores over ordinal rankings for a variety of applications, and we provide examples for A/B testing and ranking as a proof of concept. Our results thus provide novel fundamental insights in the eternal debate between cardinal and ordinal data.",Miscalibration; crowdsourcing; data collection methodologies; preference aggregation; multi-agent systems,Jingyan,,Wang,Carnegie Mellon University,Nihar,B.,Shah,Carnegie Mellon University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0150,Tracing Equilibrium in Dynamic Markets via Distributed Adaptation,,"In real-world decentralized systems, agents' actions are often coupled with changes in the environment which are out of the agents' control. Yet, in many important domains, the existing analyses presume static environments. The theme of our work is to bridge such a gap between existing work and reality, with a focus on markets.

Competitive (market) equilibrium is a central concept in economics with numerous applications beyond markets, such as scheduling, fair allocation of goods, or bandwidth distribution in networks. Natural and decentralized processes like tatonnement and proportional response dynamics (PRD) are known to converge quickly towards equilibrium in large classes of \emph{static} Fisher markets. In contrast, many large real-world markets are subject to frequent and dynamic changes.
We provide the first provable performance guarantees of discrete-time tatonnement and PRD in \emph{dynamic markets}.
We analyze the prominent class of CES (Constant Elasticity of Substitution) Fisher markets and quantify the impact of changes
in supplies of goods, budgets of agents, and utility functions of agents on the convergences of the processes to equilibrium.
Since the equilibrium becomes a dynamic object and will rarely be reached, we provide bounds expressing the distance to equilibrium that will be maintained. Our results indicate that in many cases, the processes trace the equilibrium rather closely and quickly recover conditions of approximate market clearing.

Our analyses proceed by quantifying the impact of variation in market parameters on several potential functions which guarantee convergences in static settings. This approach is captured in two general yet handy frameworks for Lyapunov dynamical systems.
They are of independent interest, which we demonstrate with the analysis of load balancing in dynamic environment setting.",Fisher Markets; Peer-to-Peer Networks; Tatonnement; Proportional Response Dynamics; Dynamic Markets,Yun Kuen,,Cheung,Singapore University of Technology and Design,Martin,,Hoefer,Goethe University Frankfurt/Main,Paresh,,Nakhe,Goethe University Frankfurt/Main,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0153,Algorithms for Gerrymandering over Graphs,,"We initiate the systematic algorithmic study for gerrymandering over graphs that was recently introduced by Cohen-Zemach, Lewenberg and Rosenschein. Namely, we study a strategic procedure for a political districting designer to draw electoral district boundaries so that a particular target candidate can win in an election. We focus on the existence of such a strategy under the plurality voting rule, and give interesting contrasts which classify easy and hard instances with respect to polynomial-time solvability. For example, we prove that the problem for trees is strongly NP-complete (thus unlikely to have a pseudo-polynomial-time algorithm), but has a pseudo-polynomial-time algorithm when the number of candidates is constant. Another example is to prove that the problem for complete graphs is NP-complete when the number of electoral districts is two, while is solvable in polynomial time when it is more than two.",Gerrymandering; Computational Social Choice; Graph Algorithms,Takehiro,,Ito,Tohoku University,Naoyuki,,Kamiyama,"Kyushu University & JST, PRESTO",Yusuke,,Kobayashi,Kyoto University,Yoshio,,Okamoto,University of Electro-Communications & RIKEN AIP,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0160,Automated Mechanism Design via Neural Networks,,"Using AI approaches to automatically design mechanisms has been a central research mission at the interface of AI and economics. Previous approaches that attempt to design revenue optimal auctions for the multi-dimensional settings fall short in at least one of the three aspects: 1) representation --- search in a space that probably does not even contain the optimal mechanism; 2) exactness --- finding a mechanism that is either not truthful or far from optimal; 3) domain dependence --- need a different design for different environment settings.</par><par>To resolve the three difficulties, in this paper, we put forward a unified neural network based framework that automatically learns to design revenue optimal mechanisms. Our framework consists of a mechanism network that takes an input distribution for training and outputs a mechanism, as well as a buyer network that takes a mechanism as input and output an action. Such a separation in design mitigates the difficulty to impose incentive compatibility constraints on the mechanism, by making it a rational choice of the buyer. As a result, our framework easily overcomes the previously mentioned difficulty in incorporating IC constraints and always returns exactly incentive compatible mechanisms.</par><par>We then applied our framework to a number of multi-item auction design settings, for a few of which the theoretically optimal mechanisms are unknown. We then go on to theoretically prove that the mechanisms found by our framework are indeed optimal.",Mechanism design; Neural network; Revenue optimal,Weiran,,Shen,"IIIS, Tsinghua University",Pingzhong,,Tang,"IIIS, Tsinghua University",Song,,Zuo,Google Research,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0161,Local Core Stability in Simple Symmetric Fractional Hedonic Games,,"We initiate the study of \emph{local core} stability in \emph{simple symmetric fractional hedonic games}. The input is an unweighted undirected graph $G$ where vertices are the agents and edges model social connection (i.e., acquaintance) among agents. We assume that if there is an edge between two agents then they value $1$ each other otherwise they value $0$ each other, i.e., we consider the simple setting where an agent values $1$ all and only her acquaintances. A coalition structure is a partition of the agents into coalitions where the utility of an agent is equal to the number of agents inside her coalition that are valued $1$ divided by the size of the coalition. A coalition structure is in the \emph{core} if no subset of agents can strictly improve all their utility by forming a new coalition together. In \cite{DBLP:conf/atal/BrandlBS15} it is shown that simple symmetric fractional hedonic games may not admit a core stable coalition structure.
However, the fact that the core is required to be resilient to deviations by any groups of agents could be sometimes unrealistic, especially in systems with large populations.
In fact, it may be difficult that agents are able to coordinate each other in order to understand whether there is the possibility of deviating together.

Motivated by the above considerations, we define a relaxation of the core, called \emph{local core}. A coalition structure is in the local core if there is no subset of agents which (1) induces a clique in the graph $G$ and (2) such that all agents can improve their utility by forming a new coalition together.
We first show that any local core dynamics converges, which implies that a local core stable coalition structure always exists.
We then study its performance with respect to the classic utilitarian social welfare and provide tight and almost tight bounds on the local core price of anarchy and stability, respectively.","Coalition Formation Games; Hedonic Games; Local Core; Price of Anarchy; Price
of Stability",Raffaello,,Carosi,"Gran Sasso Science Institute, L'aquila",Gianpiero,,Monaco,University of L'Aquila,Luca,,Moscardelli,University of Chieti-Pescara,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0170,Group Segregation in Social Networks,,"We study strategic behaviour in heterogeneous network formation, where agents are grouped into types and can choose to create or sever links whilst maximising their own private interest. We show the conditions under which social networks exhibit segregated behaviour by groups, as a function of the individual benefits and the costs of linking. By introducing the idea of an individual having a degree of 'tolerance' for others not of their own type, we further show that this enriched framework is able to generate sophisticated intra-group segregation, where a group can shun one of its own members due to the connections that member has. Moreover, we find through simulations that group segregation is an endemic feature and that, as the cost of linking increases, networks converging to a stable state exhibit common characteristics with growing certainty.",Social Networks; Jackson-Wolinsky Model; Group Segregation,Dominic,,Aits,Imperial College,Alexander,,Carver,Imperial College,Paolo,,Turrini,University of Warwick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0172,Egocentric Bias and Doubt in Cognitive Agents,,"Modeling social interactions based on individual behavior has always been an area of interest, but prior literature generally presumes rational behavior.  Thus, such models may miss out on capturing the effects of biases humans are susceptible to.  This work presents a method to model egocentric bias, the real-life tendency to emphasize one's own opinion heavily when presented with multiple opinions. We use a symmetric distribution centered at an agent's own opinion, as opposed to the Bounded Confidence (BC) model used in prior work.  We consider a game of iterated interactions where an agent cooperates based on its opinion about an opponent.  Our model also includes the concept of domain-based self-doubt, which varies as the interaction succeeds or not.  An increase in doubt makes an agent reduce its egocentricity in subsequent interactions, thus enabling the agent to learn reactively.  The agent system is modeled with factions not having a single leader, to overcome some of the issues associated with leader-follower factions.  We find that agents belonging to factions perform better than individual agents.  We observe that an intermediate level of egocentricity helps the agent perform at its best, which concurs with conventional wisdom that neither overconfidence nor low self-esteem brings benefits.",egocentric bias; cognitive psychology; doubt; factions; Continuous Prisoner's Dilemma; opinion aggregation,Nanda Kishore,,Sreenivas,Oracle,Shrisha,,Rao,"International Institute of Information Technology, Bangalore",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0175,RLBOA: A Modular Reinforcement Learning Framework for Autonomous Negotiating Agents,,"Negotiation is a complex problem, in which the variety of settings and opponents that may be encountered prohibits the use of a single predefined negotiation strategy. Hence the agent should be able to learn such a strategy autonomously. To this end we propose RLBOA, a modular framework that facilitates the creation of autonomous negotiation agents using reinforcement learning. The framework allows for the creation of agents that are capable of negotiating effectively in many different scenarios. To be able to cope with the large size of the state and action spaces and diversity of settings, we leverage the modular BOA-framework. This decouples the negotiation strategy into a Bidding strategy, an Opponent model and an Acceptance condition. Furthermore, we map the multidimensional contract space onto the utility axis which enables a compact and generic state and action description. We demonstrate the value of the RLBOA framework by implementing an agent that uses tabular Q-learning on the compressed state and action space to learn a bidding strategy. We show that the resulting agent is able to learn well-performing bidding strategies in a range of negotiation settings and is able to generalize across opponents and domains.","Bargaining and negotiation; Learning agent-to-agent interactions (negotiation, trust, coordination); Reinforcement Learning",Jasper,,Bakker,University of Amsterdam,Aron,,Hammond,University of Amsterdam,Daan,,Bloembergen,Centrum Wiskunde & Informatica,Tim,,Baarslag,Centrum Wiskunde & Informatica,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0177,On the Performance of Stable Outcomes in Modified Fractional Hedonic Games with Egalitarian Social Welfare,,"In this paper we consider {\em modified fractional hedonic games}, that are coalition formation games defined over an undirected edge-weighted graph $G=(N,E,w)$, where $N$ is the set of agents and for any edge $\{u,v\} \in E$, $w_{u,v}=w_{v,u}$ reflects how much agents $u$ and $v$ benefit from belonging to the same coalition.
More specifically, given a coalition structure, i.e., a partition of the agents into coalitions, the utility of an agent $u$ is given by the sum of $w_{u,v}$ over all other agents $v$ belonging to the same coalition of $u$ averaged over all other members of that coalition, i.e., excluding herself.

We focus on common stability notions: we are interested in strong Nash stable, Nash stable and core stable outcomes.
In \cite{MMV18}, the existence of these natural outcomes for modified fractional hedonic games is completely characterized; moreover, many tight or asymptotically tight results on their performance are shown for the classical utilitarian social welfare function, that is defined as the sum of all agents' utilities.

Motivated by the fact that an outcome with an high utilitarian social welfare could be extremely harsh for some agents, we provide a comprehensive analysis on the performance of strong Nash stable, Nash stable and core stable outcomes for modified fractional hedonic games under the egalitarian social welfare function, that is defined as the minimum among all agents' utilities.",Coalition Formation Games; Hedonic Games; Nash; Core; Egalitarian social welfare; Price of Anarchy; Price of Stability,Gianpiero,,Monaco,University of L'Aquila,Luca,,Moscardelli,University of Chieti-Pescara,Yllka,,Velaj,CWI Amsterdam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0180,Malthusian Reinforcement Learning,,"Here we explore a new algorithmic framework for multi-agent reinforcement learning, called Malthusian reinforcement learning, which extends self-play to include fitness-linked population size dynamics that drive ongoing innovation. In Malthusian RL, increases in a subpopulation's average return drive subsequent increases in its size, just as Thomas Malthus argued in 1798 was the relationship between preindustrial income levels and population growth. Malthusian reinforcement learning harnesses the competitive pressures arising from growing and shrinking population size to drive agents to explore regions of state and policy spaces that they could not otherwise reach. Furthermore, in environments where there are potential gains from specialization and division of labor, we show that Malthusian reinforcement learning is better positioned to take advantage of such synergies than algorithms based on self-play.",Intrinsic motivation; Adaptive radiation; Demography; Evolution; Artificial general intelligence,Joel,Z.,Leibo,DeepMind,Julien,,Perolat,DeepMind,Edward,,Hughes,DeepMind,Steven,,Wheelwright,DeepMind,Adam,H.,Marblestone,DeepMind,Edgar,,Duéñez-Guzmán,DeepMind,Peter,,Sunehag,DeepMind,Iain,,Dunning,DeepMind,Thore,,Graepel,DeepMind,,,,,,,,,,,,
fp0184,Optimal Control of Complex Systems through Variational Inference with a Discrete Event Decision Process,,"Complex social systems are composed of interconnected individuals whose interactions result in group behaviors. Optimal control of a real-world complex system has many applications, including road traffic management, epidemic prevention, and information dissemination. However, such real-world complex system control is difficult to achieve because of high-dimensional and non-linear system dynamics, and the exploding state and action spaces for the decision maker. Prior methods can be divided into two categories: simulation-based and analytical approaches. Existing simulation approaches have high-variance in Monte Carlo integration, and the analytical approaches suffer from modeling inaccuracy. We adopted simulation modeling in specifying the complex dynamics of a complex system, and developed analytical solutions for searching optimal strategies in a complex network with high-dimensional state-action space. To capture the complex system dynamics, we formulate the complex social network decision making problem as a discrete event decision process. To address the curse of dimensionality and search in high-dimensional state action spaces in complex systems, we reduce control of a complex system to variational inference and parameter learning, introduce Bethe entropy approximation, and develop an expectation propagation algorithm. Our proposed algorithm leads to higher system expected rewards, faster convergence, and lower variance of value function in a real-world transportation scenario than state-of-the-art analytical and sampling approaches.","Discrete event decision process; variational inference, Markov decision process; complex social systems; transportation",Fan,,Yang,University at Buffalo,Bo,,Liu,Auburn University,Wen,,Dong,University at Buffalo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0187,Supple: Multiagent Communication Protocols with Causal Types,,"A (communication) protocol captures how agents collaborate by specifying the messages they exchange. In particular, since the information content of messages characterizes the interactions a protocol specifies, message types can improve collaboration by strengthening the specification of what each agent may legitimately expect from another agent. In addition, in implementations, typing information can enable improved verification of agents.

We introduce Supple, a protocol specification language that expresses message schemas with typed parameters. Supple enables definition of causal types for parameters that constrain how other parameters are computed in a protocol enactment. We give the formal semantics of Supple; characterize the liveness and safety of Supple specifications; and provide decision procedures for them.
",interaction; protocols; norms; autonomy; information; decentralization; asynchronous communication; liveness; safety,Akin,,Günay,Lancaster University,Amit,K.,Chopra,Lancaster University,Munindar,P.,Singh,North Carolina State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0192,Microscopic Traffic Simulation by Cooperative Multi-agent Deep Reinforcement Learning,,"Expert human drivers perform actions relying on traffic laws and their previous experience.
While traffic laws are easily embedded into an artificial brain, modeling human complex behaviors which come from past experience is a more challenging task.
One of these behaviors is the capability of communicating intentions and negotiating the right of way through driving actions, as when a driver is entering a crowded roundabout and observes other cars movements to guess the best time to merge in.
In addition, each driver has its own unique driving style, which is conditioned by both its personal characteristics, such as age and quality of sight, and external factors, such as being late or in a bad mood.
For these reasons, the interaction between different drivers is not trivial to simulate in a realistic manner.
In this paper, this problem is addressed by developing a microscopic simulator using a Deep Reinforcement Learning Algorithm based on a combination of visual frames, representing the perception around the vehicle, and a vector of numerical parameters.
In particular, the algorithm called Asynchronous Advantage Actor-Critic has been extended to a multi-agent scenario in which every agent needs to learn to interact with other similar agents.
Moreover, the model includes a novel architecture such that the driving style of each vehicle is adjustable by tuning some of its input parameters, permitting to simulate drivers with different levels of aggressiveness and desired cruising speeds.",multi-agent systems; microscopic traffic simulation; agent cooperation and negotiation; deep reinforcement learning,Giulio,,Bacchiani,VisLab & University of Parma,Daniele,,Molinari,VisLab,Marco,,Patander,VisLab,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0193,Resource-bounded ATL: the Quest for Tractable Fragments,,"Resource-aware logics to represent strategic abilities in multi-agent systems are notoriously hard to handle as they combine strategic reasoning with reasoning about resources. In this work, we begin by providing a general overview of the model-checking results currently available for the Resource-bounded Alternating-time Temporal Logic RB±ATL. This allows us to identify several open problems in the literature, as well as to establish relationships with RBTL-like logics, when RB±ATL is restricted to a single agent. Then, we tackle one such open problem that we deem highly significant: we show that model checking RB±ATL is ptime-complete when restricted to a single agent and a single resource. To do so, we make a valuable detour on vector addition systems with states, by
proving new complexity results for their state-reachability and non-termination problems, when restricted to a single counter. Thus, reasoning about resources comes at no computational extra cost in the single-resource, single-agent case.",[Agent Theories and Models] Logics for agents and multi-agent systems; [Verification and Validation of Agent-based Systems] Verification techniques for multiagent systems; model checking,Francesco,,Belardinelli,Imperial College London & Université d'Evry,Stéphane,,Demri,"LSV, CNRS, ENS Paris-Saclay, Université Paris-Saclay",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0194,Coordinating the Crowd: Inducing Desirable Equilibria in Non-Cooperative Systems,,"Many real-world systems such as taxi systems, traffic networks and smart grids involve self-interested actors that perform individual tasks in a shared environment. However, in such systems, the self-interested behaviour of agents produces welfare inefficient and globally suboptimal outcomes that are detrimental to all - common examples are congestion in traffic networks, demand spikes for resources in electricity grids and over-extraction of environmental resources such as fisheries. We propose an incentive-design method which modifies agents' rewards in non-cooperative multi-agent systems that results in independent, self-interested agents choosing actions that produce optimal system outcomes in strategic settings. Our framework combines multi-agent reinforcement learning to simulate (real-world) agent behaviour and black-box optimisation to determine the optimal modifications to the agents' rewards or incentives given some fixed budget that results in optimal system performance. By modifying the reward functions and generating agents' equilibrium responses in a sequence of offline Markov games, our method enables optimal incentive structures to be determined offline through iterative updates of the reward functions of a simulated game. Our theoretical results show that our method converges to reward modifications that induce system optimality. We demonstrate the applications of our framework by tackling a challenging problem in economics that involves thousands of selfish agents and tackle a traffic congestion problem.",Multi-agent systems; incentive design; principal-agent; potential games; reinforcement learning; Bayesian optimisation; Markov decision process,David,,Mguni,PROWLER.io,Joel,,Jennings,PROWLER.io,Emilio,,Sison,Massachusetts Institute of Technology,Sergio,,Valcarcel Macua,PROWLER.io,Sofia,,Ceppi,PROWLER.io,Enrique,,Munoz de Cote,PROWLER.io,,,,,,,,,,,,,,,,,,,,,,,,
fp0195,What If I Speak Now? A Decision-Theoretic Approach to Personality-Based Turn-Taking,,"Embodied conversational agents, which are increasingly prevalent in our society, require turn-taking mechanisms that not only generate fluent conversations but are also consistent with the personality and interpersonal stance required in the given context.
We present a decision-theoretic approach for deriving the turn-taking behavior of such an agent from the personality it is meant to convey. For this we gathered relevant theories from psychology and communications research, as well as related systems employing utility-based reasoning. On this basis we describe the construction of an influence diagram which decides between acting and waiting based on those actions' expected utility for the agent's personality-related interaction goals.
To test our approach, we integrated our model into an application which simulates conversations between two virtual characters. We then evaluated our prototype by presenting videos of those conversations in an online survey. Our results confirmed that differences in an agent's speaking behavior, generated from different Extraversion configurations in our model, lead to the intended perceptions of its Extraversion, Agreeableness and Status.",embodied conversational agents; personality modeling; interpersonal stance; turn-taking conflicts; interruptions; decision-theoretic approach; influence diagram; Bayesian network,Kathrin,,Janowski,University of Augsburg,Elisabeth,,André,University of Augsburg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0198,Trusted AI and the Contribution of Trust Modeling in Multiagent Systems,,"Researchers in the field of artificial intelligence today are increasingly concerned with whether the systems which they build will be ``trusted AI"", in other words, whether they will be accepted by their human users. The claim of this paper is that these researchers should be aware of the rich set of solutions being developed in the multiagent systems subfield of trust modeling. We propose a specific perspective on how to leverage trust modeling solutions towards assurances for trusted artificial intelligence. We conclude by advocating for greater dialogue between these AI communities.",Agent Societies; Trust and Reputation,Robin,,Cohen,University of Waterloo,Mike,,Schaekermann,University of Waterloo,Sihao,,Liu,University of Waterloo,Michael,,Cormier,University of Waterloo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0199,Active Attention-Modified Policy Shaping,Socially Interactive Agents Track,"We present the Active Attention-Modified Policy Shaping (Active AMPS) algorithm, which allows learning robots to request feedback from multi-tasking human teachers. Active AMPS uses Reinforcement Learning supplemented with feedback from teachers, while avoiding frequently interrupting the teacher. This algorithm does so by selectively asking for attention from teachers in low-information areas of the state space when there is uncertainty about the teacher's feedback. Active AMPS allows people to take breaks from teaching the robot to complete other tasks, and is forgiving to lapses in human attention if learning occurs over long periods of time. We test Active AMPS both in simulation and on a physical robot in a human study. In simulation, we find that Active AMPS outperforms Attention-Modified Policy Shaping (AMPS), achieving an 11.0% increase in area under its learning curve while receiving 89.9% less feedback. In the human study, we find statistically significant results showing that Active AMPS allows people to complete 77.5% more work than AMPS while the robot receives 48.5% less feedback, without decreasing performance.",human-robot interaction; reinforcement learning; active learning,Taylor,,Kessler Faulkner,University of Texas at Austin,Reymundo,A.,Gutierrez,University of Texas at Austin,Elaine,Schaertl,Short,University of Texas at Austin,Guy,,Hoffman,Cornell University,Andrea,L.,Thomaz,University of Texas at Austin,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0205,Goal Recognition for Rational and Irrational Agents,,"Contemporary cost-based goal-recognition assumes rationality: that observed behaviour is more or less optimal. Probabilistic systems, however, generate probability distributions on the basis of suboptimality. We show that, when an observed agent is only slightly irrational (suboptimal), state-of-the-art systems produce counter-intuitive results. We present a definition of rationality appropriate to situations where the ground truth is unknown, define a rationality measure (RM) that quantifies an agent's expected degree of suboptimality, and present a novel self-modulating probability distribution formula for goal recognition. Our formula recognises suboptimality and adjusts its level of confidence accordingly, thereby handling irrationality---and rationality---in an intuitive, principled manner.",planning; plan/goal recognition; agent reasoning,Peta,,Masters,RMIT University,Sebastian,,Sardina,RMIT University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0206,PLOTS: Procedure Learning from Observations using subTask Structure,,"In many cases an intelligent agent may want to learn how to mimic a single observed demonstrated trajectory. In this work we consider how to perform such procedural learning from observation, which could help to enable agents to better use the enormous set of video data on observation sequences. Our approach exploits the properties of this setting to incrementally build an open loop action plan that can yield the desired subsequence, and can be used in both Markov and partially observable Markov domains. In addition, procedures commonly involve repeated extended temporal action subsequences.
Our method optimistically explores actions to leverage potential repeated structure in the procedure. In comparing to some state-of-the-art approaches we find that our explicit procedural learning from observation method is about 100 times faster than policy-gradient based approaches that learn a stochastic policy and is faster than model based approaches as well. We also find that performing optimistic action selection yields substantial speed ups when latent dynamical structure is present.",Reinforcement Learning; Learning from Demonstration; Behavior Cloning; Hierarchy,Tong,,Mu,Stanford University,Karan,,Goel,Stanford University,Emma,,Brunskill,Stanford University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0209,Dynamic Source Weight Computation for Truth Inference over Data Streams,,"Truth inference, a method that resolves conflicts among multi-agent data, has been widely studied in the field of AI. Most existing truth inference methods use iterative approaches to achieve high accuracy, but are inefficient to infer object truths over data streams. The methods developed for streaming data can achieve high efficiency but suffer from low accuracy. In this paper, we propose a novel truth inference method, Dynamic Source Weight Computation truth inference (DSWC), that can work with a wide range of iterative-based truth inference methods to dynamically compute source weights over data streams. Specifically, we use Taylor expansion to analyze the unit error of object truths inferred by source weights computed at a previous timestamp. If the source weight at present is predicted to be able to limit the error under a threshold, we use the source weights computed previously to approximate object truths at present to avoid the expensive source weight computation step. Compared with the existing work, the proposed method is more effective in predicting source weights and can be applied to a wider range of applications. Experimental results based on four real-world datasets demonstrate that DSWC is both accurate and efficient for truth inference over data streams.",Truth Inference;Multi-agent Reliabilities,Yi,,Yang,Auckland University of Technilogy,Quan,,Bai,University of Tasmania,Qing,,Liu,CSIRO,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0215,Interleaved Q-Learning with Partially Coupled Training Process,,"This paper studies estimating the maximum expected value (MEV) of several independent random variables (RVs). No unbiased estimator exists without knowing the distributions of those RVs a priori. Two of the most famous estimators, maximum estimator (ME) and double estimator (DE), yield positive bias and negative bias respectively. We propose a coupled estimator (CE) which subsumes ME and DE as special cases and yields a bias between that of ME and DE, while maintaining the same variance bound. Furthermore, a simple yet effective variance reduction technique is proposed and verified in the experiments. The instantiated algorithm in the Markov decision process (MDP) setting, called interleaved Q-learning, outperforms Q-learning and double Q-learning in some highly stochastic environments. Insights on how to adapt the coupling ratio in CE and hence make interleaved Q-learning automatically shift between Q-learning and double Q-learning are provided and verified in the experimental section.",Maximum Estimator; Double Estimator; Coupled Estimator; Estimation Bias; Q-Learning; Double Q-Learning; Interleaved Q-learning,Min,,He,University of Electronics Science and Technology of China,Hongliang,,Guo,University of Electronic Science and Technology of China School of Automation Engineering,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0219,A Child and a Robot Getting Acquainted - Interaction Design for Eliciting Self-Disclosure,,"In order to facilitate a sustainable long-term interaction between a child and a robot they need to get acquainted with one another. In this paper we discuss the foundation, the rationale, and the evaluation (N = 75) of our design for an autonomous robot conversational partner that engages with Dutch children (8-11 y.o.) in a getting acquainted interaction. The main objective of the robot is to elicit children to self-disclose.

Firstly, we discuss five interaction design patterns (IDPs) that proved to be successful in autonomously eliciting and processing self-disclosures. Secondly, we compared two robot behavior profiles. The behavior profiles can be relatively considered as being more and less energetic. We manipulated the movement speed, the speech rate and volume, the use of high/low energy language, waiting time before responding, and the order of high/low energy activities. Results show that the less energetic behavior profile significantly leads to more self-disclosure.",Child-Robot Interaction; Social Robots; Interaction Design Patterns,Mike,,Ligthart,Vrije Universiteit Amsterdam,Timo,,Fernhout,Delft University of Technology,Mark,A.,Neerincx,Delft University of Technology & TNO Soesterberg,Kelly,L. A.,van Bindsbergen,"Emma Children's Hospital, Amsterdam UMC & Princess Máxima Center for pediatric oncology",Martha,A.,Grootenhuis,Princess Máxima Center for pediatric oncology,Koen,V.,Hindriks,Vrije Universiteit Amsterdam,,,,,,,,,,,,,,,,,,,,,,,,
fp0221,Reasoning about Changes of Observational Power in Logics of Knowledge and Time,,"We study dynamic changes of agents’ observational power in logics of knowledge and time. We consider CTL*K, the extension of CTL* with knowledge operators, and enrich it with a new operator that models a change in an agent’s way of observing the system. We extend the classic semantics of knowledge for agents with perfect recall to account for changes of observational power, and we show that this new operator increases the expressivity of CTL*K. We reduce the model-checking problem for our logic to that for CTL*K, which is known to be decidable. This provides a solution to the model-checking problem for our logic, but it is not optimal, and we provide a direct model-checking procedure with better complexity.
",Model checking; Knowledge and time; Epistemic temporal logics,Aurèle,,Barrière,ENS Rennes,Bastien,,Maubert,Università degli Studi di Napoli,Aniello,,Murano,University of Napoli Federico II,Sasha,,Rubin,University of Naples,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0244,Unsupervised Role Discovery Using Temporal Observations of Agents,,"Agent-based modeling of multi-agent systems has enormous potential with applications in modeling social, economic, medical and other application domains containing temporal data. We propose an unsupervised approach to discovering common roles by observing agents over time, allowing us to construct a role-based representation of multi-agent systems that aids in understanding and interpreting the state of the system. We validate our approach on both a soccer and a StarCraft dataset, and show that unsupervised role discovery through observation can provide meaningful insight into the state of a multi-agent system, aiding or even replacing game state data for interpretation or understanding of the system.",Multi-agent systems; Unsupervised learning; Temporal learning; Interpretability,Andrew,,Silva,Georgia Institute of Technology,Sonia,,Chernova,Georgia Institute of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0247,Online Resource Allocation with Matching Constraints,,"Matching markets with historical data are abundant in many applications, \emph{e.g.,} matching candidates to jobs in hiring, workers to tasks in crowdsourcing markets, and jobs to servers in cloud services. In all these applications, a match consumes one or more \emph{shared} and \emph{limited} resources and the goal is to best utilize these to maximize a global objective. Additionally, one often has historical data and hence some statistics (usually first-order moments) of the arriving agents (\emph{e.g.,} candidates, workers, and jobs) can be learnt. To model these scenarios, we propose a unifying framework, called \emph{Multi-Budgeted Online Assignment with Known Adversarial Distributions}. In this model, we have a set of \emph{offline} servers with different deadlines and a set of \emph{online} job types. At each time, a job of type $j$ arrives. Assigning this job to a server $i$ yields a profit $w_{i, j}$ while consuming $\mathbf{a}_e \in [0, 1]^K$ quantities of distinct resources. The goal is to design an (online) assignment policy that maximizes the total expected profit without violating the (hard) budget constraint. We propose and theoretically analyze two linear programming (LP) based algorithms which are almost optimal among all LP-based approaches. We also propose several heuristics adapted from our algorithms and compare them to other LP-agnostic algorithms using both synthetic as well as real-time cloud scheduling and public safety datasets. Experimental results show that our proposed algorithms are effective and \emph{significantly} out-perform the baselines. Moreover, we show empirically the trade-off between \emph{fairness} and \emph{efficiency} of our algorithms which does well even on fairness metrics without explicitly optimizing for it.
",Online Scheduling; Online Matching; Randomized Algorithms; Fairness,John,P.,Dickerson,University of Maryland,Karthik,Abinav,Sankararaman,University of Maryland College Park,Kanthi,Kiran,Sarpatwar,IBM Research AI,Aravind,,Srinivasan,"University of Maryland, College Park",Kun-Lung,,Wu,IBM Research AI,Pan,,Xu,"University of Maryland, College Park",,,,,,,,,,,,,,,,,,,,,,,,
fp0255,Community Regularization of Visually-Grounded Dialog,,"The task of conducting visually grounded dialog involves learning goal-oriented cooperative dialog between autonomous agents who exchange information about a scene through several rounds of questions and answers in natural language. We posit that requiring artificial agents to adhere to the rules of human language, while also requiring them to maximize information exchange through dialog is an ill-posed problem. We observe that humans do not stray from a common language because they are social creatures who live in communities, and have to communicate with many people everyday, so it is far easier to stick to a common language even at the cost of some efficiency loss. Using this as inspiration, we propose and evaluate a multi-agent community-based dialog framework where each agent interacts with, and learns from, multiple agents, and show that this community-enforced regularization results in more relevant and coherent dialog (as judged by human evaluators) without sacrificing task performance (as judged by quantitative metrics).",Visual Dialog; Multi-Agent Reinforcement Learning; Curriculum Learning; Emergent Communication,Akshat,,Agarwal,Carnegie Mellon University,Swaminathan,,Gurumurthy,Carnegie Mellon University,Vasu,,Sharma,Carnegie Mellon University,Mike,,Lewis,University of Pittsburgh,Katia,,Sycara,Carnegie Mellon University,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0257,The Volatility of Weak Ties: Co-evolution of Selection and Influence in Social Networks,,"In this work we look at opinion formation and the effects of two phenomena both of which promote consensus between agents connected by ties: \emph{influence}, agents changing their opinions to match their neighbors; and \emph{selection}, agents re-wiring to connect to new agents when the existing neighbor has a different opinion.
In our agent-based model, we assume that only weak ties can be rewired and strong ties do not change. The network structure as well as the opinion landscape thus co-evolve with two important parameters: the probability of influence versus selection; and the fraction of strong ties versus weak ties.
Using empirical and theoretical methodologies we discovered that on a two-dimensional spatial network:

\begin{itemize}
\item With no/low selection the presence of weak ties enables fast consensus. This conforms with the classical theory that weak ties are helpful for quickly mixing and spreading information, and strong ties alone act much more slowly.
\item With high selection, too many weak ties inhibit any consensus at all---the graph partitions. The weak ties reinforce the differing opinions rather than mixing them. However, sufficiently many strong ties promote convergence, though at a slower pace.
\end{itemize}
We additionally test the aforementioned results using a real network. Our study relates two theoretical ideas: the strength of weak ties---that weak ties are useful for spreading information; and the idea of echo chambers or filter bubbles, that people are typically bombarded by the opinions of like-minded individuals. The difference is in how (much) selection operates.",Opinion formation; Polarization; Echo chambers; Tie strength; Social network structure,Jie,,Gao,Stony Brook University,Grant,,Schoenebeck,University of Michigan,Fang-Yi,,Yu,University of Michigan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0258,Heterogeneous Two-facility Location Games with Minimum Distance Requirement,,"We study the mechanism design problem of a social planner for locating two heterogeneous facilities on a line interval $[0,1],$ where a set of $n$ strategic agents report their locations and a mechanism determines the locations of the two facilities.
Unlike prior work on two-facility location games, we consider the requirement of the minimum distance $d$ between the two facilities.
As the two facilities are heterogeneous and have additive effects on agents, we model
that the cost of an agent is the sum of his distances to both facilities and the social cost is the total cost of all agents.
In the two-facility location game to minimize the social cost,
we show that the optimal solution can be computed in polynomial time and prove that carefully choosing one optimal solution as output is strategyproof.
In the obnoxious two-facility location game for maximizing the social utility, a mechanism outputting the optimal solution is not strategyproof and we propose new deterministic group strategyproof mechanisms with provable approximation ratios.
Moreover, we establish a lower bound $\frac{7-d}{6}$ for
the approximation ratio achievable by deterministic strategyproof mechanisms.
Finally, we study the two-facility location game with triple-preference, where each of the two facilities may be favorable, obnoxious, indifferent for any agent.
We further allow each agent to misreport his location and preference towards the two facilities and design a deterministic group strategyproof mechanism with approximation ratio $4$.",approximation algorithms; minimum distance; mechanism design; facility location,Lingjie,,Duan,Singapore University of Technology and Design,Bo,,Li,Stony Brook University,Minming,,Li,City University of Hong Kong,Xinping,,Xu,Singapore University of Technology and Design,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0277,Optimal Value of Information Based Elicitation During Negotiation,,"Autonomous agents engaging in automatic negotiations on behalf of humans or institutions are usually assumed to have full knowledge of the utility function for the actors they represent. In many cases, these utility functions are difficult to know apriori for every possible outcome of the negotiation. Moreover, it may not be necessary for the agent to know the utility of outcomes that are never offered or considered during the negotiation. State-of-the-art approaches to utility elicitation during negotiation assume that the agent can ask \emph{questions} from a predefined countable set to reduce its uncertainty about the utility function. This paper extends that body of work by lifting the countability assumption providing an optimal algorithm for selecting the best outcome and utility level about which to ask the actor. The paper reports the results of comparing the proposed algorithm with state-of-the-art algorithms using both synthetic and realistic negotiation scenarios. These evaluations support the applicability of the proposed approach.",Autonomous Negotiation; Utility Elicitation; Probabilistic Inference; Preference Elicitation,Yasser,,Mohammad,AIST & Assiut University,Shinji,,Nakadai,NEC-AIST Collaboration Laboratory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0278,Modeling  People's  Voting Behavior with Poll Information,,"Despite the prevalence of voting systems in the real world there is no consensus among researchers of how people vote strategically, even in simple voting settings. This paper addresses this gap by comparing different approaches that have been used to model strategic voting, including expected utility maximization, heuristic decision-making, and bounded
rationality models.
The models are applied to data collected from hundreds of people in controlled voting experiments, where people vote after observing non-binding poll information.
We introduce a new voting model, the Attainability-Utility (AU) heuristic, which weighs the popularity of a candidate according to the poll, with the utility of the candidate to the voter.
We argue that the AU model is cognitively plausible, and show that it is able to predict people's voting behavior
significantly better than other models from the literature. It was almost at par with (and sometimes better than) a machine learning algorithm that uses substantially more information.
Our results provide new insights into the strategic considerations of voters, that undermine the prevalent assumptions of much theoretical work in social choice.",Social choice theory;  Behavioral game theory; Coordination and control; multiagent systems,Roy,,Fairstein,Ben Gurion University of the Negev,Adam,,Lauz,Ben Gurion University of the Negev,Reshef,,Meir,"Technion, Israel Institute of Technology",Kobi,,Gal,Ben-Gurion Univ. of the Negev & University of Edinburgh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0281,Towards Completing the Puzzle: Solving Open Problems for Control in Elections,,"We investigate the computational complexity of electoral control in elections. Electoral control describes the scenario where the election chair seeks to alter the outcome of the election by structural changes such as adding, deleting, or replacing either candidates or voters. Such control actions have been studied in the literature for a lot of prominent voting correspondences. In this paper, we complement those results by solving several open cases for Copeland$^{\alpha}$, Maximin, $k$-Veto,  Plurality with Runoff, and Veto with Runoff.",voting control; complexity; veto with runoff; plurality with runoff,Gábor,,Erdélyi,University of Canterbury,Christian,,Reger,University of Siegen,Yongjie,,Yang,Saarland University & Central South University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0282,Computing Optimal <i>Ex Ante</i> Correlated Equilibria in Two-Player Sequential Games,,"We investigate the computation of equilibria in extensive-form games when ex ante correlation is possible, focusing on correlated equilibria requiring the least amount of communication between the players and the mediator. Motivated by hardness results on normal-form correlated equilibria, we investigate whether it is possible to compute normal-form coarse correlated equilibria efficiently. We show that an optimal (e.g., social welfare maximizing) normal-form coarse correlated equilibrium can be computed in polynomial time in two-player games without chance moves, and that in general multi-player games (including two-player games with chance) the problem is NP-hard. For the two-player case, we provide both a polynomial-time algorithm based on the ellipsoid method and a column generation algorithm based on the simplex method which can be efficiently applied in practice. We also show that the pricing oracle employed in the column generation procedure can be extended to games with two players and chance.",Equilibrium computation; correlated equilibrium,Andrea,,Celli,Politecnico di Milano,Stefano,,Coniglio,University of Southampton,Nicola,,Gatti,Politecnico di Milano,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0286,Cooperation via Codes in Restricted Hat Guessing Games,,"
Hat guessing games have drawn a lot of attention among mathematicians, computer scientists, coding theorists and even the mass press, due to their relations to graph theory, circuit complexity, network coding, and auctions. In this paper, we investigate a new variant where there is exactly one hat of each color and where each player may receive multiple hats. Assume there are $n$ players and $T$ hats with different colors. A dealer randomly places $k$ hats to each player and holds $T-nk$ hats in hand. After observing the (colors of) hats of other players but not those of themselves, the players shall guess their colors simultaneously by a pre-coordinated strategy. We present methods to compute the best strategy under two common winning rules: \emph{all guesses are right} or \emph{at least one guess is right}, and derive exact value of the maximum winning probability for several cases. Especially, we introduce a novel notion called Latin matching between ${[2n-1]\choose n-1}$ and ${[2n-1]\choose n}$ and establish its connection to the solution of some restricted cases.
Here, ${[2n-1]\choose n-1}$ (respectively, ${[2n-1]\choose n}$) denotes the set of $(n-1)$-element (respectively, $n$-element) subsets of $\{1,\ldots,2n-1\}$. Moreover, we show that some well-known combinatorial results (e.g. the antipodal matching between two symmetric layers of the subset lattice and the ordered design $OD(t,k,v)$ given in modern design theory) can be applied to design explicit strategies in other cases.
From our results we observe an interesting phenomenon that a leader is necessary for consensus but unnecessary for decentralization.
",Agent cooperation; Hat guessing game; Theory of error-correcting code; Ordered design; Hamming code,Kai,,Jin,The Hong Kong University of Science and Technology,Ce,,Jin,Tsinghua University,Zhaoquan,,Gu,Guangzhou University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0287,Incentivizing Distributive Fairness for Crowdsourcing Workers,,"In a crowd market such as Amazon Mechanical Turk, the remuneration of Human Intelligence Tasks is determined by the requester, for which they are not given many cues to ascertain how to ``fairly'' pay their workers. Furthermore, the current methods for setting a price are mostly binary -- in that, the worker either gets paid or not -- as opposed to paying workers a ``fair'' wage based on the quality and utility of work completed. Instead, the price should better reflect the historical performance of the market and the requirements of the task. In this paper, we introduce a game theoretical model that takes into account a more balanced set of market parameters, and propose a pricing policy and a rating policy to incentivize requesters to offer ``fair'' compensation for crowdsourcing workers. We present our findings from applying and developing this model on real data gathered from workers on Amazon Mechanical Turk and simulations that we ran to validate our assumptions. Our simulation results also demonstrate that our policies motivate requesters to pay their workers more ``fairly'' compared with the payment set by the current market.",Crowdsourcing; human intelligence task; fair compensation,Chenxi,,Qiu,Rowan University,Anna,,Squicciarini,Pennsylvania State University,Benjamin,,Hanrahan,Pennsylvania State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0305,Task and Path Planning for Multi-Agent Pickup and Delivery,,"We study the offline Multi-Agent Pickup-and-Delivery (MAPD) problem, where a team of agents has to execute a batch of tasks with release times in a known environment. To execute a task, an agent has to move first from its current location to the pickup location of the task and then to the delivery location of the task. The MAPD problem is to assign tasks to agents and plan collision-free paths for them to execute their tasks. Online MAPD algorithms can be applied to the offline MAPD problem, but do not utilize all of the available information and may thus not be effective. Therefore, we present two novel offline MAPD algorithms that improve a state-of-the-art online MAPD algorithm with respect to task planning, path planning, and deadlock avoidance for the offline MAPD problem. Our MAPD algorithms first compute one task sequence for each agent by solving a special traveling salesman problem and then plan paths according to these task sequences. We also introduce an effective deadlock avoidance method, called ``reserving dummy paths.'' Theoretically, our MAPD algorithms are complete for well-formed MAPD instances, a realistic subclass of all MAPD instances. Experimentally, they produce solutions of smaller makespans and scale better than the online MAPD algorithm in simulated warehouses with hundreds of robots and thousands of tasks.",agent coordination; multi-agent path finding; path planning; pickup and delivery task; task assignment; traveling salesman problem,Minghua,,Liu,Tsinghua University,Hang,,Ma,University of Southern California,Jiaoyang,,Li,University of Southern California,Sven,,Koenig,University of Southern California,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0310,Approximation Algorithms for BalancedCC Multiwinner Rules,,"X-BalancedCC multiwinner voting rules constitute an attractive but computationally intractable compromise between the proportionality provided by the Monroe rule and the diversity provided by the Chamberlin--Courant rule. We show how to use the GreedyMonroe algorithm to get improved approximation results for the X-BalancedCC rules and for the Chamberlin--Courant rule, by appropriately setting a ""schedule"" for the sizes of virtual districts. We describe a polynomial-time algorithm for computing a schedule that guarantees high approximation ratio, but show that finding the best possible schedule for a given election is NP-hard. We further evaluate our algorithms experimentally and show that they perform very well in practice.",multiwinner elections; approximation algorithms;  Monroe rule; Chamberlin--Courant rule; greedy algorithms,Markus,,Brill,TU Berlin,Piotr,,Faliszewski,AGH University,Frank,,Sommer,Philipps-Universität Marburg,Nimrod,,Talmon,Ben-Gurion University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0315,Marginal Cost Pricing with a Fixed Error Factor in Traffic Networks,,"It is well known that charging \textit{marginal cost tolls} (MCT) from self interested agents participating in a congestion game leads to optimal system performance, i.e., minimal total latency. However, it is not generally possible to calculate the correct marginal costs tolls precisely, and it is not known what the impact is of charging incorrect tolls. This uncertainty could lead to reluctance to adopt such schemes in practice. This paper studies the impact of charging MCT with some fixed factor error on the system's performance.
We prove that under-estimating MCT results in a system performance that is at least as good as that obtained by not applying tolls at all. This result might encourage adoption of MCT schemes with conservative MCT estimations. Furthermore, we prove that no local extrema can exist in the function mapping the error value, $r$, to the system's performance, $T(r)$. This result implies that accurately calibrating MCT for a given network can be done by identifying an extremum in $T(r)$ which, consequently, must be the global optimum.
Experimental results from simulating several large-scale, real-life traffic networks are presented and provide further support for our theoretical findings.",Marginal-cost pricing; User equilibrium; Traffic flow optimization,Guni,,Sharon,Texas A&M University,Stephen,D.,Boyles,The University of Texas at Austin,Shani,,Alkoby,The University of Texas at Austin,Peter,,Stone,The University of Texas at Austin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0317,Experiential Preference Elicitation for Autonomous Heating and Cooling Systems,,"AI systems that act on behalf of users require knowledge of user preferences, which can be acquired by preference elicitation. In many settings, users can respond more easily and accurately to preference queries reflecting their current, or recently experienced, context (e.g., state of the environment), than to those reflecting contexts further removed. We develop and study a formal model of experiential elicitation (EE) in which query costs and response noise are state-dependent. EE settings tightly couple the problems of control and elicitation. We provide some analysis of this abstract model, and illustrate its applicability in household heating/cooling management. We propose the use of relative value queries,
asking the user to compare the immediate utility of two states, whose difficulty is related to the degree and recency of a user's experience with those states. We develop a Gaussian process-based approach for modeling user preferences in dynamic EE domains and show that it accrues higher reward than several natural baselines.",Single and multi-agent planning and scheduling; Multi-user/multi-virtual-agent interaction,Andrew,,Perrault,University of Southern California,Craig,,Boutilier,Google,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0322,Enforcing Equilibria in Multi-Agent Systems,,"We introduce and investigate Normative Synthesis: a new class of problems for the equilibrium verification that counters the absence of equilibria by purposely constraining multi-agent systems. We show that norms are powerful enough to ensure a positive answer to every instance of the equilibrium verification problem. Subsequently, we focus on two optimization versions, that aim at providing a solution in compliance with implementation costs. We show that the complexities of our procedures range between 2ExpTime and 3ExpTime, thus that the problems are no harder than the corresponding equilibrium verification ones.","Logics for Agents and Multi-Agent Systems; Synthesis of Agent-Based Systems; Verification Techniques for Multiagent Systems, including Model Checking",Giuseppe,,Perelli,University of Leicester,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0323,Optimal Online Coverage Path Planning with Energy Constraints,,"We consider the problem of covering an unknown polygonal environment possibly containing obstacles using a robot of square size $L\times L$.
The environment is structured as a grid with resolution proportional to the robot size $L\times L$, imposed on it. The robot has a limited energy budget -- it has to visit a charging station before it runs out of its energy; there is a single charging station in the environment. In a single time step, the robot can move from one grid cell to one of its four adjacent cells.
The energy budget $B$ allows the robot to travel at most $B$ distance, i.e., $B$ grid cells.
The objective of the robot is to minimize both total distance traveled to cover the environment (visit each cell of the environment not occupied by obstacles) and the number of visits to the charging station. In this paper, we present the \textit{first} online coverage
path planning algorithm that achieves $O(\log (B/L))$-approximation
for both objectives.
Our bound is optimal since there exists a lower bound of $\Omega(\log (B/L))$ for this problem for both objectives. Simulation results show the efficiency of our approach.
",Robotics; Coverage Path Planning; Energy Constraint; Unknown Environment; Approximation,Gokarna,,Sharma,Kent State University,Ayan,,Dutta,University of North Florida,Jong-Hoon,,Kim,Kent State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0324,Forecast-Based Mechanisms for Demand Response,,"We study mechanisms to incentivize demand response in smart energy systems. We assume agents that can respond (reduce their demand) with some probability if they prepare prior to the realization of the demand. Both preparation and response incur costs to agents. Previous work studies truthful mechanisms that select a minimal set of agents to prepare and respond such that a fixed demand reduction target is achieved with high probability. In this work we additionally consider the balancing responsibility of a retailer under a given demand forecast and imbalance price: the retailer is responsible to purchase additional reserve capacity at a high imbalance price to cover any excess in the demand. In this extended setting we study mechanisms that request only a subset of prepared agents to respond since the reduction target depends on the realization of the demand. We propose: (i) a sequential mechanism that in each round embeds a second-price auction and is truthful under some mild assumptions for the setting, and (ii) a truthful combinatorial mechanism that runs in polynomial time and uses VCG payments. We show that both mechanisms guarantee non-negative utility in expectation for both agents and the retailer (mechanism), and can further be used for simultaneous downward and upward flexibility. Last, we verify our theoretical findings in an empirical evaluation over a wide range of mechanism parameters.",mechanism design; demand response; demand forecast,Georgios,,Methenitis,Centrum Wiskunde & Informatica (CWI),Michael,,Kaisers,Centrum Wiskunde & Informatica (CWI),Han,,La Poutre,Centrum Wiskunde & Informatica (CWI),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0331,Anticipatory Bayesian Policy Selection for Online Adaptation of Collaborative Robots to Unknown Human Types,,"As a key component of collaborative robots (cobots) working with humans, existing decision-making approaches try to model the uncertainty in human behaviors as latent variables. However, as more possible contingencies are covered by such intention-aware models, they face slow convergence times and less accurate responses. For this purpose, we present a novel anticipatory policy selection mechanism built on existing intention-aware models, where a robot is required to choose from an existing set of policies based on an estimate of the human. Each of these intention-aware robot models anticipates and adapts to a different human's short-term changing behaviors. Our contribution is the Anticipatory Bayesian Policy Selection (ABPS) mechanism which selects from a library of different response policies that are generated from such models, and converges to a reliable policy after as few interactions as possible when faced with unknown humans. The selection is based on the estimation of the human in terms of long-term workplace characteristics that we call types, such as level of expertise, stamina, attention and collaborativeness. Our results show that incorporating this policy selection mechanism contributes positively to the efficiency and naturalness of the collaboration, when compared to the best intention-aware model in hindsight running alone.",Human-Robot Collaboration; Anticipatory Policy Selection; Human Type and Intent Inference; Socially Collaborative Robots,O. Can,,Görür,"DAI-Labor, Technische Universität Berlin",Benjamin,,Rosman,"CSIR, and University of the Witwatersrand",Sahin,,Albayrak,"DAI-Labor, Technische Universität Berlin",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0334,Robust Decentralised Agent Based Approach for Microgrid Energy Management,,"This paper introduces a decentralized agent based implementation of an off-line optimization technique applied to the configuration of electrical microgrids with distributed generation. This implementation conforms the ''Agent Based Energy Management System'' (ABEMS). Software agent principles are used to decentralize the microgrid control without having an implicit or explicit control hierarchy. Unlike most approaches, they work as peers. Orders to be sent to the energy micro-generation units are produced off-line by an evolutionary computation method. The orders are encoded into the agents once the micro-grid is started and agents decide when to apply them. The paper includes experiments that show the resulting behavior in several scenarios executed in real time. Results indicate the technique is stable enough and outperforms more simple approaches, a necessary initial step prior to a more comprehensive comparison with other solutions.",Multi-Agent System; Smart Grid; Optimisation;  Simulation; Decentralization,Sandra,,Garcia-Rodriguez,"CEA, LIST",Jorge,J.,Gomez-Sanz,Universidad Complutense de Madrid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0339,A Counter Abstraction Technique for the Verification of Probabilistic Swarm Systems,,We introduce a method for formally verifying properties of arbitrarily large swarms whose agents are modelled probabilistically. We define a parameterised probabilistic semantics for modelling swarms and observe that their verification problem against PLTL specifications is undecidable. We develop a partial procedure for verifying arbitrarily large swarms based on counter abstraction and show its correctness. We present an implementation and report the experimental results obtained when verifying a swarm foraging protocol.,Probabilistic verification; Parameterised model checking; Robot swarms,Alessio,,Lomuscio,Imperial College London,Edoardo,,Pirovano,Imperial College London,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0340,Revenue Maximization with Imprecise Distribution,,"We study the revenue maximization problem with an imprecisely estimated distribution of a single buyer or several independent and identically distributed buyers given that this estimation is not far away from the true distribution. We use the earth mover's distance to capture the estimation error between those two distributions in terms of both values and their probabilities, i.e., the error in value space given a quantile, and the error in quantile space given a value. We give explicit characterization of the optimal mechanisms for the single buyer setting. For the multi-buyer case, we provide an algorithm that finds an approximately optimal mechanism (FPTAS) among the family of second price mechanisms with a fixed reserve.",robust mechanism design; revenue maximization; earth mover's distance,Yingkai,,Li,Northwestern University,Pinyan,,Lu,Shanghai University of Finance and Economics,Haoran,,Ye,Shanghai Jiao Tong University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0346,Courtesy as a Means to Coordinate,,"We investigate the problem of multi-agent coordination under rationality constraints. Specifically, role allocation, task assignment, resource allocation, etc. Inspired by human behavior, we propose a framework (CA^3NONY) that enables fast convergence to efficient and fair allocations based on a simple convention of courtesy. We prove that following such convention induces a strategy which constitutes an epsilon-subgame-perfect equilibrium of the repeated allocation game with discounting. Simulation results highlight the effectiveness of CA^3NONY as compared to state-of-the-art bandit algorithms, since it achieves more than two orders of magnitude faster convergence, higher efficiency, fairness, and average payoff.",Multi-agent learning; Non-cooperative games: theory & analysis,Panayiotis,,Danassis,École Polytechnique Fédérale de Lausanne,Boi,,Faltings,École Polytechnique Fédérale de Lausanne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0352,Discriminatively Learning Inverse Optimal Control Models for Predicting Human Intentions,,"More accurately inferring human intentions/goals can help robots complete collaborative human-robot tasks more safely and efficiently. Bayesian reasoning has become a popular approach for predicting the intention or goal of a partial sequence of actions/controls using a trajectory likelihood model. However, the mismatch between the training objective for these models (maximizing trajectory likelihood) and the application objective (maximizing intention likelihood) can be detrimental. In this paper, we seek to improve the goal prediction of maximum entropy inverse reinforcement learning (MaxEnt IRL) models by training to maximize goal likelihood. We demonstrate the benefits of our method on pointing task goal prediction with multiple possible goals and predicting goal based activities in the Cornell Activity Dataset (CAD-120).",Goal Prediction; Intent Prediction; Inverse Reinforcement Learning; Maximum Likelihood Estimation,Sanket,,Gaurav,University of Illinois at Chicago,Brian,,Ziebart,University of Illinois at Chicago,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0353,Agent Embeddings: A Latent Representation for Pole-Balancing Networks,,"We show that it is possible to reduce a high-dimensional object like a neural network agent into a low-dimensional vector representation with semantic meaning that we call \textit{agent embeddings}, akin to word or face embeddings. This can be done by collecting examples of existing networks, vectorizing their weights, and then learning a generative model over the weight space in a supervised fashion. We investigate a pole-balancing task, Cart-Pole, as a case study and show that multiple \textit{new} pole-balancing networks can be generated from their agent embeddings without direct access to training data from the Cart-Pole simulator. In general, the learned embedding space is helpful for mapping out the space of solutions for a given task. We observe in the case of Cart-Pole the surprising finding that good agents make different decisions despite learning similar representations, whereas bad agents make similar (bad) decisions while learning dissimilar representations. Linearly interpolating between the latent embeddings for a good agent and a bad agent yields an agent embedding that generates a network with intermediate performance, where the performance can be tuned according to the coefficient of interpolation. Linear extrapolation in the latent space also results in performance boosts, up to a point.","Learning agent capabilities (agent models, communication, observation); Analysis of agent-based simulations; Simulation of complex systems; Deep learning",Oscar,,Chang,Columbia University in the City of New York,Robert,,Kwiatkowski,Columbia University in the City of New York,Siyuan,,Chen,Columbia University in the City of New York,Hod,,Lipson,Columbia University in the City of New York,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0354,Contingent Payment Mechanisms for Resource Utilization,,"We introduce the problem of assigning resources to improve their utilization, for settings where agents have uncertainty about their own values for using a resource, and where it is in the interest of the society or the planner that resources be used and not wasted. Done in the right way, improved utilization maximizes social welfare--- balancing the utility of a high value but unreliable agent with the group's preference that resources be used. We introduce the family of contingent payment mechanisms (CP), which may charge an agent contingent on use (a penalty). A CP mechanism is parameterized by a maximum penalty, and has a simple dominant-strategy equilibrium. Under a set of axiomatic properties, we establish welfare-optimality for the special case CP(W), with CP instantiated for a maximum penalty equal to societal value W for utilization. The special case with no upper bound on penalty, the contingent second-price mechanism, maximizes utilization. We extend the mechanisms to assign multiple, heterogeneous resources, and present a simulation study of the welfare properties of these mechanisms.",mechanism design; resource utilization; uncertainty,Hongyao,,Ma,Harvard University,Reshef,,Meir,Technion -- Israel Institute if Technology,David,C.,Parkes,Harvard University,James,,Zou,Stanford University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0355,TBQ(&#963;): Improving Efficiency of Trace Utilization for Off-Policy Reinforcement Learning,,"Off-policy reinforcement learning with eligibility traces faces is challenging because of the discrepancy between target policy and behavior policy. One common approach is to measure the difference between two policies in a probabilistic way, such as importance sampling and tree-backup. However, existing off-policy learning methods based on probabilistic policy measurement are inefficient when utilizing traces under a greedy target policy, which is ineffective for control problems. The traces are cut immediately when a non-greedy action is taken, which may lose the advantage of eligibility traces and slow down the learning process. Alternatively, some non-probabilistic measurement methods such as General Q($\lambda$) and Naive Q($\lambda$) never cut traces, but face convergence problems in practice. To address the above issues, this paper introduces a new method named TBQ($\sigma$), which effectively unifies the tree-backup algorithm and Naive Q($\lambda$). By introducing a new parameter $\sigma$ to illustrate the \emph{degree} of utilizing traces, TBQ($\sigma$) creates an effective integration of TB($\lambda$) and Naive Q($\lambda$) and continuous role shift between them. The contraction property of TB($\sigma$) is theoretically analyzed for both policy evaluation and control settings. We also derive the online version of TBQ($\sigma$) and give the convergence proof. We empirically show that, for $\epsilon\in(0,1]$ in $\epsilon$-greedy policies, there exists some degree of utilizing traces for $\lambda\in[0,1]$, which can improve the efficiency in trace utilization for off-policy reinforcement learning, to both accelerate the learning process and improve the performance.",Reinforcement learning; Eligibility traces; Deep learning,Longxiang,,Shi,Zhejiang University,Shijian,,Li,Zhejiang University,Longbing,,Cao,University of Technology Sydney,Long,,Yang,Zhejiang University,Gang,,Pan,Zhejiang University,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0356,Cooperation with Bottom-up Reputation Dynamics,,"Cooperation among selfish agents can be promoted by allowing agents to condition behavior on reputation. Social norms -- dictating how agents update the reputations of others --  are central in determining whether this mechanism is effective. In particular, norms that reward justified defection have been shown to promote cooperation. A major limitation of existing models is that they assume all agents adopt a uniform norm, in a top down fashion. Here we show that when agents can spontaneously adopt novel norms, a learning process will see them drift towards socially undesirable outcomes. We present a model where agents can choose both how to react to reputations and how to assign the reputations of others -- making social norms emergent. In this scenario cooperation can only be achieved when the space of norms is severely restricted.  In the real world, reputation systems have a mixed record. This is often attributed to the costly nature of assigning reputations, and the ability of agents to easily whitewash their reputations. Our result suggests that even if these issues are overcome, enabling cooperation via reputation is likely to require additional mechanisms or restrictions upon the norms of the agents in the system.",Reputation; Cooperation; Dynamics; Evolutionary Game Theory,Jason,,Xu,Monash University,Julian,,García,Monash University,Toby,,Handfield,Monash University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0361,Patrol Scheduling Against Adversaries with Varying Attack Durations,,"We consider a generalization of zero-sum patrolling security game that allows the attacker choosing when, where, and how long to launch an attack, under three different attacker models. The attacker's payoff is the acquired utilities of the attack minus a penalty if the attacker is caught by the defender in patrol. The goal is to reduce the payoff of the attacker. To find the optimal defender/ attacker strategy, the game is converted to a combinatorial minimax problem with a closed-form objective function. Due to the complexity of the utility functions, we show that the minimax problem is not convex for all attacker models, even when the defender strategy is assumed as the time-homogeneous first-order Markov chain (i.e., the patroller's next visit only depends on his current location). However, for the zero penalty case, we prove that the optimal solution is either minimizing the expected hitting time or return time, based on different attacker models. We also observe that increasing the randomness of the patrol schedule helps to reduce the attacker's expected payoff for high penalty cases. Thus, to find solutions for general cases, we formulated a bi-criteria optimization problem and proposed three algorithms that support finding a trade-off between the expected maximum reward and the randomness. Another characteristic is that the third algorithm is able to find the optimal deterministic patrol schedule, although the running time is exponential on the number of patrol spots. Experiments demonstrate the effectiveness and scalability of our solutions. It also shows that our solutions outperform the baselines from state of the art in both artificial and real-world crime datasets.",Patrol Security Game;Vehicle Routing Problems;Markov Process,Hao-Tsung,,Yang,Stony Brook University,Shih-Yu,,Tsai,Stony Brook University,Kin Sum,,Liu,Stony Brook University,Shan,,Lin,Stony Brook University,Jie,,Gao,Stony Brook University,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0366,The Diverse Cohort Selection Problem,,"How should a firm allocate its limited interviewing resources to select the optimal cohort of new employees from a large set of job applicants? How should that firm allocate cheap but noisy resume screenings and expensive but in-depth in-person interviews? We view this problem through the lens of combinatorial pure exploration (CPE) in the multi-armed bandit setting, where a central learning agent performs costly exploration of a set of arms before selecting a final subset with some combinatorial structure. We generalize a recent CPE algorithm to the setting where arm pulls can have different costs and return different levels of information. We then prove theoretical upper bounds for a general class of arm-pulling strategies in this new setting. We apply our general algorithm to a real-world problem with combinatorial structure: incorporating diversity into university admissions. We take real data from admissions at one of the largest US-based computer science graduate programs and show that a simulation of our algorithm produces a cohort with hiring overall utility while spending comparable budget to the current admissions process at that university.
",AAMAS; ACM proceedings; organisations and institutions; social choice theory,Candice,,Schumann,University of Maryland,Samsara,N.,Counts,George Washington University,Jeffrey,S.,Foster,Tufts University,John,P.,Dickerson,University of Maryland,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0380,Protagonist vs Antagonist PROVANT: Narrative Generation as Counter Planning,Socially Interactive Agents Track,"Our motivation in this work is to develop a narrative generation mechanism for Interactive Storytelling that removes some of the authoring burden that is inherent to plan-based approaches. We focus on the class of narratives that dominate in Hollywood movies, television serial dramas and situation comedies. These narratives revolve around a central Protagonist in pursuit of a goal and who faces a series of obstructions placed in their way by an Antagonist and which they must overcome in order to reach their goal.

We cast this problem as a non-cooperative multi-agent planning problem, in other words counter planning. We build on recent techniques in goal recognition and landmark identification to develop a novel plan-based narrative generation mechanism. A key opportunity that goal recognition provides is to reason explicitly with partially observed action sequences, reflecting the reasoning process of the antagonist. Thus the antagonist can only act to obstruct if it is reasonable (to the viewer) that they have guessed the protagonist's intentions. Starting from the believed goal, the narrative generator can reason about the protagonist's plan and what must be done to achieve it i.e., the plan landmarks and use these to automatically identify suitable points of obstruction. In the paper we detail the approach and illustrate it with a worked example. We report the results of an experimental evaluation and user study in a number of representative narrative domains. Results of the user study with system generated narratives confirm that viewers can clearly recognise agent roles and narrative structure.
",Interactive Narrative and Storytelling; Entertainment,Julie,,Porteous,RMIT University,Alan,,Lindsay,University of Huddersfield,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0397,Self-Improving Generative Adversarial Reinforcement Learning,,"The lack of data efficiency and stability is one of the main challenges in end-to-end model free reinforcement learning (RL) methods. Recent researches solve the problem resort to supervised learning methods by utilizing human expert demonstrations, e.g. imitation learning. In this paper we present a novel framework which builds a self-improving process upon a policy improvement operator, which is used as a black box such that it has multiple implementation options for various applications. An agent is trained to iteratively imitate behaviors that are generated by the operator. Hence the agent can learn by itself without domain knowledge from human. We employ generative adversarial networks (GAN) to implement the imitation module in the new framework. We evaluate the framework performance over multiple application domains and provide comparison results in support.",Reinforcement Learning; Generative Adversarial Nets; Imitation learning; Policy Iteration; Policy distillation; Deep Learning,Yang,,Liu,Teesside University,Yifeng,,Zeng,Teesside University,Yingke,,Chen,Teesside University,Jing,,Tang,Teesside University,Yinghui,,Pan,Jiangxi Uni. of Finance&Economics,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0400,Independent Generative Adversarial Self-Imitation Learning in Cooperative Multiagent Systems,,"Many tasks in practice require the collaboration of multiple agents through reinforcement learning. In general, cooperative multiagent reinforcement learning algorithms can be classified into two paradigms: Joint Action Learners (JALs) and Independent Learners (ILs). In many practical applications, agents are unable to observe other agents' actions and rewards, making JALs inapplicable. In this work, we focus on independent learning paradigm in which each agent makes decisions based on its local observations only. However, learning is challenging in independent settings due to the local viewpoints of all agents, which perceive the world as a non-stationary environment due to the concurrently exploring teammates. In this paper, we propose a novel framework called Independent Generative Adversarial Self-Imitation Learning (IGASIL) to address the coordination problems in fully cooperative multiagent environments. To the best of our knowledge, we are the first to combine self-imitation learning with generative adversarial imitation learning (GAIL) and apply it to cooperative multiagent systems. Besides, we put forward a Sub-Curriculum Experience Replay mechanism to pick out the past beneficial experiences as much as possible and accelerate the self-imitation learning process. Evaluations conducted in the testbed of StarCraft unit micromanagement and a commonly adopted benchmark show that our IGASIL produces state-of-the-art results and even outperforms JALs in terms of both convergence speed and final performance.",Multiagent learning; Learning agent-to-agent interactions (coordination); Adversarial machine learning,Xiaotian,,Hao,Tianjin University,Weixun,,Wang,Tianjin University,Jianye,,Hao,Tianjin University,Yaodong,,Yang,Tianjin University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0402,Bandit Learning with Biased Human Feedback,,"We study a multi-armed bandit problem with biased human feedback. In our setting, each arm is associated with an unknown reward distribution. When an arm is played, a user receives a realized reward drawn from the distribution of the arm. She then provides feedback, a biased report of the realized reward, that depends on both the realized reward and the feedback history of the arm.
The learner can observe only the biased feedback but not the realized rewards. The goal is to design a strategy to sequentially choose arms to maximize the total rewards users receive while only having access to the biased user feedback. We explore two natural feedback models. When user feedback is biased only by the average feedback of the arm (i.e., the ratio of positive feedback),
we demonstrate that the evolution of the average feedback over time is mathematically equivalent to users performing online gradient descent for some latent function with a decreasing step size.
With this mathematical connection, we show that under some mild conditions, it is possible to design a bandit algorithm achieving regret (i.e., the difference between the algorithm performance and the optimal performance of always choosing the best arm) sublinear in the number of rounds. However, in another model when user feedback is biased by both the average feedback and the number of feedback instances, we show that there exist no bandit algorithms that could achieve sublinear regret. Our results demonstrate the importance of understanding human behavior when applying bandit approaches in systems with humans in the loop.
",multi-armed bandits; herding effects; social learning,Wei,,Tang,Washington University in St. Louis,Chien-Ju,,Ho,Washington University in St. Louis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0410,Engineering Scalable Distributed Environments and Organizations for MAS,,"In MAS programming and engineering, the environment and the organisation can be exploited as first-class design and programming abstractions besides the agent one. A main example of a platform implementing this view is JaCaMo, which allows the programming of a MAS in terms of an organisation of cognitive agents sharing a common artifact-based environment. However, MAS models and platforms in general do not provide a satisfactory approach for MAS developers to uniformly deal with distribution at multiple dimensions -- agent, environment, and organisation. Typically, environments are either centralised in a single node, or composed by parts that run on different nodes but with a poor support at the programming and execution levels to deal with that. In this paper, we tackle this problem by proposing a model for engineering world-wide distributed environments and organisations for MAS. The approach integrates the A&A (Agents and Artifacts) conceptual model with a web/resource-oriented view of distributed systems as proposed by the REST architectural style. To evaluate the approach, an extension of the JaCaMo open-source platform has been developed implementing the proposed model.
",Multi-Agent Systems Engineering; Artifact-based Environments; Distributed Systems; JaCaMo,Alessandro,,Ricci,University of Bologna,Andrei,,Ciortea,University of St. Gallen,Simon,,Mayer,University of St. Gallen,Olivier,,Boissier,"Univ. Lyon, MINES Saint-Etienne",Rafael,H.,Bordini,"School of Technology, PUCRS",Jomi,Fred,Hubner,Federal University of Santa Catarina,,,,,,,,,,,,,,,,,,,,,,,,
fp0417,A Representation Theorem for Reasoning in First-Order Multi-Agent Knowledge Bases,,"Levesque's notion of only-knowing provides a natural formalisation of a knowledge base: it precisely captures the beliefs and non-beliefs that follow from the knowledge base, including introspection and de dicto versus de re distinctions in a first-order setting. Apart from its attractive properties in terms of specification, a major result about only-knowing is Levesque's representation theorem, which shows how reasoning in (single-agent) knowledge bases can be Turing-reduced to ordinary first-order logic. While numerous proposals have been made to lift the logic of only-knowing to the multi-agent case, generalising the representation theorem has remained an open problem. In this paper, we develop a Turing reduction from reasoning in multi-agent knowledge bases to ordinary, non-epistemic first-order logic and thus obtain a new representation theorem for the multi-agent case.",Reasoning in multi-agent systems; only-knowing; first-order logic,Christoph,,Schwering,The University of New South Wales,Maurice,,Pagnucco,The University of New South Wales,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0418,Improved Cooperative Multi-agent Reinforcement Learning Algorithm Augmented by Mixing Demonstrations from Centralized Policy,,"Many decision problems for complex systems that involve multiple decision makers can be formulated as a decentralized partially observable markov decision process (dec-POMDP) problem. Due to the computational difficulty with obtaining optimal policies, recent approaches to dec-POMDP often use a multi-agent reinforcement learning (MARL) algorithm. We propose a method to improve the existing cooperative MARL algorithms by adopting an imitation learning technique. For a reference policy in the imitation learning part, we use a centralized policy from a multi-agent MDP or a multi-agent POMDP model reduced from the original dec-POMDP model. In the proposed method, during the training process, we mix demonstrations from the reference policy by using a demonstration buffer. Demonstration samples from the buffer are used in the augmented policy gradient function for policy updates. We assess the performance of the proposed method for three well-known dec-POMDP benchmark problems -- Mars rover, co-operative box pushing, and dec-tiger. Experimental results indicate that augmenting the baseline MARL algorithm by mixing the demonstrations significantly improves the quality of policy solutions. With these results, we conclude that the imitation learning can enhance MARL algorithms and that policy solutions from MMDP and MPOMDP models are a reasonable reference policy to use in the proposed algorithm.
",Multi-agent reinforcement learning; Cooperative decision making problem; dec-POMDP; Imitation learning,Hyun-Rok,,Lee,Korea Advanced Institute of Science & Technology,Taesik,,Lee,Korea Advanced Institute of Science & Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0426,Multi-Agent Learning in Network Zero-Sum Games is a Hamiltonian System,,"Zero-sum games are natural, if informal, analogues of closed physical systems where no energy/utility can enter or exit. This analogy can be extended even further if we consider zero-sum network (polymatrix) games where multiple agents interact in a closed economy. Typically, (network) zero-sum games are studied from the perspective of Nash equilibria. Nevertheless, this comes in contrast with the way we typically think about closed physical systems, e.g., Earth-moon systems which move perpetually along recurrent trajectories of constant energy.


We establish a formal and robust connection between multi-agent systems and Hamiltonian dynamics -- the same dynamics that describe conservative systems in physics. Specifically, we show that no matter the size, or network structure of such closed economies, even if agents use different online learning dynamics from the standard class of Follow-the-Regularized-Leader, they
yield Hamiltonian dynamics. This approach generalizes the known connection to Hamiltonians for the special case of replicator dynamics in two agent zero-sum games developed by Hofbauer.
Moreover, our results extend beyond zero-sum settings and provide a type of a Rosetta stone (see e.g. Table 1) that helps to translate results and techniques between online optimization, convex analysis, games theory, and physics.",Multi-agent Learning; Network Games; Conservative System; Follow-the-Regularized-Leader; Regret; Poincare Recurrence,James,P.,Bailey,Singapore University of Technology and Design,Georgios,,Piliouras,Singapore University of Technology and Design,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0427,Single Transferable Vote: Incomplete Knowledge and Communication Issues,,"Single Transferable Vote (STV) is used in large political elections around the world. It is easy to understand and has desirable normative properties such as clone-proofness.
However, voters need to report full rankings, which can make it less practical than plurality voting.
We study ways to minimize the amount of communication required to use single-winner STV.
In the first part of the paper, voters are assumed to report their top-$k$ alternatives in a single shot. We empirically evaluate the extent to which STV with truncated ballots approximates STV with full information.
We also study the computational complexity of the possible winner problem for top-$k$ ballots. For $k=1$, it can be solved in polynomial time, but is NP-complete when $k\geq 2$.
In the second part, we consider interactive communication protocols for STV. Building on a protocol proposed by Conitzer and Sandholm (2005), we show how we can reduce the amount of communication required in practice. We then study empirically the average communication complexity of these protocols, based on randomly generated profiles, and on real-world election data.
Our conclusion is that STV needs, in practice, much less information than in the worst case.",STV; communication protocol; approximation; truncated ballots,Manel,,Ayadi,Université de Tunis & Université Paris Dauphine France,Nahla,,Ben Amor,Université de Tunis,J&#233;r&#244;me,,Lang,Université Paris-Dauphine,Dominik,,Peters,University of Oxford,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0434,Hedonic Diversity Games,,"We consider a coalition formation setting where each agent belongs to one of the two types, and agents' preferences over coalitions are determined by the fraction of the agents of their own type in each coalition. This setting differs from the well-studied Schelling's model in that some agents may prefer homogeneous coalitions, while others may prefer to be members of a diverse group, or a group that mostly consists of agents of the other type. We model this setting as a hedonic game and investigate the existence of stable outcomes using hedonic games solution concepts. We show that a core stable outcome may fail to exist and checking the existence of core stable outcomes is computationally hard. On the other hand, we propose an efficient algorithm to find an individually stable outcome under the natural assumption that agents' preferences over fractions of the agents of their own type are single-peaked.",Hedonic games; Schelling segregation; fractional hedonic games,Robert,,Bredereck,TU Berlin,Edith,,Elkind,University of Oxford,Ayumi,,Igarashi,Kyushu University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0437,Argumentation-based Negotiation with Incomplete Opponent Profiles,,"Computational argumentation has taken a predominant place in the modeling of negotiation dialogues over the last years. A competent agent participating in a negotiation process is expected to decide its next move taking into account an, often incomplete, model of its opponent.
This work provides a complete computational account of argumentation-based negotiation under incomplete opponent profiles. After the agent identifies its best option, in any state of a negotiation, it looks for suitable arguments that support this option in the theory
of its opponent. As the knowledge on the opponent is uncertain, the challenge is to
find arguments that, ideally, support the selected option despite the
uncertainty. We present a negotiation framework based on these ideas, along with experimental evidence that highlights the advantages of our approach.",Argumentation; Automated Negotiation; Multi-Agent Systems,Yannis,,Dimopoulos,University of Cyprus,Jean-Guy,,Mailly,Paris Descartes University,Pavlos,,Moraitis,Paris Descartes University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0440,On Domination and Control in Strategic Ability,,"We propose a technique to compare partial strategies for agents and coalitions in multi-agent environments with imperfect information. The proposed relation mimics the game-theoretic notion of strategic dominance: a stronger partial strategy can replace a weaker one in a full strategy, while preserving the enforceability of a given goal.
Our version of dominance is based on a new notion of input/output characteristics for partial strategies. Intuitively, the inputs are the states where the strategy gains control over execution, and the outputs are the states where it returns the control to the environment.
Moreover, we identify the sources of dependence between partial strategies, and show conditions under which they can be analysed in a fully independent way.
We evaluate our approach on two scalable benchmarks from the literature. The experiments are carried out for the tasks of incremental synthesis of uniform strategies and optimization of a winning strategy, with very promising results.",alternating-time temporal logic; imperfect information; strategy synthesis; model checking,Damian,,Kurpiewski,Polish Academy of Sciences,Micha&#322;,,Knapik,Polish Academy of Sciences,Wojciech,,Jamroga,Polish Academy of Sciences,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0443,Maximizing the Spread of an Opinion when Tertium Datur Est,,"Opinion diffusion has been largely studied in the literature on settings where the opinion whose spread has to be maximized, say \emph{white}, competes against one opinion only, say \emph{black}. For instance, for diffusion mechanisms modeled in terms of best response dynamics over \emph{majority} agents (who change their opinion as to conform it to the majority of their neighbors), it is known that the spread can be maximized via certain greedy dynamics that can be computed in polynomial time. This setting is precisely the one considered in the paper. However, differently from earlier literature, it is assumed that one further opinion, say \emph{gray}, is available to the agents. Moving from the observation that, with the third alternative to hand, greedy dynamics can dramatically fail to maximize the spread of opinion
\emph{white}, the paper then embarks in thorough computational, algorithmic and experimental studies.</par><par>The picture that emerges is totally different from what is known for the case when two opinions are available only.</par><par>Indeed, it is shown that greedy dynamics can dramatically fail in maximizing the spread. In particular, deciding whether there exists a dynamics that can spread the opinion \emph{white} to at least $k$ agents or can reach a
consensus is shown to be intractable, formally $\NP$-hard.
On the other hand, islands of tractability based on certain structural properties of the interaction graph are identified. Finally,
experimental results are discussed, which shed lights on opinion diffusion in real social networks.",Opinion Formation; Greedy Dynamics; Social Networks,Vincenzo,,Auletta,University of Salerno,Diodato,,Ferraioli,University of Salerno,Valeria,,Fionda,University of Calabria,Gianluigi,,Greco,University of Calabria,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0448,Parameterized Complexity of Committee Elections with Dichotomous and Trichotomous Votes,,"We study the winner determination problem for three prevalent committee election rules: Chamberlin-Courant Approval Voting(CCA), Proportional Approval Voting (PAV), and Satisfaction Approval Voting (SAV). Axiomatic and algorithmic studies of elections under these rules have been conducted recently. It is known that the winner determination problem is NP-hard for CCA and PAV and polynomial-time solvable for SAV, if the input votes are dichotomous. Moreover, parameterized complexity of the two NP-hard cases has been examined with respect to some natural parameters such as the number of candidates or the number of votes. In this paper, we extend the above studies to committee elections with trichotomous votes and identify cases, where trichotomous votes lead to an increase of parameterized complexity. We also consider the maximin (or egalitarian) variations of the rules, where the minimum satisfaction of the voters is maximized.",,Aizhong,,Zhou,Shandong University,Jiong,,Guo,Shandong University,Yongjie,,Yang,Saarland University Chair of Economic Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0449,Negative Update Intervals in Deep Multi-Agent Reinforcement Learning,,"In Multi-Agent Reinforcement Learning (MA-RL), independent cooperative learners must overcome a number of pathologies to learn optimal joint policies. Addressing one pathology often leaves approaches vulnerable towards others. For instance, \emph{hysteretic Q-learning} \cite{matignon2007hysteretic} addresses miscoordination while leaving agents vulnerable towards misleading stochastic rewards. Other methods, such as \emph{leniency}, have proven more robust when dealing with multiple pathologies simultaneously \cite{JMLR:v17:15-417}. However, leniency has predominately been studied within the context of strategic form games (bimatrix games) and fully observable Markov games consisting of a small number of probabilistic state transitions. This raises the question of whether these findings scale to more complex domains. For this purpose we implement a temporally extend version of the \emph{Climb Game} \cite{claus1998dynamics}, within which agents must overcome multiple pathologies \emph{simultaneously}, including relative overgeneralisation, stochasticity, the alter-exploration and moving target problems, while learning from a large observation space. We find that existing lenient and hysteretic approaches fail to consistently learn near optimal joint-policies in this environment. To address these pathologies we introduce \emph{Negative Update Intervals-DDQN (NUI-DDQN)}, a Deep MA-RL algorithm which discards episodes yielding cumulative rewards outside the range of expanding intervals. NUI-DDQN consistently gravitates towards optimal joint-policies in our environment, overcoming the outlined pathologies.",Deep Multi-Agent Reinforcement Learning,Gregory,,Palmer,University of Liverpool,Rahul,,Savani,University of Liverpool,Karl,,Tuyls,University of Liverpool,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0460,Decidable Model Checking with Uniform Strategies,,"The logic of strategic ability Resource-Bounded Alternating Time Syntactic Epistemic Logic (RB+-ATSEL) has a decidable model-checking problem for coalition uniform strategies. A strategy is coalition uniform if agents in a coalition select the same joint action in all states where the knowledge of the coalition is the same. However, this presupposes free and unbounded communication between the agents in the coalition before every action selection. In this paper we present a modified version of RB+-ATSEL, RB+-ATSELc, with explicit (and explicitly costed) communication actions. RB+-ATSELc is interpreted on communication models which have an explicit communication step before every action selection. We show that, unlike standard ATL under imperfect information, the model checking problem for RB+-ATSELc is decidable under perfect recall uniform strategies. Our decidability result also applies to ATL with imperfect information and perfect recall when interpreted on communication models.",Uniform strategies; model-checking; resource logic; communication,Natasha,,Alechina,University of Nottingham,Mehdi,,Dastani,Utrecht University,Brian,,Logan,University of Nottingham,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0461,Fraud Regulating Policy for E-Commerce via Constrained Contextual Bandits,,"Fraud sellers in e-commerce often promote themselves via fake visits or purchases to increase sales, jeopardizing the business environment of the platform. How to regulate the exposure of these sellers to buyers without affecting normal online business remains a challenging problem, since blocking them entirely without discrimination may kill the normal transactions and could potentially decrease the total transactions of the platform. To address this problem, we introduce a regulating valve which blocks fraud sellers with a certain probability. To learn the optimal blocking policy, we model the regulating valve as a contextual bandit problem with a constraint on the total transaction decline. Since existing bandit algorithms are unable to incorporate the transaction constraint, we propose a novel bandit algorithm, which decides the policy based on a set of neural networks and iteratively updates the neural networks with online observations and the constraint. Experiments on synthetic data and one of the largest e-commerce platforms in the world both show that our algorithm effectively and efficiently outperforms existing bandit algorithms by a large margin.",E-Commerce; Fraud Sellers; Buyer Impressions; Constrained Contextual Bandits,Zehong,,Hu,Alibaba Group,Zhen,,Wang,Alibaba Group,Zhao,,Li,Alibaba Group,Shichang,,Hu,Alibaba Group,Shasha,,Ruan,Alibaba Group,Jie,,Zhang,Nanyang Technological University,,,,,,,,,,,,,,,,,,,,,,,,
fp0464,A Cooperative Multi-Agent Reinforcement Learning Framework for Resource Balancing in Complex Logistics Network,,"Resource balancing within complex transportation networks is one of the most important problems in real logistics domain. Traditional solutions on these problems leverage combinatorial optimization with demand and supply forecasting. However, the high complexity of transportation routes, severe uncertainty of future demand and supply, together with non-convex business constraints make it extremely challenging in the traditional resource management field. In this paper, we propose a novel sophisticated multi-agent reinforcement learning approach to address these challenges. In particular, inspired by the externalities especially the interactions among resource agents, we introduce an innovative cooperative mechanism for state and reward design resulting in more effective and efficient transportation. Extensive experiments on a simulated ocean transportation service demonstrate that our new approach can stimulate cooperation among agents and lead to much better performance. Compared with traditional solutions based on combinatorial optimization, our approach can give rise to a significant improvement in terms of both performance and stability.",multi-agent; reinforcement learning; resource balancing; logistics network,Xihan,,Li,Peking University,Jia,,Zhang,Microsoft Research Asia,Jiang,,Bian,Microsoft Research Asia,Yunhai,,Tong,Peking University,Tie-Yan,,Liu,Microsoft Research Asia,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0468,An Evolutionary Approach to Find Optimal  Policies with an Agent-Based Simulation,,"In this paper, we introduce a new agent-based method to build a decision-aid tool aimed to improve policy design. In our approach, a policy is defined as a set of levers, modelling the set of actions, the means to impact a complex system.

Our method is generic, as it could be applied to any domain, and be coupled with any agent-based simulator. We could deal not only with simple levers (a single variable whose value is modified) but also complex ones (multiple variable modifications, qualitative effects, ...), unlike most optimization methods. It is based on the evolutionary algorithm CMA-ES, coupled with a normalized and aggregated fitness function. The fitness is normalized using estimated Ideal (best policy) and Nadir (worst policy) values, these values being dynamically computed during the execution of CMA-ES through a Pareto Front estimated with the ABM simulation.
Moreover, to deal with complex levers, we introduce the FSM-branching algorithm, where a Finite State Machine (FSM) determines whether a complex policy can potentially be improved or has to be aborted.

We tested our method with Economic Policies on the French Labor Market (FLM), allowing the modification of multiple elements of the FLM, and we compared the results to the reference, the FLM without any policy applied. The policies studied here comprise simple and complex levers. This experience shows the viability of our approach, the efficiency of our algorithms and illustrates how this combination of evolutionary optimization, multi-criteria aggregation and agent-based simulation could help any policy-maker to design better policies.",Agent-Based Simulation; Evolutionary Optimization; Multi-criteria aggregation; Policy Design; Labor Economics,Nicolas,,De Bufala,Sorbonne Université Sciences,Jean-Daniel,,Kant,Sorbonne Université Sciences,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0469,Gradual Semantics Accounting for Varied-Strength Attacks,,"The paper studies how to evaluate arguments in graphs where both arguments and attacks are weighted. It proposes a broad family of gradual semantics that assign to each argument a numerical value representing its strength, i.e., how robust is the argument against attacks. It shows that five existing gradual semantics are instances of the family, and extends each of them in various ways for accounting for weights of attacks. The extended versions of each semantics differ in the way they deal with weights of attacks. Furthermore, they are all instances of the family. The paper shows also that the family captures additional semantics, like Euler-Max-based that is investigated in the paper. The new semantics are analyzed against properties from the literature and are compared with existing semantics that deal with weighted attacks.",Argumentation; Gradual Semantics; Weighted Attacks,Leila,,Amgoud,CNRS - IRIT,Dragan,,Doder,Toulouse University – IRIT,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0470,Covert Networks: How Hard is It to Hide?,,"Covert networks are social networks that often consist of harmful users. Social Network Analysis (SNA) has played an important role in reducing criminal activities (e.g., counter terrorism) via detecting the influential users in such networks. There are various popular measures to quantify how influential or central any vertex is in a network. As expected, strategic and influential miscreants in covert networks would try to hide herself and her partners (called {\em leaders}) from being detected via these measures by introducing new edges.

Waniek et al. show that the corresponding computational problem, called Hiding Leader, is NP Complete for the degree and closeness centrality measures. We study the popular core centrality measure and show that the problem is NP Complete even when the core centrality of every leader is only 3. On the contrary, we prove that the problem becomes polynomial time solvable for the degree centrality measure if the degree of every leader is bounded above by any constant. We then focus on the optimization version of the problem and show that the Hiding Leader problem admits a 2 factor approximation algorithm for the degree centrality measure. We complement it by proving that one cannot hope to have any (2-epsilon) factor approximation algorithm for any constant $epsilon>0$ unless there is a epsilon/2 factor polynomial time algorithm for the Densest k-Subgraph problem which would be considered a significant breakthrough. We empirically establish that our 2 factor approximation algorithm frequently finds out a near optimal solution. On the contrary, for the core centrality measure, we show that the Hiding Leader problem does not admit any (1-alpha) ln n factor approximation algorithm for any constant alpha in (0,1) unless P=NP even when the core centrality of every leader is only 3. Hence, our work shows that, although classical complexity theoretic framework fails to shed any light on relative difficulty of Hiding Leader for different centrality measures, the problem is significantly ``harder'' for the core centrality measure than the degree centrality one.",Social Network Analysis; Centrality; Hiding in Networks; Algorithms; Social Influence; Covert Network; Core; Approximation Algorithm; Network Design; Densest Subgraph,Palash,,Dey,Indian Institute of Technology Kharagpur,Sourav,,Medya,"University of California, Santa Barbara",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0495,Agent Behavioral Analysis Based on Absorbing Markov Chains,,"We propose a novel technique to identify known behaviors of intelligent agents acting within uncertain environments. We employ Markov chains to represent the observed behavioral models of the agents and we formulate the problem as a classification task. In particular, we propose to use the long-term transition probability values of moving between states of the Markov chain as features. Additionally, we transform our models into absorbing Markov chains, enabling the use of standard techniques to compute such features. The empirical evaluation considers two scenarios: the identification of given strategies in classical games, and the detection of malicious behaviors in malware analysis. Results show that our approach can provide informative features to successfully identify known behavioral patterns. In more detail, we show that focusing on the long-term transition probability enables to diminish the error introduced by noisy states and transitions that may be present in an observed behavioral model. We pose particular attention to the case of noise that may be intentionally introduced by a target agent to deceive an observer agent.",Behavioral Analysis; Malware Analysis; Absorbing Markov Chains,Riccardo,,Sartea,University of Verona,Alessandro,,Farinelli,University of Verona,Matteo,,Murari,University of Verona,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0498,Formal Verification of Open Multi-Agent Systems,,"We study open multi-agent systems in which countably many agents may leave and join the system at run-time. We introduce a semantics, based on interpreted systems, to capture the openness of the system and show how an indexed variant of temporal-epistemic logic can be used to express specifications on them. We define the verification problem and show it is undecidable. We isolate one decidable class of open multi-agent systems and give a partial decision procedure for another one.  We introduce MCMAS-OP, an open-source toolkit implementing the verification procedures. We present the results obtained using our tool on two examples.",Parameterised model checking; Open systems; Multi-agent systems,Panagiotis,,Kouvaros,Imperial College London,Alessio,,Lomuscio,Imperial College London,Edoardo,,Pirovano,Imperial College London,Hashan,,Punchihewa,Imperial College London,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0500,Attacking Power Indices by Manipulating Player Reliability,,"We investigate the manipulation of power indices in TU-cooperative games by stimulating (subject to a budget constraint) changes in the propensity of other players to participate to the game.</par><par>We display several algorithms that show that the problem is often tractable for so-called network centrality games and influence attribution games, as well as an example when optimal manipulation is intractable, even though computing power indices is feasible.
",coalitional games;reliability extension;Shapley value;manipulation,Gabriel,,Istrate,West University of Timi&#351;oara & e-Austria Research Institute,Cosmin,,Bonchi?,West University of Timi&#351;oara & e-Austria Research Institute,Alin,,Brîndu?escu,West University of Timi&#351;oara & Elektrobit Automotive Romania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0503,Stability in FEN-Hedonic Games for Single-Player Deviations,,"Hedonic games model how players form coalitions based on their preferences about the coalitions they can join.  Lang et al.(2015) introduced FEN-hedonic games where each player partitions the other players into friends, enemies, and neutral players and ranks her friends and enemies.  They then use bipolar responsive extensions to derive preferences over coalitions, and since such preferences can be incomplete, they consider possible and necessary stability for various stability notions and study the related verification and existence problems in terms of computational complexity.  However, in their complexity analysis they left a number of cases open. We settle several of these open problems for stability concepts based on single-player deviations: We show that possible verification can be solved in polynomial time for Nash stability, individual stability, and contractually individual stability. Yet, necessary existence is an NP-complete problem for individual stability while possible existence is easy for contractually individual stability.",hedonic game; stability; coalition formation; complexity,Anna,Maria,Kerkmann,Heinrich-Heine-Universität Düsseldorf,Jörg,,Rothe,Heinrich-Heine-Universität Düsseldorf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0504,Groups Versus Coalitions: On the Relative Expressivity of GAL and CAL,,"Group Announcement Logic (GAL) and Coalition Announcement Logic (CAL) were proposed to study effects of public announcements by groups of agents on knowledge in multiagent systems.  Both logics have operators that quantify over such announcements. In GAL, it is possible to express that `a group of agents $G$ has a (truthful) announcement such that after this announcement, some property A holds’; for example, A may involve some agents in $G$ gaining additional knowledge, while agents outside $G$ remain ignorant. In CAL, the meaning of the coalition announcement operator is subtly different: it says that `$G$ has an announcement such that, whatever else the agents outside $G$ announce simultaneously, some property A is guaranteed to hold after the joint announcement'. It has been open for some time whether GAL and CAL are equally expressive. We show that this is not the case: there is a property expressible in GAL that is not expressible in CAL. It is still an open question whether CAL is subsumed by GAL, or whether the two logics have incomparable expressive power.",Coalition announcement logic; group announcement logic; public announcements; dynamic epistemic logic,Tim,,French,The University of Western Australia,Rustam,,Galimullin,University of Nottingham,Hans,,van Ditmarsch,"LORIA, CNRS, University of Lorraine",Natasha,,Alechina,University of Nottingham,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0510,Truthfulness on a Budget: Trading Money for Approximation through Monitoring,,"Albeit a pervasive desideratum when computing in the presence of selfish agents, truthfulness typically imposes severe limitations to what can be implemented. The price of these limitations is typically paid either \emph{economically}, in terms of the financial resources needed to enforce truthfulness, or \emph{algorithmically}, in terms of restricting the set of implementable objective functions, which often leads to renouncing optimality and resorting to approximate allocations.
In this paper, with regards to utilitarian problems, we ask two fundamental questions: (\emph{i}) what is the a minimum sufficient budget needed by optimal truthful mechanisms, and (\emph{ii}) whether it is possible to sacrifice optimality in order to achieve truthfulness with a lower budget.
To answer these questions, we connect two streams of work on mechanism design and look at monitoring -- a paradigm wherein agents' actual costs are bound to their declarations. In this setting, we prove that the social cost is always a sufficient budget, even for collusion-resistant mechanisms, and, under mild conditions, also a necessary budget for a large class of utilitarian problems that encompass set system problems.
Furthermore, for facility location, a well-studied problem outside of this class, we draw a novel picture about the relationship between approximation and frugality.",Auctions and mechanism design; Noncooperative games: theory & analysis,Paolo,,Serafino,University of Oxford,Carmine,,Ventre,University of Essex,Angelina,,Vidali,University of Athens,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0512,Multi-Issue Opinion Diffusion under Constraints,,"Most existing models of opinion diffusion on networks neglect the existence of logical constraints that might correlate individual opinions on multiple issues. In this paper we study the diffusion of constrained opinions on a social network as an iterated process of aggregating neighbouring opinions. Individual views are modelled as vectors of yes/no answers to a number of propositions subject to integrity constraints, and each individual updates her opinion by looking at the aggregated opinion of her influencers. To overcome the problem of updating towards inconsistent influencing opinions, we propose a model based on individual updates on subsets of the issues of limited size called propositionwise updates. By adapting notions from the theory of boolean functions, we identify classes of integrity constraints on which propositionwise updates decrease the influence gap between nodes of the network and their influencers caused by the presence of an integrity constraint. Furthermore, we provide a detailed study of the termination of the proposed diffusion processes.",Social networks; judgment aggregation; opinion transformation; boolean functions,Sirin,,Botan,"ILLC, University of Amsterdam",Umberto,,Grandi,"IRIT, University of Toulouse",Laurent,,Perrussel,"IRIT, University of Toulouse",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0516,Extracting Dialogical Explanations for Review Aggregations with Argumentative Dialogical Agents,,"The aggregation of online reviews is fast becoming the chosen method of quality control for users in various domains, from retail to entertainment. Consequently, fair, thorough and explainable aggregation of reviews is increasingly sought-after. We consider the movie review domain, and in particular Rotten Tomatoes’ ubiquitous (and arguably over-simplified) aggregation method, the Tomatometer Score (TS). For a movie, this amounts to the percentage of critics giving the movie a positive review. We define a novel form of argumentative dialogical agent (ADA) for explaining the reasoning within the reviews. ADA integrates: 1.) NLP with reviews to extract a Quantitative Bipolar Argumentation Framework (QBAF) for any chosen movie to provide the underlying structure of explanations, and 2.) gradual semantics for QBAFs for deriving a dialectical strength measure for movies, as an alternative to the TS, satisfying desirable properties for obtaining explanations. We evaluate ADA using some prominent NLP methods and gradual semantics for QBAFs. We show that they provide a dialectical strength which is comparable with the TS, while at the same time being able to provide dialogical explanations of why a movie obtained its strength via interactions between the user and ADA.",dialogical interactions; explainability; argument mining; quantitative argumentation,Oana,,Cocarascu,Imperial College London,Antonio,,Rago,Imperial College London,Francesca,,Toni,Imperial College London,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp052,Manipulating Elections by Selecting Issues,,"Constructive election control considers the problem of an adversary who seeks to sway the outcome of an electoral process in order to ensure that their favored candidate wins. We consider the computational problem of constructive election control via issue selection. In this problem, a party decides which political issues to focus on to ensure victory for the favored candidate.  We also consider a variation in which the goal is to maximize the number of voters supporting the favored candidate.  We present strong negative results, showing, for example, that the latter problem is inapproximable for any constant factor.  On the positive side, we show that when issues are binary, the problem becomes tractable in several cases, and admits a 2-approximation in the two-candidate case. Finally, we develop integer programming and heuristic methods for these problems.",Election control; social choice,Jasper,,Lu,Vanderbilt University,David,Kai,Zhang,Vanderbilt University,Zinovi,,Rabinovich,Nanyang Technological University,Svetlana,,Obraztsova,Nanyang Technological University,Yevgeniy,,Vorobeychik,Washington University in St. Louis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0523,Multi-unit Budget Feasible Mechanisms for Cellular Traffic Offloading,,"Cellular traffic offloading is nowadays an important problem in mobile networking. Since the offloading resource owners (agents) are self-interested and have private costs, it is highly challenging to design procurement mechanisms that motivate agents to reveal their true costs and achieve guaranteed performance under the constraint of a strict budget. In this paper, we model cellular traffic offloading as a multi-unit budget feasible procurement auction design problem with diminishing return valuations. We design a novel greedy-based randomized mechanism, and prove it is budget-feasible, truthful, individually rational and a $(3+2\ln N)$-approximation, where $N$ is the total number of available resource units. We also propose a deterministic mechanism which achieves  $(2+\ln N + \sqrt{2+3\ln N +\ln^2N})$ - approximation. We prove no budget-feasible and truthful mechanism can do better than $\ln N$-approximation in our setting, thus our mechanism approaches the optimal to a constant factor. In addition to solving the cellular traffic offloading problem, our work successfully extends solvable valuation class of greedy-based multi-unit budget-feasible mechanism with performance guarantees from the concave-additive valuations to more general local diminishing return valuations.",Auctions; Algorithmic mechanism design; Budget feasible mechanisms; Multi-unit procurement auctions,Jun,,Wu,Nanjing University,Yuan,,Zhang,Nanjing University,Yu,,Qiao,Nanjing University cs,Lei,,Zhang,Nanjing University,Chongjun,,Wang,Nanjing University,Junyuan,,Xie,Nanjing University,,,,,,,,,,,,,,,,,,,,,,,,
fp0525,Strategic Responsibility Under Imperfect Information,,"A central issue in the specification and verification of autonomous agents and multiagent systems is the ascription of responsibility to individual agents and groups of agents. When designing a (multi)agent system, we must specify which agents or groups of agents are responsible for bringing about a particular state of affairs. Similarly, when verifying a multiagent system, we may wish to determine the responsibility of agents or groups of agents for a particular state of affairs, and the contribution of each agent to bringing about that state of affairs. In this paper, we discuss several aspects of responsibility, including strategic ability of agents, their epistemic properties, and their relationship to the evolution of the system behavior. We introduce a formal framework for reasoning about the responsibility of individual agents and agent groups in terms of the agents’ strategies and epistemic properties, and state some properties of the framework.",Responsibility in Agent Systems; Strategic Reasoning; Concurrent Game Structures; Temporal and Modal Logic,Vahid,,Yazdanpanah,University of Twente,Mehdi,,Dastani,Utrecht University,Wojciech,,Jamroga,Polish Academy of Sciences,Natasha,,Alechina,University of Nottingham,Brian,,Logan,University of Nottingham,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0527,Manipulations-resistant Facility Location Mechanisms for ZV-line Graphs,,"In many real-life scenarios, a group of agents needs to agree on a common action, e.g., on a location for a public facility, while there is some consistency between their preferences, e.g., all preferences are derived from a common metric space. The facility location problem models such scenarios and it is a well-studied problem in social choice. We study mechanisms for facility location on unweighted undirected graphs, which are resistant to manipulations (strategyproof, abstention-proof, and false-name-proof ) by both individuals and coalitions and are efficient (Pareto optimal). We define a family of graphs, ZV -line graphs, and show a general facility location mechanism for these graphs which satisfies all these desired properties. Moreover, we show that this mechanism can be computed in polynomial time, the mechanism is anonymous, and it can equivalently be defined as the first Pareto optimal location according to some predefined order.
Our main result, the ZV -line graphs family and the mechanism we present for it, unifies the few current works in the literature of false-name-proof facility location on discrete graphs, including the preliminary (unpublished) works we are aware of. Finally, we discuss some generalizations and limitations of our result for problems of facility location on other structures.

",Facility location;  Strategy-proofness;  False-name-proofness; ZV-line graphs,Ilan,,Nehama,Bar-Ilan University,Taiki,,Todo,Kyushu University & RIKEN AIP,Makoto,,Yokoo,Kyushu University & RIKEN AIP,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0532,Beyond Autonomy: The Self and Life of Social Agents,Blue Sky Ideas Track,"Agents have gained popularity nowadays as virtual assistants and companions of their human users supporting daily activities in many aspects of personal life. Designed to be sociable, an agent engages its user(s) to communicate and even develop friendships. Rather than just as a lifeless toy, it is supposed to be perceived as an individual with its own personality, experiences, and social life. In this paper, we seek to highlight self-hood as another dimension that characterizes an agent. Besides levels of autonomy and reasoning, an agent can be defined based on its capacity to process and reflect on its own self as an individual that possesses identity, embodiment, mind (mental), social relationship with others, and experiences comprising memories about the past and future prospects. We argue that this self-awareness is necessary for a companion agent to engage seamlessly with people as a real actual individual. Some existing implementations and models from preliminary works on agent's self-awareness illustrate the feasibility and challenges to realize this concept. Beyond assistance and companionship, we also envisage that this model of self is applicable to other types of autonomous application and system involving extensive interaction with people potentially tackling moral and ethical issues.",Self-Awareness; Autonomy; Social Cognition,Budhitama,,Subagdja,Nanyang Technological University,Ah-Hwee,,Tan,Nanyang Technological University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0533,Learning Plans by Acquiring Grounded Linguistic Meanings from Corrections,,"We motivate and describe a novel task which is modelled on interactions between apprentices and expert teachers. In the task the agent must learn to build towers which are constrained by rules. Whenever the agent performs an action which violates a rule the teacher provides verbal corrective feedback (e.g. ``No, put red blocks on blue blocks'') and answers the learner's clarification questions. The agent must learn to build rule compliant towers from these corrections and the context in which they were given. The agent starts out unaware of the constraints as well as the domain concepts in which the constraints are expressed. Therefore an agent that takes advantage of the linguistic evidence must  learn the denotations of neologisms and adapt its conceptualisation of the planning domain to incorporate those denotations.  We show that an agent which does utilise linguistic evidence outperforms a strong baseline which does not.",human-robot interaction; interactive learning; knowledge representation and reasoning,Mattias,,Appelgren,The University of Edinburgh,Alex,,Lascarides,University of Edinburgh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0535,Buyer Signaling Games in Auctions,,"We consider an auction setting where a seller sells one item to several buyers. Before a buyer's type is realized, he can commit himself to a so-called signal scheme. Mathematically, a signal scheme can be regarded as a linear decomposition of his prior type distribution into a probability distribution over a set of posterior distributions, each of which the seller can use a revenue optimal auction tailored for that distribution. It is known, from the literature of Bayes persuasion, that such signal schemes can lead to utility increase for both the seller and the buyers.

Our goal, is to analyze how a buyer should signal his distribution, given that other buyers may also signal their distributions. In other words, we want to find an equilibrium profile of signal schemes.

We obtain the closed-form solution for the single buyer case with regular distributions, and the multiple buyers case with symmetric type distributions under certain conditions. To prove our technique results, we also obtain some interesting intermediate results. In particular, we show that, if each buyer's signal scheme is to decompose his prior distribution into a set of posteriors that has the same virtual value function (in the exact sense of Myerson's virtual value function), his expected utility is equal to his utility in a first price auction game where his bidding function is always his virtual value function. Furthermore, perhaps surprisingly, we show that, certain distributions, including the uniform distribution, satisfy the property that every buyer's optimal signal scheme is indeed to decompose the prior into a set of posteriors that has the same virtual value function. As a result, we give the closed-form of an equilibrium profile of signal schemes for these cases.",Signaling game; Auction; Equilibrium,Weiran,,Shen,"IIIS, Tsinghua University",Pingzhong,,Tang,"IIIS, Tsinghua University",Yulong,,Zeng,"IIIS, Tsinghua University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0536,Testing Individual-Based Stability Properties in Graphical Hedonic Games,,"In hedonic games, players form coalitions based on individual preferences over the group of players they belong to. Several concepts to describe the stability of coalition structures in a game have been proposed and analysed. However, prior research focuses on algorithms with time complexity that is at least linear in the input size. In the light of very large games that arise from, e.g., social networks and advertising, we initiate the study of sublinear time property testing algorithms for existence and verification problems under several notions of coalition stability in a model of hedonic games represented by graphs with bounded degree. In graph property testing, one shall decide whether a given input has a property (e.g., a game admits a stable coalition structure) or is far from it, i.e., one has to modify at least an $\epsilon$-fraction of the input (e.g., the game's preferences) to make it have the property. In particular, we consider verification of perfection, individual rationality, Nash stability, and (contractual) individual stability. Furthermore, we show that while there is always a Nash-stable coalition (which also implies individually stable coalitions), the existence of a perfect coalition can be tested. All our testers have one-sided error and time complexity that is independent of the input size.",cooperative games; sublinear algorithms; hedonic games; stability; property testing,Hendrik,,Fichtenberger,TU Dortmund University,Amer,,Krivošija,TU Dortmund University,Anja,,Rey,TU Dortmund University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0539,Can Sophisticated Dispatching Strategy Acquired by Reinforcement Learning?,A Case Study in Dynamic Courier Dispatching System,"In this paper, we study a courier dispatching problem (CDP) raised from an online pickup-service platform of Alibaba. The CDP aims to assign a set of couriers to serve pickup requests with stochastic spatial and temporal arrival rate among urban regions. The objective is to maximize the revenue of served requests given a limited number of couriers over a period of time. Many online algorithms such as dynamic matching and vehicle routing strategy from existing literature could be applied to tackle this problem. However, these methods rely on appropriately predefined optimization objectives at each decision point, which is hard in dynamic situations. This paper formulates the CDP as a Markov decision process (MDP) and proposes a data-driven approach to derive the optimal dispatching rule-set under different scenarios. Our method stacks multi-layer images of the spatial-and-temporal map and apply multi-agent reinforcement learning (MARL) techniques to evolve dispatching models. This method solves the learning inefficiency caused by traditional centralized MDP modeling. Through comprehensive experiments on both artificial dataset and real-world dataset, we show: 1) By utilizing historical data and considering long-term revenue gains, MARL achieves better performance than myopic online algorithms; 2) MARL is able to construct the mapping between complex scenarios to sophisticated decisions such as the dispatching rule. 3) MARL has the scalability to adopt in large-scale real-world scenarios.",Multi-agent reinforcement learning; Courier dispatching problem; Smart cities,Yujie,,Chen,"Zhejiang Cainiao Supply Chain Management Co., Ltd",Yu,,Qian,"Zhejiang Cainiao Supply Chain Management Co., Ltd",Yichen,,Yao,"Zhejiang Cainiao Supply Chain Management Co., Ltd",Zili,,Wu,"Zhejiang Cainiao Supply Chain Management Co., Ltd",Rongqi,,Li,"Zhejiang Cainiao Supply Chain Management Co., Ltd",Yinzhi,,Zhou,"Zhejiang Cainiao Supply Chain Management Co., Ltd",Haoyuan,,Hu,"Zhejiang Cainiao Supply Chain Management Co., Ltd",Yinghui,,Xu,"Zhejiang Cainiao Supply Chain Management Co., Ltd",,,,,,,,,,,,,,,,
fp0547,Truthful Mechanisms for Location Games of Dual-Role Facilities,,"This paper is devoted to the facility location games with payments, where every agent plays a dual role of facility and customer. In this game, each selfish agent is located on a publicly known location in a metric space, {and can allow a facility to be opened at his place. But the opening cost is his private information and he may} strategically report this opening cost. Besides, each agent also bears a service cost equal to the distance to his nearest open facility. We are concerned with designing truthful mechanisms for the game, which, given agents' reports, output a set of agents whose facilities could be opened, and a payment to each of these agents who opens a facility. The objective is to minimize (exactly or approximately) the social cost (the total opening and service costs) or the maximum agent cost of the outcome.

We characterize the normalized truthful mechanisms for this game. Concerning the minimum social-cost objective, we give an optimal truthful mechanism without regard to time complexity, and show a small gap between the best known approximation ratio of polynomial-time truthful mechanisms for the game and that of polynomial-time approximation algorithms for the counterpart of pure optimization. For the minimum maximum-cost objective, we provide an optimal truthful mechanism which runs in polynomial time. We also investigate mechanism design for the game under a budget on the total payment.",Truthful mechanism design; Facility location game; Payments,Xujin,,Chen,"Academy of Mathematics and Systems Science, Chinese Academy of Sciences",Minming,,Li,City University of Hong Kong,Changjun,,Wang,Beijing University of Technology,Chenhao,,Wang,City University of Hong Kong,Yingchao,,Zhao,Caritas Institute of Higher Education,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0549,A Grounded Interaction Protocol for Explainable Artificial Intelligence,,"Explainable Artificial Intelligence (XAI) systems need to include an explanation model to communicate the internal decisions, behaviours and actions to the interacting humans. Successful explanation involves both cognitive and social processes. In this paper we focus on the challenge of meaningful interaction between an explainer and an explainee and investigate the structural aspects of an interactive explanation to propose an interaction protocol. We follow a bottom-up approach to derive the model by analysing transcripts of different explanation dialogue types with 398 explanation dialogues. We use grounded theory to code and identify key components of an explanation dialogue. We formalize the model using the agent dialogue framework (ADF) as a new dialogue type and then evaluate it in a human-agent interaction study with 101 dialogues from 14 participants. Our results show that the proposed model can closely follow the explanation dialogues of human-agent conversations.",Explainable AI; Interpretable Machine Learning; Dialogue Model; Human-Agent Interaction,Prashan,,Madumal,University of Melbourne,Tim,,Miller,University of Melbourne,Liz,,Sonenberg,University of Melbourne,Frank,,Vetere,University of Melbourne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0550,Irony Man: Augmenting a Social Robot with the Ability to Use Irony in Multimodal Communication with Humans,,"Interpersonal communication is often full of irony and irony related humor, which can shape the quality of a conversation and how conversation partners perceive each other. If social robots were able to integrate irony in their communication style, their human conversation partners might perceive them as more natural, credible, and ultimately more attractive and acceptable. In order to explore this assumption, we first describe an approach to transform non-ironic inputs on-the-fly into multimodal ironic utterances. Irony markers are used to adapt language, prosody and facial expression. We argue that doing this allows to dynamically enrich a robot's spoken language with an expression of socially intelligent behavior. We then demonstrate the feasibility of our approach by reporting on a user study, which compares an ironic version of a robot with a non-ironic version of the same robot in a small talk dialog scenario. Results show that participants are indeed able to correctly identify a robot's use of irony and that a better user experience is associated with an ironic robot version. This is an important step for dynamically shaping a robot's personality and humor, and to increase perceived social intelligence.",social robots; irony; humor; human-robot interaction; language; social intelligence; small talk; natural language processing; natural language generation; irony marker; irony factor; facial expression; prosody;,Hannes,,Ritschel,Augsburg University,Ilhan,,Aslan,Augsburg University,David,,Sedlbauer,Augsburg University,Elisabeth,,André,Augsburg University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0552,Playing Atari with Six Neurons,,"Deep reinforcement learning, applied to vision-based problems like Atari games, maps pixels directly to actions; internally, the deep neural network bears the responsibility of both extracting useful information and making decisions based on it. By separating the image processing from decision-making, one could better understand the complexity of each task, as well as potentially find smaller policy representations that are easier for humans to understand and may generalize better. To this end, we propose a new method for learning policies and compact state representations separately but simultaneously for policy approximation in reinforcement learning. State representations are generated by an encoder based on two novel algorithms: Increasing Dictionary Vector Quantization makes the encoder capable of growing its dictionary size over time, to address new observations as they appear in an open-ended online-learning context; Direct Residuals Sparse Coding encodes observations by disregarding reconstruction error minimization, and aiming instead for highest information inclusion. The encoder autonomously selects observations online to train on, in order to maximize code sparsity. As the dictionary size increases, the encoder produces increasingly larger inputs for the neural network: this is addressed by a variation of the Exponential Natural Evolution Strategies algorithm which adapts its probability distribution dimensionality along the run. We test our system on a selection of Atari games using tiny neural networks of only 6 to 18 neurons (depending on the game's controls). These are still capable of achieving results comparable---and occasionally superior---to state-of-the-art techniques which use two orders of magnitude more neurons.",Game Playing; Neuroevolution; Evolutionary algorithms; Learning agent capabilities,Giuseppe,,Cuccu,University of Fribourg,Julian,,Togelius,New York University,Philippe,,Cudré-Mauroux,University of Fribourg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0563,A Fully Rational Argumentation System for Preordered Defeasible Rules,,"Structured argumentation is a family of formal approaches for the handling of defeasible, potentially inconsistent information. Many models for structured argumentation distinguish between strict and defeasible inference rules. Defeasible rules often come with varying degrees of strength which is formally represented by a preorder over the defeasible rules. Various lifting principles have been presented in the literature to determine the relative strength of an argument by considering the strength of the defeasible rules used in its construction. The strength of arguments then comes into play when determining whether an attack (a purely syntactic relationship between arguments) results in a defeat (i.e. a successful attack). In \cite{caminada2007evaluation,Wu2012}, several rationality postulates were proposed that serve as a measure to assess the normative rationality of structured argumentation formalisms. In \cite{heyninck2017revisiting}, the first formalism satisfying all rationality postulates for structured argumentation when taking into account totally ordered defeasible rules was proposed. In many settings, assuming a total order greatly limits the realistic modelling capabilities of a formal system, e.g. when agents do not know the actual preferences of each rule or since different agents have different preferences over defeasible rules. Our paper shows that in the more general setting of preorders, violations of several rationality postulates can occur. We show how for a wide class of lifting principles, these violations can be avoided, resulting in the first Dung-based system that satisfies all four rationality postulates for preordered defeasible rule bases.",Non-monotonic Reasoning; Preferences; Formal Argumentation; Structured Argumentation,Jesse,,Heyninck,Ruhr-University Bochum,Christian,,Straßer,Ruhr-University Bochum,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0570,Well-behaved Online Load Balancing Against Strategic Jobs,,"In the online load balancing problem on related machines, we have a set of jobs (with different sizes) arriving online, and we need to assign each job to a machine immediately upon its arrival, so as to minimize the makespan, i.e., the maximum completion time. In classic mechanism design problems, we assume that the jobs are controlled by selfish agents, with the sizes being their private information. Each job (agent) aims at minimizing its own cost, which is its completion time plus the payment charged by the mechanism. Truthful mechanisms guaranteeing that every job minimizes its cost by reporting its true size have been well-studied [Aspnes et al. JACM 1997, Feldman et al. EC 2017].

In this paper, we study truthful online load balancing mechanisms that are well-behaved [Epstein et al., MOR 2016]. Well-behavior is important as it guarantees fairness between machines, and implies truthfulness in some cases when machines are controlled by selfish agents. Unfortunately, existing truthful online load balancing mechanisms are not well-behaved. We first show that to guarantee producing a well-behaved schedule, any online algorithm (even non-truthful) has a competitive ratio at least $\Omega(\sqrt{m})$, where $m$ is the number of machines. Then we propose a mechanism that guarantees truthfulness of the online jobs, and produces a schedule that is almost well-behaved. We show that our algorithm has a competitive ratio of O(\log m). Moreover, for the case when the sizes of online jobs are bounded, the competitive ratio of our algorithm improves to O(1). Interestingly, we show several cases for which our mechanism is actually truthful against selfish machines.",Online Load Balancing; Truthful Mechanism; Well-behaved,Bo,,Li,Stony Brook University,Minming,,Li,City University of Hong Kong,Xiaowei,,Wu,City University of Hong Kong & University of Vienna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0582,Multiple Assignment Problems under Lexicographic Preferences,,"We study the problem of allocating multiple objects to agents without transferable utilities, where each agent may receive more than one object according to a quota.
Under lexicographic preferences, we characterize the set of strategyproof, non-bossy, and neutral quota mechanisms and show that under a mild Pareto efficiency condition, serial dictatorship quota mechanisms are the only mechanisms satisfying these properties. We then extend quota mechanisms to randomized settings, and show that the random serial dictatorship quota mechanisms (RSDQ) are envyfree, strategyproof, and ex post efficient for any number of agents and objects and any quota system, proving that the well-studied Random Serial Dictatorship (RSD) satisfies envyfreeness when preferences are lexicographic.",Multiple assignment; Random allocation; Strategyproofness; Lexicographic preferences,Hadi,,Hosseini,Rochester Institute of Technology,Kate,,Larson,University of Waterloo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0583,Exploring Improvisational Approaches to Social Knowledge Acquisition,,"To build agents that can engage user in more open-ended social contexts, more and more attention has been focused on data-driven approaches to reduce the requirement of extensive, hand-authored behavioral content creation. However, one fundamental challenge of data-driven approaches, is acquiring human social interaction data with sufficient variety to capture more open-ended social interactions, as well as their coherency. Previous work has attempted to extract such social knowledge using crowdsourced narratives.

This paper proposes an approach to acquire the knowledge of social interaction by integrating an improvisational theatre training technique into a crowdsourcing task aimed at collecting social narratives. The approach emphasizes theory of mind concepts, through an iterative prompting process about the mental states of characters in the narrative and paired writing, in order to encourage the authoring of diverse social interactions. To assess the effectiveness of integrating prompting and two-worker improvisation to the knowledge acquisition process, we systematically compare alternative ways to design the crowdsourcing task, including a) single worker vs. two workers authoring interaction between different characters in a given social context, and b) with or without prompts. Findings from 175 participants across two different social contexts show that the prompts and two-workers collaboration could significantly improve the diversity and the objective coherency of the narratives. The results presented in this paper can provide a rich set of diverse and coherent action sequences to inform the design of socially intelligent agents.",social knowledge acquisition; crowdsourcing; online collaboration,Dan,,Feng,Northeastern University,Elin,,Carstensdottir,Northeastern University,Magy,,Seif El-Nasr,Northeastern University,Stacy,,Marsella,University of Glasgow,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0585,Online Abstraction with MDP Homomorphisms for Deep Learning,,"Abstraction of Markov Decision Processes is a useful tool for solving complex problems, as it can ignore unimportant aspects of an environment, simplifying the process of learning an optimal policy. In this paper, we propose a new algorithm for finding abstract MDPs in environments with continuous state spaces. It is based on MDP homomorphisms, a structure-preserving mapping between MDPs. We demonstrate our algorithm's ability to learn abstractions from collected experience and show how to reuse the abstractions to guide exploration in new tasks the agent encounters. Our novel task transfer method outperforms baselines based on a deep Q-network in the majority of our experiments. The source code is at~\url{https://github.com/ondrejba/aamas_19}.",reinforcement learning; abstraction; mdp homomorphism; transfer learning; deep learning,Ondrej,,Biza,Czech Technical University,Robert,,Platt,Northeastern University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0589,Efficient Allocation of Free Stuff,,"We study online matching settings with selfish agents when everything is free. Inconsiderate agents break ties arbitrarily amongst equal maximal value available choices, even if the maximal value is equal to zero.
\par
Even for the simplest case of zero/one valuations, where agents arrive online in an arbitrary order, and agents are restricted to taking at most one item, the resulting social welfare may be negligible for a deterministic algorithm. This may be surprising when contrasted with the 1/2 approximation of the greedy algorithm, analogous to this setting, except that agents are considerate (i.e., they don't take zero-valued items).
\par
We overcome this challenge by introducing a new class of algorithms, which we refer to as prioritization algorithms.
We show that upgrading a random subset of the agents to ``business class"" already improves the approximation to a constant.
For more general valuations, we achieve a constant approximation using $\log n$ priority classes, when the valuations are known in advance. We extend these results to settings where agents have additive valuations and are restricted to taking up to some $q\geq 1$ items. Our results are tight up to a constant.",Online matching; Welfare approximation; Selfish agents,Yossi,,Azar,Tel Aviv University,Allan,,Borodin,University of Toronto,Michal,,Feldman,Tel Aviv University,Amos,,Fiat,Tel Aviv University,Kineret,,Segal,Tel Aviv University,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0593,Generalized Matching Games for International Kidney Exchange,,"We introduce generalized matching games defined on a graph $G=(V,E)$ with an edge weighting $w$ and a partition $V=V_1 \cup \dots \cup V_n$ of~$V$. The player set is $N = \{ 1, \dots, n\}$, and player $p \in N$ owns the vertices in $V_p$. The value $v(S)$ of coalition $S \subseteq N$ is the maximum weight of a matching in  the subgraph of $G$  induced by the vertices owned by players in $S$. If $|V_p|=1$ for every player~$p$ we obtain the classical matching game. We prove that checking core non-emptiness is polynomial-time solvable if $|V_p|\leq 2$ for each $p$ and co-\NP-hard if $|V_p|\leq 3$ for each $p$. We do so via pinpointing a relationship with $b$-matching games and also settle the complexity classification on testing core non-emptiness for $b$-matching games. We  propose generalized matching games as a suitable model for international kidney exchange programs, where the vertices in $V$ correspond to patient-donor pairs and each $V_p$ represents one country. For this setting we prove a number of complexity results.",kidney exchanges; matching game; core; computational complexity,Péter,,Biró,"Institute of Economics, Hungarian Academy of Sciences",Walter,,Kern,University of Twente,Dömötör,,Pálvölgyi,ELTE,Daniel,,Paulusma,Durham University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0599,Information Gathering in Decentralized POMDPs by Policy Graph Improvement,,"Decentralized policies for information gathering are required when multiple autonomous agents are deployed to collect data about a phenomenon of interest without the ability to communicate.
Decentralized partially observable Markov decision processes (Dec-POMDPs) are a general, principled model well-suited for such decentralized multiagent decision-making problems.
In this paper, we investigate Dec-POMDPs for decentralized information gathering problems.
An optimal solution of a Dec-POMDP maximizes the expected sum of rewards over time.
To encourage information gathering, we set the reward as a function of the agents' state information, for example the negative Shannon entropy.
We prove that if the reward is convex, then the finite-horizon value function of the corresponding Dec-POMDP is also convex.
We propose the first heuristic algorithm for information gathering Dec-POMDPs, and empirically prove its effectiveness by solving problems an order of magnitude larger than previous state-of-the-art.",decentralized POMDPs; multi-agent planning; planning under uncertainty; information theory,Mikko,,Lauri,University of Hamburg,Joni,,Pajarinen,TU Darmstadt,Jan,,Peters,TU Darmstadt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp060,Monotonicity Axioms in Approval-based Multi-winner Voting Rules,,"In this paper we study several monotonicity axioms in approval-based
multi-winner voting rules. We consider monotonicity with respect to
the support received by the winners and also monotonicity in the size
of the committee. Monotonicity with respect to the support is studied
when the set of voters does not change and when new voters enter the
election. For each of these two cases we consider a strong and a weak
version of the axiom. We observe certain incompatibilities between the
monotonicity axioms and well-known representation axioms
(extended/proportional justified representation) for the voting rules
that we analyze, and provide formal proofs of incompatibility between
some monotonicity axioms and perfect representation.
",Multi-winner voting rules; approval ballots; monotonicity axioms,Luis,,Sanchez-Fernandez,Universidad Carlos III de Madrid,Jesus,A.,Fisteus,Universidad Carlos III de Madrid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0609,Soft Labeling in Stochastic Shortest Path Problems,,"The Stochastic Shortest Path (SSP) problem is an established model for goal-directed probabilistic planning.  Despite its broad applicability, wide adoption of the model has been impaired by its high computational complexity. Efforts to address this challenge have produced promising algorithms that leverage two popular mechanisms: labeling and short-sightedness.  The resulting algorithms can generate near-optimal solutions much faster than optimal solvers, albeit at the cost of poor theoretical guarantees. In this work, we introduce a generalization of labeling, called soft labeling, which results in a framework that encompasses a wide spectrum of efficient labeling algorithms, and offers better theoretical guarantees than existing short-sighted labeling approaches. We also propose a novel instantiation of this framework, the soft-FLARES algorithm, which achieves state-of-the-art performance on a diverse set of benchmarks.",Markov decision processes; probabilistic planning; short-sighted algorithms; stochastic shortest path problems,Luis,,Pineda,University of Massachusetts Amherst & Facebook AI Research,Shlomo,,Zilberstein,University of Massachusetts Amherst,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0617,The Effect of Virtual Agent Warmth on Human-Agent Negotiation,,"The perception of warmth and competence in others influences social interaction and decision making. Virtual agents have been used in many domains including serious gaming and training. In this work we study the effect of warmth expressed in the behavior of a virtual agent on a human-agent negotiation. We design and conduct an experiment where participants negotiate with two versions of the same agent displaying varying levels of warmth. The results show that humans are more satisfied with the warm agent, are more willing to renegotiate with it, would recommend the agent more to their friends and had a better interaction experience, even though there is no difference in negotiation outcome (utility, agreement or rounds needed). While studies have shown effects of emotional displays on negotiation and collaboration, this is - to our knowledge - the first time that a clear effect of behavioral style is shown on the post-hoc appraisal of a human-agent collaboration, in our case a negotiation.",Negotiation; Virtual Agents; Warmth; Agent Modelling,Pooja,,Prajod,Delft University of Technology,Mohammed,,Al Owayyed,Delft University of Technology,Tim,,Rietveld,Delft University of Technology,Jaap-Jan,,van der Steeg,Delft University of Technology,Joost,,Broekens,Delft University of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0618,Using Causal Analysis to Learn Specifications from Task Demonstrations,,"Learning models of user behaviour is an important problem that is broadly applicable across many application domains requiring human-robot interaction. In this work we show that it is possible to learn a generative model for distinct user behavioral types, extracted from human demonstrations, by enforcing clustering of preferred task solutions within the latent space. We use this model to differentiate between user types and to find cases with overlapping solutions. Moreover, we can alter an initially guessed solution to satisfy the preferences that constitute a particular user type by backpropagating through the learned differentiable model. An advantage of structuring generative models in this way is that it allows us to extract causal relationships between symbols that might form part of the user's specification of the task, as manifested in the demonstrations. We show that the proposed method is capable of correctly distinguishing between three user types, who differ in degrees of cautiousness in their motion, while performing the task of moving objects with a kinesthetically driven robot in a tabletop environment. Our method successfully identifies the correct type, within the specified time, in 99% [97.8 - 99.8] of the cases, which outperforms an IRL baseline. We also show that our proposed method correctly changes a default trajectory to one satisfying a particular user specification even with unseen objects. The resulting trajectory is shown to be directly implementable on a PR2 humanoid robot completing the same task.",Human-robot interaction; robot learning; explainability,Daniel,,Angelov,The University of Edinburgh,Yordan,,Hristov,The University of Edinburgh,Subramanian,,Ramamoorthy,The University of Edinburgh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0624,"Efficiency, Sequenceability and Deal-Optimality in Fair Division of Indivisible Goods",,"In fair division of indivisible goods, using sequences of sincere choices (or picking sequences) is a natural way to allocate the objects. The idea is as follows: at each stage, a designated agent picks one object among those that remain. Another intuitive way to obtain an allocation is to give objects to agents in the first place, and to let agents exchange them as long as such ""deals"" are beneficial. This paper investigates these notions, when agents have additive preferences over objects, and unveils surprising connections between them, and with other efficiency and fairness notions. In particular, we show that an allocation is sequenceable if and only if it is optimal for a certain type of deals, namely cycle deals involving a single object.  Furthermore, any Pareto-optimal allocation is sequenceable, but not the converse. Regarding fairness, we show that an allocation can be envy-free and non-sequenceable, but that every competitive equilibrium with equal incomes is sequenceable. To complete the picture, we show how some domain restrictions may affect the relations between these notions. Finally, we experimentally explore the links between the scales of efficiency and fairness.",Multiagent Resource Allocation; Fair Division; Efficiency,Aurélie,,Beynier,"LIP6, Sorbonne Université",Sylvain,,Bouveret,Grenoble INP - Univ. Grenoble-Alpes LIG,Michel,,Lemaître,Formerly ONERA,Nicolas,,Maudet,"LIP6, Sorbonne Université",Simon,,Rey,"CNRS, ENS Paris-Saclay",Parham,,Shams,"LIP6, Sorbonne Université",,,,,,,,,,,,,,,,,,,,,,,,
fp0632,Newtonian Action Advice: Integrating Human Verbal Instruction with Reinforcement Learning,,"A goal of Interactive Machine Learning is to enable people without specialized training to teach agents how to perform tasks. Many of the existing algorithms that learn from human instructions are evaluated using simulated feedback and focus on how quickly the agent learns. While this is valuable information, it ignores important aspects of the human-agent interaction such as frustration. To correct this, we propose a method for the design and verification of interactive algorithms that includes a human-subject study that measures the human's experience working with the agent. In this paper, we present Newtonian Action Advice, a method of incorporating human verbal action advice with Reinforcement Learning in a way that improves the human-agent interaction. In addition to simulations, we validated the Newtonian Action Advice algorithm by conducting a human-subject experiment. The results show that Newtonian Action Advice can perform better than Policy Shaping, a state-of-the-art IML algorithm, both in terms of RL metrics like cumulative reward and human factors metrics like frustration.",Interactive Machine Learning; Learning from Human Teachers; Reinforcement Learning; Natural Language Interface; Human-Subject Experiment; Verification,Samantha,,Krening,Georgia Institute of Technology,Karen,M.,Feigh,Georgia Institute of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0634,Balanced task allocation by partitioning the multiple traveling salesperson problem,,"Task assignment and routing are tightly coupled problems for teams of mobile agents. To fairly balance the workload, each agent should be assigned a set of tasks which take a similar amount of time to complete. The completion time depends on the time needed to travel between tasks which depends on the order of tasks. We formulate the task assignment problem as the minimum Hamiltonian partition problem (MHPP) for m agents, which is equivalent to the minmax multiple traveling salesperson problem (m-TSP). While the MHPP's cost function depends on the order of tasks, its solutions are similar to solutions of the average Hamiltonian partition problem (AHPP) whose cost function is order-invariant. We prove that the AHPP is NP-hard and present an effective heuristic, AHP, for solving it. AHP improves a partitions of a graph using a series of transfer and swap operations which are guaranteed to improve the solution's quality. The solution generated by AHP is used as an initial partition for an algorithm, AHP-mTSP, which solves the combined task assignment and routing problems to near optimality. For n tasks and m agents, each iteration of AHP is O(n^2) and AHP-mTSP has an average run-time that scales with n^2.11 m^0.33. Compared to state-of-the-art approaches, our approach found approximately 10% better solutions for large problems in a similar run-time.
",Task allocation; vehicle routing; combinatorial optimization; multi-agent systems,Isaac,,Vandermeulen,The University of Sheffield,Roderich,,Groß,The University of Sheffield,Andreas,,Kolling,iRobot Corporation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0641,Distributed Self-Reconfiguration using a Deterministic Autonomous Scaffolding Structure,,"In the context of large distributed modular robots, self-reconfiguration is the process of having modules, seen as autonomous agents, acting together and moving to transform the morphology of their physical arrangement to produce a desired shape. However, due to motion constraints, the number of modules that can move concurrently is greatly limited, thus making self-reconfiguration a very slow process.\\
In this paper, we propose an approach for accelerating self-reconfiguration to build a porous version of the desired shape, using scaffolding. We expand this idea and propose a method for constructing a parametric scaffolding model that increases the parallelism of the reconfiguration, supports its mechanical stability, and simplifies planning and coordination between agents. Each agent has a set of basic rules using only four states which guarantees that module movements and the construction of the scaffold are deterministic. \\
Coupled with an underneath reserve of modules that allows the introduction of rotating quasi-spherical modules at various ground locations of the growing porous structure, our method is able to build the scaffolding structure in $O(N^{\frac{2}{3}})$ time with $N$ the number of modules composing the structure. Furthermore, we provide simulation results showing that our method uses $O(N^\frac{4}{3})$ messages with no congestion.",Self-Reconfiguration; Autonomous Robots; Distributed Algorithm,Pierre,,Thalamy,"University of Bourgogne Franche-Comté, FEMTO-ST, CNRS",Benoit,,Piranda,"University of Bourgogne Franche-Comté, FEMTO-ST, CNRS",Julien,,Bourgeois,"University of Bourgogne Franche-Comté, FEMTO-ST, CNRS",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0642,Online Inverse Reinforcement Learning Under Occlusion,,"Inverse reinforcement learning (IRL) is the problem of learning the
preferences of an agent from observing its behavior on a task. While this problem is witnessing sustained attention, the related problem of online IRL – where the observations are incrementally accrued, yet the real-time demands of the application often prohibit a full rerun of an IRL method – has received much less attention. We
introduce a formal framework for online IRL, called incremental
IRL (I2RL), and a new method that advances maximum entropy IRL
with hidden variables, to this setting. Our formal analysis shows that
the new method has a monotonically improving performance with
more demonstration data, as well as probabilistically bounded error,
both under full and partial observability. Experiments in a simulated
robotic application, which involves learning under occlusion, show
the significantly improved performance of online IRL as compared
to both batch IRL and an online imitation learning method.",Robot Learning; Online Learning; Robotics; Reinforcement Learning; Inverse Reinforcement Learning,Saurabh,,Arora,University of Georgia,Prashant,,Doshi,University of Georgia,Bikramjit,,Banerjee,University of Southern Mississippi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0650,Runtime Revision of Norms and Sanctions based on Agent Preferences,,"To fulfill the overall objectives of a multiagent system, the behavior of individual agents should be controlled and coordinated. Runtime norm enforcement is one way to do so without over-constraining the agents' autonomy. Due to the dynamicity and uncertainty of the environment, however, it is hard to specify norms that, when enforced, will fulfill the system-level objectives in every operating context. In this paper, we propose a mechanism for the automated revision of norms by altering their sanctions, based on the data monitored during the system execution and on some knowledge about the agents' preferences. We use a Bayesian Network to learn at runtime the relationship between the obedience/violation of a norm and the achievement of the system objectives. We propose two heuristic strategies that explore the updated Bayesian Network and automatically revise the sanction of an enforced norm. An evaluation of our heuristics using a traffic simulator shows that our mechanisms outperform uninformed heuristics in terms of convergence speed.",Multiagent Systems; Norm Revision; Norm Enforcement; Bayesian Network,Davide,,Dell'Anna,Utrecht University,Mehdi,,Dastani,Utrecht University,Fabiano,,Dalpiaz,Utrecht University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0658,Domain Authoring Assistant for Intelligent Virtual Agent,,"Developing intelligent virtual characters has attracted a lot of attention in the recent years. The process of creating such characters often involves a team of creative authors who describe different aspects of the characters in natural language, and planning experts that translate this description into a planning domain. This can be quite challenging as the team of creative authors should diligently define every aspect of the character especially if it contains complex human-like behavior. Also a team of engineers has to manually translate the natural language description of a character’s personality into the planning domain knowledge. This can be extremely time and resource demanding and can be an obstacle to author’s creativity. The goal of this paper is to introduce an authoring assistant tool to automate the process of domain generation from natural language description of virtual characters, thus bridging between the creative authoring team and the planning domain experts. More- over, the proposed tool also identifies possible missing information in the domain description and iteratively makes suggestions to the author.",Intelligent Virtual Agents; Natural Language Understanding; Text Simplification; Planning Domain Acquisition; Domain Authoring,Sepehr,,Janghorbani,Rutgers University at New Brunswick,Ashutosh,,Modi,Disney Research,Jakob,,Buhmann,Disney Research,Mubbasir,,Kapadia,Rutgers University at New Brunswick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0659,Robust Temporal Difference Learning for Critical Domains,,"We present a new Q-function operator for temporal difference (TD) learning methods that explicitly encodes robustness against significant rare events (SRE) in critical domains. The operator, which we call the $\ko$-operator, allows to learn a robust policy in a model-based fashion without actually observing the SRE. We introduce single- and multi-agent robust TD methods using the operator $\ko$. We prove convergence of the operator to the optimal robust Q-function with respect to the model using the theory of Generalized Markov Decision Processes. In addition we prove convergence to the optimal Q-function of the original MDP given that the probability of SREs vanishes. Empirical evaluations demonstrate the superior performance of $\ko$-based TD methods both in the early learning phase as well as in the final converged stage. In addition we show robustness of the proposed method to small model errors, as well as its applicability in a multi-agent context.",reinforcement learning; robust learning; multi-agent learning,Richard,,Klima,University of Liverpool,Daan,,Bloembergen,Centrum Wiskunde & Informatica (CWI),Michael,,Kaisers,Centrum Wiskunde & Informatica (CWI),Karl,,Tuyls,University of Liverpool,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0660,Dynamic Particle Allocation to Solve Interactive POMDP Models for Social Decision Making,,"In social dilemma settings, such as repeated Public Goods Games (PGGs), humans often come across a dilemma whether to contribute or not based on past contributions from others. In such settings, the decision taken by an agent/human actually depends not only on the belief the agent has about other agents and the environment, but also on their beliefs about others’ beliefs. To factor in these aspects, we propose a novel formulation of computational theory of mind (ToM) to model human behavior in a repeated PGG using interactive partially observable Markov decision processes (I-POMDPs). Interactive particle filter (IPF) is a well-known algorithm used to approximately solve I-POMDP models for the agents to find their optimal contributions. Number of particles assigned to an agent in IPF can be translated into time and computational resources. Solving I-POMDPs in a time-memory efficient manner even in the case of small state spaces is a largely intractable problem. Also, maintaining a fixed number of particles assigned to each agent, over time, will be highly inefficient in terms of resource utilization. To address this problem, we propose a dynamic particle allocation algorithm for different agents based on how well they could predict. We validate our proposed algorithm through real experiments involving human agents. Our results suggest that dynamic particle allocation based IPF for I-POMDPs is effective in modelling human behaviours in repeated social dilemma setting while utilizing computational resources in an effective manner.",Theory-of-Mind modelling; Interactive Partially Observerable Markov Decision Processes; Interactive Particle Filter; Public Good Games; Bayesian analysis; Partially Observable Monte Carlo Planning,Rohith,Dwarakanath,Vallam,IBM Research - India,Sarthak,,Ahuja,Carnegie Mellon University (CMU),Surya Shravan Kumar,,Sajja,IBM Research - India,Ritwik,,Chaudhuri,IBM Research - India,Rakesh,,Pimplikar,IBM Research - India,Kushal,,Mukherjee,IBM Research - India,Ramasuri,,Narayanam,IBM Research - India,Gyana,,Parija,IBM Research - India,,,,,,,,,,,,,,,,
fp0661,Exploring the No-Show Paradox for Condorcet Extensions Using Ehrhart Theory and Computer Simulations,,"Results from voting theory are increasingly used when dealing with collective decision making in computational multiagent systems. An important and surprising phenomenon in voting theory is the No-Show Paradox (NSP), which occurs if a voter is better off by abstaining from an election. While it is known that certain voting rules suffer from this paradox in principle, the extent to which it is of practical concern is not well understood. We aim at filling this gap by analyzing the likelihood of the NSP for six Condorcet extensions (Black's rule, Baldwin's rule, Nanson's rule, MaxiMin, Tideman's rule, and Copeland's rule) under various preference models using Ehrhart theory as well as extensive computer simulations. We find that, for few alternatives, the probability of the NSP is rather small (less than 4% for four alternatives and all considered preference models, except for Copeland's rule). As the number of alternatives increases, the NSP becomes much more likely and which rule is most susceptible to abstention strongly depends on the underlying distribution of preferences.",Social choice theory; voting; no-show paradox; Ehrhart theory,Felix,,Brandt,Technische Universität München,Johannes,,Hofbauer,Technische Universität München,Martin,,Strobel,National University of Singapore,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0667,Model Primitive Hierarchical Lifelong Reinforcement Learning,,"Learning interpretable and transferable subpolicies and performing task decomposition from a single, complex task is difficult. Some traditional hierarchical reinforcement learning techniques enforce this decomposition in a top-down manner, while meta-learning techniques require a task distribution at hand to learn such decompositions. This paper presents a framework for using diverse suboptimal world models to decompose complex task solutions into simpler modular subpolicies. This framework performs automatic decomposition of a single source task in a bottom up manner, concurrently learning the required modular subpolicies as well as a controller to coordinate them. We perform a series of experiments on high dimensional continuous action control tasks to demonstrate the effectiveness of this approach at both complex single task learning and lifelong learning. Finally, we perform ablation studies to understand the importance and robustness of different elements in the framework and limitations to this approach.",Reinforcement learning; Task decomposition; Transfer; Lifelong learning,Bohan,,Wu,Columbia University,Jayesh,K.,Gupta,Stanford University,Mykel,J.,Kochenderfer,Stanford University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0669,Fall if it Lifts your Teammate: A Novel Type of Candidate Manipulation,,"We present a new interpretation of the traditional computational social choice
framework, where what are traditionally the candidates are construed as the
agents. The particular implementation in mind is the proposed system for
determining the medal winners for sports climbing in the 2020 Olympic games. We
consider the issues of ties and of potential manipulation with respect to this
interpretation. Simulation results suggest that for the proposed system ties
are unlikely to be a problem, but that there is at least potential for
manipulation, of a novel type. We formalise this conception of manipulation
axiomatically. The strongest axioms lead to an impossibility along the lines of
Arrow's impossibility, while a small weakening leads to a possibility. We also
provide a hardness result concerning the determination of possible manipulation.
",Social choice theory; Game Theory for practical applications,Justin,,Kruger,Paris Dauphine University and CNRS LAMSADE,Sebastian,,Schneckenburger,University of Tuebingen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0670,The Body is Not a Given: Joint Agent Policy Learning and Morphology Evolution,,"Reinforcement learning (RL) has proven to be a powerful paradigm for deriving complex behaviors from simple reward signals in a wide range of environments. When applying RL to continuous control agents in simulated physics environments, the body is usually considered to be part of the environment. However, during evolution the physical body of biological organisms and their controlling brains are co-evolved, thus exploring a much larger space of actuator/controller configurations. Put differently, the intelligence does not reside only in the agent's mind, but also in the design of their body.
We propose a method for uncovering strong agents, consisting of a good combination of a body and policy, based on combining RL with an evolutionary procedure. Given the resulting agent, we also propose an approach for identifying the body changes that contributed the most to the agent performance. We use the Shapley value from cooperative game theory to find the fair contribution of individual components, taking into account synergies between components.
We evaluate our methods in an environment similar to the the recently proposed Robo-Sumo task, where agents in a software physics simulator compete in tipping over their opponent or pushing them out of the arena. Our results show that the proposed methods are indeed capable of generating strong agents, significantly outperforming baselines that focus on optimizing the agent policy alone.

A video is available at: https://youtu.be/CHlecRim9PI",Reinforcement Learning; Evolutionary Computation,Dylan,,Banarse,DeepMind,Yoram,,Bachrach,DeepMind,Siqi,,Liu,DeepMind,Guy,,Lever,DeepMind,Nicolas,,Heess,DeepMind,Chrisantha,,Fernando,DeepMind,Pushmeet,,Kohli,Deepmind,Thore,,Graepel,DeepMind,,,,,,,,,,,,,,,,
fp0675,Urban Driving with Multi-Objective Deep Reinforcement Learning,,"Autonomous driving is a challenging domain that entails multiple aspects: a vehicle should be able to drive to its destination as fast as possible while avoiding collision, obeying traffic rules and ensuring the comfort of passengers. In this paper, we present a deep learning variant of \emph{thresholded lexicographic Q-learning} for the task of urban driving. Our multi-objective DQN agent learns to drive on multi-lane roads and intersections, yielding and changing lanes according to traffic rules. We also propose an extension for \emph{factored Markov Decision Processes} to the DQN architecture that provides auxiliary features for the Q function. This is shown to significantly improve data efficiency.~\footnote{Data efficiency as measured by the number of training steps required to achieve similar performance.} We then show that the learned policy is able to zero-shot transfer to a ring road without sacrificing performance.",reinforcement learning; multi-objective optimization; markov decision process (MDP); deep learning; autonomous driving,Changjian,,Li,University of Waterloo,Krzysztof,,Czarnecki,University of Waterloo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0676,Decentralised Planning for Multi-Agent Programming Platforms,,"Extensive research has been done on planning for single-agent problems, but multi-agent planning has not as yet been thoroughly explored in agent development platforms. These platforms typically provide various mechanisms for runtime coordination, which are often useful in online planning. In this context decentralised multi-agent planning can be efficient as well as effective, especially in loosely-coupled domains, whilst also ensuring important properties in agent systems such as privacy and autonomy. In this paper, we describe the DOMAP planning framework and its integration with a multi-agent programming platform to support the achievement of social goals. Our planning framework has separate phases for goal allocation and individual HTN planning, whilst relying on available runtime coordination. Experiments on three different multi-agent systems implemented in JaCaMo show that DOMAP outperforms four other state-of-the-art multi-agent planners with regards to both planning and execution time.",multi-agent planning; hierarchical task network; multi-agent development platforms; goal allocation,Rafael,C.,Cardoso,University of Liverpool,Rafael,H.,Bordini,PUCRS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0690,Minimizing Travel in the Uniform Dispersal Problem for Robotic Sensors,,"The limited energy capacity of individual robotic agents in a swarm
often limits the possible cooperative tasks they can perform. In this
work, we investigate the problem of covering an unknown connected
grid environment (e.g. a maze or connected corridors) with a
robotic swarm so as to minimize the maximal number of steps that
each member of the swarm makes and their activity time before
their work is finished, thereby minimizing the energy requirements.
The robots are autonomous, anonymous and identical, with local
sensors and finite memory, and possess no communication capabilities.
They are assumed to disperse over time from a fixed location,
and to move synchronously. The robots are tasked with occupying
every cell of the environment, while avoiding collisions.</par><par>In the literature such topics are known as uniform dispersal problems. The goal of minimizing the number of steps traveled by the robots has previously been studied in this context. Our
contribution is a local robotic strategy for simply connected grid
environments that, by exploiting their topology, achieves optimal
makespan (the amount of time it takes to cover the environment)
and minimizes the maximal number of steps taken by the individual
robots before their deactivation. The robots succeed in discovering
optimal paths to their eventual destinations, and finish the covering
process in 2V ? 1 time steps, where V is the number of cells in the environment.",Mobile robot; Minimizing movement; Unknown environment; Uniform dispersal; Grid environment; Area coverage,Michael,,Amir,Technion -- Israel Institute of Technology,Alfred,M.,Bruckstein,Technion -- Israel Institute of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0692,Complexity of Manipulation in Premise-Based Judgment Aggregation with Simple Formulas,,"Judgment aggregation is a framework to aggregate individual opinions on multiple, logically connected issues into a collective outcome. It is open to manipulative attacks such as \textsc{Manipulation} where judges cast their judgments strategically. Previous works have shown that most computational problems corresponding to these manipulative attacks are \NP-hard. This desired computational barrier, however, often relies on formulas that are either of unbounded size or of complex structure.

We revisit the computational complexity for a large class of \textsc{Manipulation} problems in judgment aggregation, now focusing on simple and realistic formulas. We restrict all formulas to be clauses that are (positive) monotone, Horn-clauses, or have bounded length. For basic variants of \textsc{Manipulation}, we show that these restrictions make several variants, which were in general known to be \NP-hard, polynomial-time solvable. Moreover, we provide a P vs.\ NP dichotomy for a large class of clause restrictions (generalizing monotone and Horn clauses) by showing a close relationship between variants of \textsc{Manipulation} and variants of \textsc{Satisfiability}. For Hamming distance based \textsc{Manipulation}, we show that \NP-hardness even holds for positive monotone clauses of length three, but the problem becomes polynomial-time solvable for positive monotone clauses of length two.",Judgment Aggregation; Social Choice Theory; Computational Complexity,Robert,,Bredereck,TU Berlin,Junjie,,Luo,University of Chinese Academy of Sciences,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0695,Monte Carlo Continual Resolving for Online Strategy Computation in Imperfect Information Games,,"Online game playing algorithms produce high-quality strategies with a fraction of memory and computation required by their offline alternatives.
Continual Resolving (CR) is a recent theoretically sound approach to online game playing that has been used to outperform human professionals in poker.
However, parts of the algorithm were specific to poker, which enjoys many properties not shared by other imperfect information games.
We present a domain-independent formulation of CR applicable to any two-player zero-sum extensive-form games (EFGs).
It works with an abstract resolving algorithm, which can be instantiated by various EFG solvers.
We further describe and implement its Monte Carlo variant (MCCR) which uses Monte Carlo Counterfactual Regret Minimization (MCCFR) as a resolver.
We prove the correctness of CR and show an $O(T^{-1/2})$-dependence of MCCR's exploitability on the computation time.
Furthermore, we present an empirical comparison of MCCR with incremental tree building to Online Outcome Sampling and Information-set MCTS on several domains.",counterfactual regret minimization; resolving; imperfect information; Monte Carlo; online play; extensive-form games; Nash equilibrium,Michal,,Šustr,Czech Technical University,Vojtech,,Kovarik,Czech Technical University,Viliam,,Lisý,Czech Technical University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0697,Between the Megalopolis and the Deep Blue Sky: Challenges of Transport with UAVs in Future Smart Cities,Blue Sky Ideas Track,"With the rapid increase of the world's urban population, the infrastructure of the constantly expanding metropolitan areas is undergoing an immense pressure. To meet the growing demands of sustainable urban environments and improve the quality of life for citizens, municipalities will increasingly rely on novel transport solutions. In particular, Unmanned Aerial Vehicles (UAVs) are expected to have a crucial role in the future smart cities thanks to their interesting features such as autonomy, flexibility, mobility, adaptive altitude, and small dimensions.  However, densely populated megalopolises of the future are administrated by several municipals, governmental and civil society actors, where vivid economic activities involving a multitude of individual stakeholders take place. In such megalopolises, the use of agents for UAVs is gaining more interest especially in complex application scenarios where coordination and cooperation are necessary. This paper sketches a visionary view of the UAVs' role in the transport domain of future smart cities. Additionally, four challenging research directions are highlighted including problems related to autonomy, explainability, security and validation & verification of the UAVs behavior.",Multiagent Systems; Unmanned Aerial Vehicles; Intelligent Transport Systems; Smart Cities,Yazan,,Mualla,"CIAD, Univ. Bourgogne Franche-Comté, UTBM",Amro,,Najjar,Umeå University,Stéphane,,Galland,"CIAD, Univ. Bourgogne Franche-Comté, UTBM",Christophe,,Nicolle,"CIAD, Univ. Bourgogne Franche-Comté, UB",Igor,,Haman Tchappi,University of Ngaoundere,Ansar-Ul-Haque,,Yasar,"Transportation Research Institute (IMOB), Hasselt University",Kary,,Främling,Umeå University,,,,,,,,,,,,,,,,,,,,
fp0705,Parameterized Heuristics for Incomplete Weighted CSPs with Elicitation Costs,,"Weighted Constraint Satisfaction Problems (WCSPs) are an elegant paradigm for modeling combinatorial optimization problems. A key assumption in this model is that all constraints are specified or known a priori, which does not hold in some applications where constraints may encode preferences of human users. Incomplete WCSPs (IWCSPs) extend WCSPs by allowing some constraints to be partially specified, and they can be elicited from human users during the execution of IWCSP algorithms. Unfortunately, existing approaches assume that the elicitation of preferences does not incur any additional cost. This assumption is unrealistic as human users are likely bothered by repeated elicitations and will refuse to provide an unbounded number of preferences. Therefore, we propose the IWCSP with Elicitation Cost (IWCSP+EC) model, which extends IWCSPs to include elicitation costs, as well as three parameterized heuristics that allow users to trade off solution quality for fewer elicited preferences and faster computation times. They provide theoretical quality guarantees for problems where elicitations are free. Our model and heuristics thus extend the state of the art in constraint reasoning to better model and solve agent-based applications with user preferences.",Weighted CSPs; Incomplete Weighted CSPs; Preference Elicitation,Atena,M.,Tabakhi,Washington University in St. Louis,William,,Yeoh,Washington University in St. Louis,Makoto,,Yokoo,Kyushu University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0707,A New Concept of Convex based Multiple Neural Networks Structure,,"In this paper, a new concept of convex based multiple neural networks structure is proposed. This new approach uses the collective information from multiple neural networks to train the model. From both theoretical and experimental analysis, it is going to demonstrate that the new approach gives a faster training speed of convergence with a similar or even better test accuracy, compared to a conventional neural network structure. Two experiments are conducted to demonstrate the performance of our new structure: the first one is a semantic frame parsing task for spoken language understanding (SLU) on ATIS dataset, and the other is a hand written digits recognition task on MNIST dataset. We test this new structure using both recurrent neural network and convolutional neural networks through these two tasks. The results of both experiments demonstrate a 4x-8x faster training speed with better or similar performance by using this new concept.",Multiple Models? Neural Networks; Machine Learning;NLP; Image Recognition,Yu,,Wang,Samsung Research America,Yue,,Deng,Samsung Research America,Yilin,,Shen,Samsung Research America,Hongxia,,Jin,Samsung Research America,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0708,Prediction in Intelligence: An Empirical Comparison of Off-policy Algorithms on Robots,,"The ability to continually make predictions about the world may be central to intelligence. Off-policy learning and general value functions (GVFs) are well-established algorithmic techniques for learning about many signals while interacting with the world. In the past couple of years, many ambitious works have used off-policy GVF learning to improve control performance in both simulation and robotic control tasks. Many of these works use semi-gradient temporal-difference (TD) learning algorithms, like Q-learning, which are potentially divergent. In the last decade, several TD learning algorithms have been proposed that are convergent and computationally efficient, but not much is known about how they perform in practice, especially on robots. In this work, we perform an empirical comparison of modern off-policy GVF learning algorithms on three different robot platforms, providing insights into their strengths and weaknesses. We also discuss the challenges of conducting fair comparative studies of off-policy learning on robots and develop a new evaluation methodology that is successful and applicable to a relatively complicated robot domain.",artificial intelligence; robotics; reinforcement learning; off-policy learning; temporal-difference learning; general value functions,Banafsheh,,Rafiee,University of Alberta,Sina,,Ghiassian,University of Alberta,Adam,,White,University of Alberta,Richard,S.,Sutton,University of Alberta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0720,Reducing Sampling Error in Policy Gradient Learning,,"This paper studies a class of reinforcement learning algorithms known as policy gradient methods. Policy gradient methods optimize the performance of a policy by estimating the gradient of the expected return with respect to the policy parameters. One of the core challenges of applying policy gradient methods is obtaining an accurate estimate of this gradient. Most policy gradient methods rely on Monte Carlo sampling to estimate this gradient. When only a limited number of environment steps can be collected, Monte Carlo policy gradient estimates may suffer from sampling error -- samples receive more or less weight than they will in expectation. In this paper, we introduce the Sampling Error Corrected policy gradient estimator that corrects the inaccurate Monte Carlo weights. Our approach treats the observed data as if it were generated by a different policy than the policy that actually generated the data. It then uses importance sampling between the two -- in the process correcting the inaccurate Monte Carlo weights. Under a limiting set of assumptions we can show that this gradient estimator will have lower variance than the Monte Carlo gradient estimator. We show experimentally that our approach improves the learning speed of two policy gradient methods compared to standard Monte Carlo sampling even when the theoretical assumptions fail to hold.",Reinforcement learning; Policy gradient; importance sampling; Monte Carlo,Josiah,P.,Hanna,The University of Texas at Austin,Peter,,Stone,The University of Texas at Austin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0721,A Decade in Hindsight: The Missing Bridge Between Multi-Agent Systems and the World Wide Web,,"The World Wide Web has evolved drastically over the past decade -- and the proliferation of Web APIs has turned it into the middleware of choice for most distributed systems. The recent focus on hypermedia-driven APIs together with initiatives such as the Web of Things and Linked Data are now promoting and advancing the development of a new generation of dynamic, open, and long-lived systems on the Web. These systems require agent-based solutions to the point that Web researchers have started to build autonomous systems on their own. It is thus both timely and necessary to investigate and align the latest developments in Web research and multi-agent systems (MAS) research. We argue that the answer lies equally in a lack of practical use cases as well as the premature development and alignment of Web and agent technologies. We then present our vision for a new generation of autonomous systems on the Web, which we call hypermedia MAS, together with the research opportunities and challenges they bring.",Hypermedia; Multi-Agent Systems; Web of Things; Semantic Web; Linked Data,Andrei,,Ciortea,"University of St. Gallen & Inria, Université Côte d'Azur, CNRS",Simon,,Mayer,University of St. Gallen & ETH Zürich,Fabien,,Gandon,"Inria, Université Côte d'Azur, CNRS",Olivier,,Boissier,"MINES Saint-Étienne, CNRS",Alessandro,,Ricci,University of Bologna,Antoine,,Zimmermann,"MINES Saint-Étienne, CNRS",,,,,,,,,,,,,,,,,,,,,,,,
fp0725,An Optimization Approach for Structured Agent-Based Provider/Receiver Tasks,,"This work contributes an optimization framework in the context of structured interactions between an agent playing the role of a `provider' and a human `receiver'. Examples of provider/receiver interactions of interest include ones between occupational therapist and patient, or teacher and student. We specifically consider tasks where the provider agent needs to plan a sequence of actions with a fixed horizon, where actions are organized along a hierarchy with increasing probabilities of success and associated costs. The goal of the provider is to achieve a success with the lowest expected cost possible. In our application domains, a success may be for instance eliciting a desired behavior or a correct response from the receiver. We present a linear-time optimal planning algorithm that generates cost-optimal sequences for given action parameters. We also provide proofs for a number of properties of optimal solutions that align with typical human provider strategies. Finally, we instantiate our general formulation in the context of robot-assisted therapy tasks for children with Autism Spectrum Disorders (ASD). In this context, we present methods for determining action parameters, namely (1) an online survey with experts for determining action costs, and (2) a probabilistic model of child response based on data collected in a real child-robot interaction scenario. Our contributions may unlock increased levels of adaptivity for agents introduced in a variety of assistive contexts.",Social agents; User modeling; Optimization; Human-robot interaction; Robot-assisted therapy,Kim,,Baraka,"Instituto Superior Técnico / INESC-ID, Universidade de Lisboa & Carnegie Mellon University",Marta,,Couto,"Hospital Garcia de Orta, EPE",Francisco,S,Melo,Universidade de Lisboa & Instituto Superior Técnico / INESC-ID,Manuela,,Veloso,Carnegie Mellon University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0734,Trust-Aware Behavior Reflection for Robot Swarm Self-Healing,,"The deployment of robot swarms is influenced by real-world factors, such as motor issues, sensor failure, and wind disturbances. These factors cause the appearance of faulty robots. In a decentralized swarm, sharing incorrect information from faulty robots will lead to undesired swarm behaviors, such as swarm disconnection and incorrect heading directions. We envision a system where a human operator is exerting supervisory control over a remote swarm by indicating changes in trust to the swarm via a ""trust-signal"". By correcting faulty behaviors, trust between the human and the swarm is maintained to facilitate human-swarm cooperation. In this research, a trust-aware behavior reflection method -- \textit{\textbf{Trust-R}} -- is designed based on a weighted mean subsequence reduced algorithm (WMSR). By using Trust-R, detected faulty behaviors are automatically corrected by the swarm in a decentralized fashion by referring to the motion status of their trusted neighbors and isolating failed robots from the others. Based on real-world scenarios, three types of robot faults -- degraded performance caused by motor wear, abnormal motion caused by system uncertainty and motion deviation caused by an external disturbance such as wind -- were simulated to test the effectiveness of Trust-R. Results show that Trust-R is effective in correcting swarm behaviors for swarm self-healing.",Trust-R; WMSR; Trust; Behavior Reflection; Swarm Self-Healing,Rui,,Liu,Carnegie Mellon University,Fan,,Jia,Carnegie Mellon University,Wenhao,,Luo,Carnegie Mellon University,Meghan,,Chandarana,Carnegie Mellon University,Changjoo,,Nam,Korea Institute of Science and Technology,Michael,,Lewis,University of Pittsburgh,Katia,,Sycara,Carnegie Mellon University,,,,,,,,,,,,,,,,,,,,
fp0749,Bayesian Reinforcement Learning in Factored POMDPs,,"Model-based Bayesian Reinforcement Learning (BRL) provides a principled solution to dealing with the exploration-exploitation trade-off, but such methods typically assume a fully observable environments. The few Bayesian RL methods that are applicable in partially observable domains, such as the Bayes-Adaptive POMDP (BA-POMDP), scale poorly.  To address this issue, we introduce the Factored BA-POMDP model (FBA-POMDP), a framework that is able to learn a compact model of the dynamics by exploiting the underlying structure of a POMDP. The FBA-POMDP framework casts the problem as a planning task, for which we adapt the Monte-Carlo Tree Search planning algorithm and develop a belief tracking method to approximate the joint posterior over the state and model variables. Our empirical results show that this method outperforms a number of BRL baselines and is able to learn efficiently when the factorization is known, as well as learn both the factorization and the model parameters simultaneously.",Bayesian reinforcement learning; POMDPs; Monte-Chain Monte-Carlo; Monte-Carlo Tree Search; Bayes Networks,Sammie,,Katt,Northeastern University,Frans,A.,Oliehoek,Delft University of Technology,Christopher,,Amato,Northeastern University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0759,Reinforcement Learning in Stationary Mean-field Games,,"Multi-agent reinforcement learning has made significant progress in recent years, but it remains a hard problem. Hence, one often resorts to developing learning algorithms for specific classes of multi-agent systems. In this paper we study reinforcement learning in a specific class of multi-agent systems systems called mean-field games. In particular, we consider learning in stationary mean-field games. We identify two different solution concepts---stationary mean-field equilibrium and stationary mean-field social-welfare optimal policy---for such games based on whether the agents are non-cooperative or cooperative, respectively. We then generalize these solution concepts to their local variants using bounded rationality based arguments. For these two local solution concepts, we present two reinforcement learning algorithms. We show that the algorithms converge to the right solution under mild technical conditions and demonstrate this using two numerical examples.",Multi-agent reinforcement learning; mean-field games; stationary mean-field games; bounded rationality,Jayakumar,,Subramanian,McGill University,Aditya,,Mahajan,McGill University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0761,Incentivizing Collaboration in a Competition,,"Research and design competitions aim to promote innovation or creative production, which are often best achieved through collaboration. The nature of a competition, however, typically necessitates sorting by individual performance. This presents tradeoffs for the competition designer, between incentivizing global performance and distinguishing individual capability. We model this situation in terms of an abstract collaboration game, where individual effort also benefits neighboring agents. We propose a scoring mechanism called LSWM that rewards agents based on localized social welfare.
We show that LSWM promotes global performance, in that social optima are equilibria of the mechanism. Moreover, we establish conditions under which the mechanism leads to increased collaboration, and under which it ensures a formally defined distinguishability property. Through experiments, we evaluate the degree of distinguishability achieved whether or not the theoretical conditions identified hold.",collaboration incentives; mechanism design; scoring competitions,Arunesh,,Sinha,University of Michigan,Michael,P.,Wellman,University of Michigan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0762,Stream Reasoning Agents,Blue Sky Ideas Track,"Data streams are increasingly needed for different types of applications and domains, where dynamicity and data velocity are of foremost importance.
In this context, research challenges raise regarding the generation, publication, processing, and discovery of these streams, especially in distributed, heterogeneous and collaborative environments such as the Web.
Stream reasoning has addressed some of these challenges in the last decade, presenting a novel data processing paradigm that lays at the intersection among semantic data modeling, stream processing, and inference techniques.
However, stream reasoning works have focused almost exclusively on architectures and approaches that assume an isolated processing environment.
Therefore, they lack, in general, the means for discovering, collaborating, negotiating, sharing, or validating data streams on a highly heterogeneous ecosystem as the Web.
Agents and multi-agent systems research has long developed principles and foundations for enabling some of these features, although usually under assumptions that require to be revised in order to comply with the characteristics of data streams.
This paper presents a vision for a Web of stream reasoning agents, capable of sharing not only streaming data, but also processing duties, using collaboration and negotiation protocols, while relying on common vocabularies and protocols that take into account the high dynamicity of their knowledge, goals, and behavioral patterns.",MAS; stream reasoning; Web streams,Riccardo,,Tommasini,Politecnico di Milano,Davide,,Calvaresi,University of Applied Sciences & Arts Western Switzerland HES-SO,Jean-Paul,,Calbimonte,University of Applied Sciences & Arts Western Switzerland HES-SO,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0786,Swarms Can be Rational,,"A fundamental challenge in multi-robot systems is spatial coordination (avoiding collisions) between robots,
each under its own control. Swarm methods, where by robots coordinate \textit{ad-hoc} and \textit{locally},
offer a promising approach. However, while empirically demonstrated to be viable in practice, no guarantees of
performance are known. % journal:, nor a formalization of the task in a way that admits analysis.
This paper formalizes a class of multi-robot cooperative tasks as differential extensive-form games. We show that the system coordination overhead is a differential function, forming a connection between the theoretical maximum-payoff equilibrium of the system, and the rational self-interested choices of individual robots during task execution: \textit{robot swarms can be rational in theory}. We then show how to approximate the rational decision-making in practice using reinforcement learning, % journal while operating strictly within
using internal measures for rewards. We empirically show this leads to consistent optimal performance in with physical and simulated robots.",Multi-Robot Systems; Swarm; Game-Theory,Yinon,,Douchan,Tel Aviv University,Ran,,Wolf,Bar Ilan University,Gal,,Kaminka,The MAVERICK Group & Bar-Ilan University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0791,Human-guided Trajectory Adaptation for Tool Transfer,,"We introduce ""transfer by correction"": a method for transferring a robot's tool-based task models to use unfamiliar tools. By having the robot receive corrections from a human teacher when repeating a known task with a new tool, it can learn the relationship between the two tools, allowing it to transfer additional tasks learned with the original tool to the new tool. The goal is to enable the robot to generalize its task knowledge to accommodate tool replacements and thus be more robust to changes in its environment. We demonstrate how the tool transform models learned from one episode of task corrections can be used to perform that task with >=85% of maximum performance in 83% of tool/task combinations. Furthermore, these transformations generalize to unseen tool/task combinations in 27.8% of our transfer evaluations, and up to 41% of transfer problems when the source and replacement tool share tooltip similarities. Overall, these results indicate that successful task adaptation for a new tool is dependent on the the tool's usage within that task, and that the transform model learned from interactive corrections can be generalized to other tasks providing a similar context for the new tool.",Human-robot interaction; Interactive learning; Transfer learning,Tesca,,Fitzgerald,Georgia Institute of Technology,Elaine,,Short,University of Texas at Austin,Ashok,,Goel,Georgia Institute of Technology,Andrea,,Thomaz,University of Texas at Austin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0799,Don't Put All Your Strategies in One Basket: Playing Green Security Games with Imperfect Prior Knowledge,,"Security efforts for wildlife monitoring and protection of endangered species (e.g., elephants, rhinos, etc.) are constrained by limited resources available to law enforcement agencies. Recent progress in Green Security Games (GSGs) has led to patrol planning algorithms for strategic allocation of limited patrollers to deter adversaries in environmental settings.
Unfortunately, previous approaches to these problems suffer from several limitations. Most notably, (i) previous work in GSG literature relies on exploitation of error-prone machine learning (ML) models of poachers' behavior trained on (spatially) biased historical data; and (ii) online learning approaches for repeated security games (similar to GSGs) do not account for spatio-temporal scheduling constraints while planning patrols, potentially causing significant shortcomings in the effectiveness of the planned patrols.
Thus, this paper makes the following novel contributions: (I) We propose MINION-sm, a novel online learning algorithm for GSGs which does not rely on any prior error-prone model of attacker behavior, instead, it builds an implicit model of the attacker on-the-fly while simultaneously generating scheduling-constraint-aware patrols. MINION-sm achieves a sublinear regret against an optimal hindsight patrol strategy. (II) We also propose MINION, a hybrid approach where our MINION-sm model and an ML model (based on historical data) are considered as two patrol planning experts and we obtain a balance between them based on their observed empirical performance. (III) We show that our online learning algorithms significantly outperform existing state-of-the-art solvers for GSGs.",Green Security Games; Game Theory; Online Learning; Adversarial Bandits; Machine Learning; Wildlife Protection,Shahrzad,,Gholami,University of Southern California,Amulya,,Yadav,Pennsylvania State University College of Information Sciences and Technology,Long,,Tran-Thanh,"University of Southampton, UK Electronics and",Bistra,,Dilkina,University of Southern California,Milind,,Tambe,University of Southern California,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0802,Robot Learning by Collaborative Network Training: A Self-Supervised Method using Ranking,,"We introduce Collaborative Network Training -- a self-supervised method for training neural networks with aims of: 1) enabling task objective functions that are not directly differentiable w.r.t. the network output; 2) generating continuous-space actions; 3) more direct optimization for achieving a desired task; 4) learning parameters when a process for measuring performance is available, but labeled data is unavailable. The procedure involves three randomly initialized independent networks that use ranking to train one another on a single task. The method incorporates qualities from ensemble and reinforcement learning as well as gradient free optimization methods such as Nelder-Mead. We evaluate the method against various baselines using a variety of robotics-related tasks including inverse kinematics, controls, and planning in both simulated and real-world environments.",robot learning; collaborative network training; controls,Mason,,Bretan,Samsung Research America,Sageev,,Oore,Dalhousie University & Vector Institute,Siddharth,,Sanan,Samsung Research America,Larry,,Heck,Samsung Research America,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0805,Outcome-based Partner Selection in Collective Risk Dilemmas,,"Understanding how to design agents that sustain cooperation in multi-agent systems has been a long lasting goal in distributed Artificial Intelligence. Solutions proposed rely on identifying defective agents and avoid cooperating or interacting with them. These mechanisms of social control are traditionally studied in games with linear and deterministic payoffs, such as the Prisoner's Dilemma or the Public Goods Game. In reality, however, agents often face dilemmas in which payoffs are uncertain and non-linear, as collective success requires a minimum number of cooperators. These games are called Collective Risk Dilemmas (\textbf{CRD}), and it is unclear whether the previous mechanisms of cooperation remain effective in this case. Here we study cooperation in \textbf{CRD} through partner-based selection. First, we discuss an experiment in which groups of humans and robots play a \textbf{CRD}. We find that people only prefer cooperative partners when they lose a previous game (\textit{i.e.}, when collective success was not previously achieved). Secondly, we develop a simplified evolutionary game theoretical model that sheds light on these results, pointing the evolutionary advantages of selecting cooperative partners only when a previous game was lost. We show that this strategy constitutes a convenient balance between strictness (only interact with cooperators) and softness (cooperate and interact with everyone), thus suggesting a new way of designing agents that promote cooperation in \textbf{CRD}.",Cooperation; Collective Risk Dilemma; Game Theory; Partner selection; Human-Robot Interaction; Complex systems,Fernando,P.,Santos,Princeton University,Samuel,F.,Mascarenhas,"INESC-ID & Instituto Superior Tecnico, Universidade Lisboa",Francisco,C.,Santos,"INESC-ID & Instituto Superior Tecnico, Universidade Lisboa",Filipa,,Correia,"INESC-ID & Instituto Superior Tecnico, Universidade Lisboa",Samuel,,Gomes,"INESC-ID & Instituto Superior Tecnico, Universidade Lisboa",Ana,,Paiva,"INESC-ID & Instituto Superior Tecnico, Universidade Lisboa",,,,,,,,,,,,,,,,,,,,,,,,
fp0827,Fully Convolutional One-Shot Object Segmentation for Industrial Robotics,,"The ability to identify and localize new objects robustly and effectively is vital for robotic grasping and manipulation in warehouses or smart factories. Deep convolutional neural networks (DCNNs) have achieved the state-of-the-art performance on established image datasets for object detection and segmentation. However, applying DCNNs in dynamic industrial scenarios, e.g., warehouses and autonomous production, remains a challenging problem. DCNNs quickly become ineffective when tasked with detecting objects that they have not been trained on. Given that re-training using the latest data is time consuming, DCNNs cannot meet the requirement of the \emph{Factory of the Future (FoF)} regarding rapid development and production cycles. To address this problem, we propose a novel one-shot object segmentation framework, using a fully convolutional Siamese network architecture, to detect previously unknown objects based on a single prototype image. We turn to multi-task learning to reduce training time and improve classification accuracy. Furthermore, we introduce a novel approach to automatically cluster the learnt feature space representation in a weakly supervised manner. We test the proposed framework on the RoboCup@Work dataset, simulating requirements for the \emph{FoF}. Results show that the trained network on average identifies $73\percent$ of previously unseen objects correctly from a single example image. Correctly identified objects are estimated to have a $87.53\percent$ successful pick-up rate. Finally, multi-task learning lowers the convergence time by up to $33\percent$, and increases accuracy by $2.99\percent$.",One-Shot Segmentation; Fully Convolutional Network; Industrial Robotics,Benjamin,,Schnieders,University of Liverpool,Shan,,Luo,University of Liverpool,Gregory,,Palmer,University of Liverpool,Karl,,Tuyls,University of Liverpool,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0838,Cooperative Concurrent Games,,"In rational verification, one is interested in understanding which temporal logic properties will hold in a concurrent game, under the assumption that players choose strategies that form an equilibrium. Players are assumed to behave rationally in pursuit of individual goals, typically specified as temporal logic formulae. To date, rational verification has only been studied in noncooperative settings. In this paper, we extend the rational verification framework to cooperative games, in which players may form coalitions to collectively achieve their goals. We base our study on the computational model given by concurrent game structures and focus on the core as our basic solution concept. We show the core of a concurrent game can be logically characterised using ATL*, and study the computational complexity of key decision problems associated with the core, which range from problems in PSPACE to problems in 3EXPTIME. We also discuss a number of variants of the main definition of the core, leading to the issue of credible coalition formations, and a possible implementation of the main reasoning framework.
",Concurrent Games; Cooperative Games; Logic; Formal Verification,Julian,,Gutierrez,University of Oxford,Sarit,,Kraus,Bar-Ilan University,Michael,,Wooldridge,University of Oxford,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0842,Observational Learning by Reinforcement Learning,,"Observational learning is a type of learning that occurs as a function of observing, retaining and possibly imitating the behaviour of another agent. It is a core mechanism appearing in various instances of social learning and has been found to be employed in several intelligent species, including humans. In this paper, we investigate to what extent the explicit modelling of other agents is necessary to achieve observational learning through machine learning. Especially, we argue that observational learning can emerge from pure Reinforcement Learning (RL), potentially coupled with memory. Through simple scenarios, we demonstrate that an RL agent can leverage the information provided by the observations of an other agent performing a task in a shared environment. The other agent is only observed through the effect of its actions on the environment and never explicitly modeled. Two key aspects are borrowed from observational learning: i) the observer behaviour needs to change as a result of viewing a 'teacher' (another agent) and ii) the observer needs to be motivated somehow to engage in making use of the other agent's behaviour. The later is naturally modeled by RL, by correlating the learning agent's reward with the teacher agent's behaviour.",Reinforcement Learning; Observational Learning; Learning from other agents; Information seeking; Imitation,Diana,,Borsa,DeepMind,Nicolas,,Heess,DeepMind,Bilal,,Piot,DeepMind,Siqi,,Liu,DeepMind,Leonard,,Hasenclever,DeepMind,Remi,,Munos,DeepMind,Olivier,,Pietquin,Google Brain,,,,,,,,,,,,,,,,,,,,
fp0848,Safe Policy Search Using Gaussian Process Models,,"We propose a method to optimise the parameters of a policy which will be used to safely perform a given task in a data-efficient manner. We train a Gaussian process model to capture the system dynamics, based on the PILCO framework. The model has useful analytic properties, which allow closed form computation of error gradients and the probability of violating given state space constraints. Even during training, only policies that are deemed safe are implemented on the real system, minimising the risk of catastrophic failure.
",Model-based reinforcement learning; Safety critical systems; Gaussian processes,Kyriakos,,Polymenakos,University of Oxford,Alessandro,,Abate,University of Oxford,Stephen,,Roberts,University of Oxford,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0850,Agent-Environment Interactions in Large-Scale Multi-Agent Based Simulation Systems,,"In this paper we present the Action-Potential/Result (APR) model for agent-environment interactions in Multi-Agent Based Simulation systems (MABS) involving thousands of perception-based agents executing on a single host. The environment structure is partitioned into cells which are managed by specialized agents called controller and coordinator. The agents send their stimuli to the controller managing the cell in which they are situated. The controller assesses the received agent stimuli as well as user-triggered and events propagation stimuli. It combines them, attempts to resolve potential conflicts, communicates with adjacent controllers and the coordinator, and ensures that the updated environment state is communicated back to its agents. The APR model has been implemented as a component of the DIVAs framework and can be reused with minimal changes for the construction of agent-based simulations with DIVAs. Experimental results show a significant improvement in scalability over conventional centralized solutions.",Multi-Agent Simulation Systems; Interaction Model,Mohammad,,Al-Zinati,Jordan University of Science and Technology,Rym,,Zalila-Wenkstern,The University of Texas at Dallas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0860,Using Reinforcement Learning to Optimize the Policies of an Intelligent Tutoring System for Interpersonal Skills Training,,"Reinforcement Learning (RL) has been applied successfully to Intelligent Tutoring Systems (ITSs) in a limited set of well-defined domains such as mathematics and physics. This work is unique in using a large state space and for applying RL to tutoring interpersonal skills. Interpersonal skills are increasingly recognized as critical to both social and economic development. In particular, this work enhances an ITS designed to teach basic counseling skills that can be applied to challenging issues such as sexual harassment and workplace conflict. An initial data collection was used to train RL policies for the ITS, and an evaluation with human participants compared a hand-crafted ITS which had been used for years with students (control) versus the new ITS guided by RL policies. The RL condition differed from the control condition most notably in the strikingly large quantity of guidance it provided to learners. Both systems were effective and there was an overall significant increase from pre- to post-test scores. Although learning gains did not differ significantly between conditions, learners had a significantly higher self-rating of confidence in the RL condition. Confidence and learning gains were both part of the reward function used to train the RL policies, and it could be the case that there was the most room for improvement in confidence, an important learner emotion. Thus, RL was successful in improving an ITS for teaching interpersonal skills without the need to prune the state space (as previously done).
",Intelligent Tutoring Systems; Interpersonal Skills Training; Social Agents; Reinforcement Learning,Kallirroi,,Georgila,University of Southern California,Mark,G.,Core,University of Southern California,Benjamin,D.,Nye,University of Southern California,Shamya,,Karumbaiah,University of Pennsylvania,Daniel,,Auerbach,University of Southern California,Maya,,Ram,University of Southern California,,,,,,,,,,,,,,,,,,,,,,,,
fp0879,Facility Location Games with Externalities,,"Facility location games study the scenario where a facility is to be placed based on the reported information from agents. In the society where there are relationships between agents, it is quite natural that one agent's gain will affect other agents' gain (either increase for a collaborator or decrease for a competitor). By using externality to represent this type of agent interaction, for the first time we introduce it into the facility location games in this paper. Namely, we study the extension where agents' utilities will be affected by other agents. We derive necessary and sufficient conditions for well known existing mechanisms and also prove strong lower bounds.",Mechanism Design; Facility Location Games; Externalities,Minming,,Li,City University of Hong Kong & University of Hong Kong Shenzhen Research Institute,Lili,,Mei,Caritas Institute of Higher Education,Yi,,Xu,Xi'an University of Technology,Guochuan,,Zhang,Zhejiang University,Yingchao,,Zhao,Caritas Institute of Higher Education,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0884,A Multi-task Selected Learning Approach for Solving  3D Flexible Bin Packing Problem,,"A 3D flexible bin packing problem (3D-FBPP) arises from the process of warehouse packing in e-commerce. An online customer's order usually contains several items and needs to be packed as a whole before shipping. In particular, 5% of tens of millions of packages are using plastic wrapping as outer packaging every day, which brings pressure on the plastic surface minimization to save traditional logistics costs. Because of the huge practical significance, we focus on the issue of packing cuboid-shaped items orthogonally into a least-surface-area bin. The existing heuristic methods for classic 3D bin packing don't work well for this particular NP-hard problem and designing a good problem-specific heuristic is non-trivial. In this paper, rather than designing heuristics, we propose a novel multi-task framework based on Selected Learning to learn a heuristic-like policy that generates the sequence and orientations of items to be packed simultaneously. Through comprehensive experiments on a large scale real-world transaction order dataset and online AB tests, we show: 1) our selected learning method trades off the imbalance and correlation among the tasks and significantly outperforms the single task Pointer Network and the multi-task network without selected learning; 2) our method obtains an average 5.47% cost reduction than the well-designed greedy algorithm which is previously used in our online production system.",Intelligent System; Reinforcement Learning; Multi-task Learning; 3D Flexible Bin Packing,Lu,,Duan,"Zhejiang Cainiao Supply Chain Management Co., Ltd",Haoyuan,,Hu,"Zhejiang Cainiao Supply Chain Management Co., Ltd",Yu,,Qian,"Zhejiang Cainiao Supply Chain Management Co., Ltd",Yu,,Gong,"Search Algorithm Team, Alibaba Group",Xiaodong,,Zhang,"Zhejiang Cainiao Supply Chain Management Co., Ltd",Jiangwen,,Wei,"Zhejiang Cainiao Supply Chain Management Co., Ltd",Yinghui,,Xu,"Zhejiang Cainiao Supply Chain Management Co., Ltd",,,,,,,,,,,,,,,,,,,,
fp0889,The Matrix: An Agent-Based Modeling Framework for Data Intensive Simulations,,"Human decision-making is influenced by social, psychological, neurological, emotional, normative, and learning factors, as well as individual traits like age and education level. Social/cognitive computational models that incorporate these factors are increasingly used to study how humans make decisions. A result is that agent models, within agent-based modeling (ABM), are becoming more heavyweight, i.e., are more computationally demanding, making scalability and at-scale simulations all the more difficult to achieve. To address these challenges, we have developed an ABM simulation framework that addresses data-intensive simulation at-scale. We describe system requirements and design, and demonstrate at-scale simulation by modeling 3 million users (each as an individual agent), 13 million repositories, and 239 million user-repository interactions on GitHub. Simulations predict user interactions with GitHub repositories, which, to our knowledge, are the first simulations of this kind. Our simulations demonstrate a three-order of magnitude increase in the number of cognitive agents simultaneously interacting.
",agent-based simulation; simulation framework; distributed simulation,Parantapa,,Bhattacharya,University of Virginia,Saliya,,Ekanayake,Lawrence Berkeley National Laboratory,Chris,J.,Kuhlman,University of Virginia,Christian,,Lebiere,Carnegie Mellon University,Don,,Morrison,Carnegie Mellon University,Samarth,,Swarup,University of Virginia,Mandy,L.,Wilson,University of Virginia,Mark,G.,Orr,University of Virginia,,,,,,,,,,,,,,,,
fp0890,On the Pitfalls of Measuring Emergent Communication,,"How do we know if communication is emerging in a multi-agent system? The vast majority of recent papers on emergent communication show that adding a communication channel leads to an increase in reward or task success. This is a useful indicator, but provides only a coarse measure of the agent's learned communication abilities. As we move towards more complex environments, it becomes imperative to have a set of finer tools that allow qualitative and quantitative insights into the emergence of communication. This may be especially useful to allow humans to monitor agents' behaviour, whether for fault detection, assessing performance, or even building trust. In this paper, we examine a few intuitive existing metrics for measuring communication, and show that they can be misleading. Specifically, by training deep reinforcement learning agents to play simple matrix games augmented with a communication channel, we find a scenario where agents appear to communicate (their messages provide information about their subsequent action), and yet the messages do not impact the environment or other agent in any way. We explain this phenomenon using ablation studies and by visualizing the representations of the learned policies.  We also survey some commonly used metrics for measuring emergent communication, and provide recommendations as to when these metrics should be used.",Deep learning; learning agent capabilities; Multi-agent learning,Ryan,,Lowe,MILA & Facebook AI Research,Jakob,,Foerster,Facebook AI Research,Y-Lan,,Boureau,Facebook AI Research,Joelle,,Pineau,MILA & Facebook AI Research,Yann,,Dauphin,Google AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0893,Privacy-Preserving Federated Data Sharing,,"Consider a set of agents with sensitive datasets who are interested in the same prediction task and would like to share their datasets without revealing private information. For instance, the agents may be medical centers with their own historical databases and the task may be the diagnosis of a rare form of a disease. This paper investigates whether sharing privacy-preserving versions of these datasets may improve the agent predictions. It proposes a Privacy-preserving Federated Data Sharing (PFDS) protocol that each agent can run locally to produce a privacy-preserving version of its original dataset. The PFDS protocol is evaluated on several standard prediction tasks and experimental results demonstrate the potential of sharing privacy- preserving datasets to produce accurate predictors.",Differential Privacy; Federated Learning,Ferdinando,,Fioretto,Georgia Institute of Technology,Pascal,,Van Hentenryck,Georgia Institute of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0902,Learning Curriculum Policies for Reinforcement Learning,,"Curriculum learning in reinforcement learning is a training methodology that seeks to speed up learning of a difficult target task, by first training on a series of simpler tasks and transferring the knowledge acquired to the target task. Automatically choosing a sequence of such tasks (i.e., a curriculum) is an open problem that has been the subject of much recent work in this area. In this paper, we build upon a recent method for curriculum design, which formulates the curriculum sequencing problem as a Markov Decision Process. We extend this model to handle multiple transfer learning algorithms, and show for the first time that a curriculum policy over this MDP can be learned from experience. We explore various representations that make this possible, and evaluate our approach by learning curriculum policies for multiple agents in two different domains. The results show that our method produces curricula that can train agents to perform on a target task as fast or faster than existing methods.",Reinforcement Learning; Transfer Learning; Curriculum Learning,Sanmit,,Narvekar,University of Texas at Austin,Peter,,Stone,University of Texas at Austin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0913,How You Act Tells a Lot: Privacy-Leaking Attack on Deep Reinforcement Learning,,"Machine learning has been widely applied to various applications, some of which involve training with privacy-sensitive data. A modest number of data breaches have been studied, including credit card information in natural language data and identities from face dataset.
However, most of these studies focus on supervised learning models.
As deep reinforcement learning (DRL) has been deployed in a number of real-world systems, such as indoor robot navigation, whether trained DRL policies can leak private information requires in-depth study.To explore such privacy breaches in general, we mainly
propose two methods: environment dynamics search via genetic algorithm and candidate inference based on shadow policies.
We conduct extensive experiments to demonstrate such privacy vulnerabilities in DRL under various settings. We leverage the proposed algorithms to infer floor plans from some trained Grid World navigation DRL agents with LiDAR perception. The proposed algorithm can correctly infer most of the floor plans and reaches an average recovery rate of 95.83\% using policy gradient trained agents.
In addition, we are able to recover the robot configuration in continuous control environments and an autonomous driving simulator with high accuracy.
To the best of our knowledge, this is the first work to investigate privacy leakage in DRL settings and we show that DRL-based agents do potentially leak privacy-sensitive information from the trained policies.",Deep Reinforcement Learning; Privacy; Dynamics Recovery,Xinlei,,Pan,"University of California, Berkeley",Weiyao,,Wang,Duke University,Xiaoshuai,,Zhang,Peking University,Bo,,Li,University of Illinois at Urbana-Champaign,Jinfeng,,Yi,JD AI Research,Dawn,,Song,"University of California, Berkeley",,,,,,,,,,,,,,,,,,,,,,,,
fp0916,What do we express without knowing? Emotion in Gesture,,"Emotional Gestural Expression is an aspect of nonverbal communication that is critical for social agents. The way we perform gestures provides both intentional and unintentional information, leaking our emotional state. In this paper, we analyze the perception of a group of base gestures under a wide set of modifications to understand how users perceive the emotional content of the movement, according to the valence and arousal dimensions of Russell’sCircumplex model. From these results, we are able to extract the perceived emotional quality of the base gesture forms and how the modifications shift that perception. An analysis is provided on the impact of different performance modifications of the gesture",Embodied Computational Agents; Emotional Expression; Gesture Analysis,Gabriel,,Castillo,"University of California, Davis",Michael,,Neff,"University of California, Davis",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0918,Attack-Resilient Connectivity Game for UAV Networks using Generative Adversarial Learning,,"The continuous link connectivity is critical for the efficient collaboration of multiple unmanned aerial vehicles (UAVs). However, the UAV communication environments are not only harsh, but are also confronted with the threats of smart attackers, which pose great barriers in maintaining the links unblocked. In this paper, we leverage the paradigm of the Generative Adversarial Network (GAN) to formulate an attack-resilient connectivity game between a pair of neighboring UAVs and an attacker. In the three-agent adversary game, the attacker acts as the generator, which attempts to generate highly approximate information as the UAVs so as to maximize its jamming capability; while the pairwise UAVs act as the discriminators, which attempt to enhance the capability of refusing the fake information (i.e., the opponent's attack). As the state-of-the-art GAN learning algorithms suffer from the instability dilemma (i.e., either with the unsuccessful convergence or with the low generation/discrimination performance), we incorporate the conditional GAN with the least square objective loss function as well as the mean square error such that the attacker can improve the detection capability from UAVs' historical activity patterns and the UAVs can accordingly adjust the connectivity strategy. We validate the effectiveness of the proposed algorithm through extensive evaluations. Results demonstrate that the proposed algorithm can improve the convergence efficiency, reduce the connection latency, and enhance the attack-resilience capability significantly.",Unmanned aerial vehicles; Connectivity establishment; Smart attacks; Adversarial learning,Bo,,Yang,"Institute of Computing Technology, Chinese Academy of Sciences",Min,,Liu,"Institute of Computing Technology, Chinese Academy of Sciences",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0924,FASTER: Fusion AnalyticS for public Transport Event Response,,"Increasing urban concentration raises operational challenges that can benefit from integrated monitoring and decision support. Such complex systems need to leverage the full stack of analytical methods, from state estimation using multi-sensor fusion for situational awareness, to prediction and computation of optimal responses.  The FASTER platform that we describe in this work, deployed at nation scale and handling 1.5 billion public transport trips a year, offers such a full stack of techniques for this large-scale, real-time problem. FASTER provides fine-grained situational awareness and real-time decision support with the objective of improving the public transport commuter experience. The methods employed range from statistical machine learning to agent-based simulation and mixed-integer optimization. In this work we present an overview of the challenges and methods involved, with details of the commuter movement prediction module, as well as a discussion of open problems.",Real-world AI; Public transport; Spatio-temporal models; Network optimization,Sebastien,,Blandin,IBM Research,Laura,,Wynter,IBM Research,Hasan,,Poonawala,IBM Research,Sean,,Laguna,IBM Research,Basile,,Dura,Ecole Polytechnique,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0933,Gehrlein Stability in Committee Selection: Parameterized Hardness and Algorithms,,"In a multiwinner election based on the Condorcet criterion, we are given a set of candidates, and a set of voters with strict preference ranking over the candidates. A committee is {\em weakly Gehrlein stable} (WGS) if each committee member is preferred to each non-member by at least half of the voters. Recently, Aziz et al. [IJCAI 2017] studied the computational complexity of finding a WGS committee of size $k$. They show that this problem is
{\sf NP}-hard in general and polynomial time solvable when the number of voters is odd. In this article, we initiate a systematic study of the problem in the realm of parameterized complexity. We first show that the problem is {\sf W}[1]-hard when parameterized by the size of the committee. To overcome this intractability result, we use a known reformulation of WGS as a problem on directed graphs and then use parameters that measure the ``structure'' of these directed graphs.

In particular, we consider the majority graph, defined as follows: there is a vertex corresponding to each candidate, and there is a directed arc from a candidate $c$ to $c'$ if the number of voters that {\em prefer} $c$ over $c'$ is more than those that prefer $c'$ over $c$. The problem of finding WGS committee of size $k$ corresponds to finding a vertex subset $X$ of size $k$ in the majority graph with the following property: the set $X$ contains no vertex outside the committee that has an in-neighbor in $X$. Observe that the polynomial time algorithm of Aziz et al. [IJCAI 2017] corresponds to solving the problem on a tournament (a complete graph with orientation on edges). Thus, natural parameters to study our problem are ``closeness'' to being a tournament. We define closeness as the number of missing arcs in the given directed graph and the number of vertices we need to delete from the given directed graph such that the resulting graph is a tournament. We show that the problem is fixed parameter tractable ({\sf FPT}) and admits linear kernels with respect to closeness parameters. Finally, we also design an exact exponential time algorithm running in time $\OO(1.2207^nn^{\OO(1)})$. Here, $n$ denotes the number of candidates.",committee selection; social choice; parameterized complexity,Sushmita,,Gupta,National Institute for Science Education and Research,Pallavi,,Jain,Institute of Mathematical Sciences,Sanjukta,,Roy,The institute of Mathematical Sciences,Saket,,Saurabh,The Institute of Mathematical Sciences,Meirav,,Zehavi,Ben-Gurion University,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0943,Competitive Bridge Bidding with Deep Neural Networks,,"The game of bridge consists of two stages: bidding and playing. While playing is proved to be relatively easy for computer programs, bidding is very challenging. During the bidding stage, each player knowing only his/her own cards needs to exchange information with his/her partner and interfere with opponents at the same time. Existing methods for solving perfect-information games cannot be directly applied to bidding. Most bridge programs are based on human-designed rules, which, however, cannot cover all situations and are usually ambiguous and even conflicting with each other. In this paper, we, for the first time, propose a competitive bidding system based on deep learning techniques, which exhibits two novelties. First, we design a compact representation to encode the private and public information available to a player for bidding. Second, based on the analysis of the impact of other players' unknown cards on one's final rewards, we design two neural networks to deal with imperfect information, the first one inferring the cards of the partner and the second one taking the outputs of the first one as part of its input to select a bid. Experimental results show that our bidding system outperforms the top rule-based program.",contract bridge; reinforcement learning; artificial intelligence,Jiang,,Rong,"Institute of Computing Technology, Chinese Academy of Sciences & University of Chinese Academy of Sciences",Tao,,Qin,Microsoft Research Asia,Bo,,An,Nanyang Technological University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0945,Decision Procedures for Epistemic Logic Exploiting Belief Bases,,"We provide tableau-based PSPACE satisfiability checking procedures for a family of
multi-agent epistemic logics with a semantics defined in terms of belief bases. Such logics distinguish an agent's explicit beliefs, i.e., all facts included in the agent's belief base, from the agent's implicit beliefs, i.e., all facts deducible from the agent's belief base. We provide a simple dynamic extension for one of these logics by propositional assignments performed by agents. A propositional assignment captures a simple form of action
that changes not only the environment but also the agents' beliefs depending on how they jointly perceive its execution.
After having provided a PSPACE satisfiability checking procedure for this dynamic extension,
we show how it can be used in human-robot interaction in which both the human and the robot have higher-order beliefs about the other's beliefs and can modify the environment by acting.",Dynamic Epistemic Logic; Belief Bases; Theory of Mind,Emiliano,,Lorini,"IRIT-CNRS, Toulouse University",Fabian,,Romero,"IRIT, Toulouse University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0949,Multi-Agent Path Finding for UAV Traffic Management,,"Unmanned aerial vehicles (UAVs) are expected to provide a wide range of services, whereby UAV fleets will be managed by several independent service providers in shared low-altitude airspace. One important element, or redundancy, for safe and efficient UAV operation is pre-flight Conflict Detection and Resolution (CDR) methods that generate conflict-free paths for UAVs before the actual flight. Multi-Agent Path Finding (MAPF) has already been successfully applied to comparable problems with ground robots.
However, most MAPF methods were tested with simplifying assumptions which do not reflect important characteristics of many real-world domains, such as delivery by UAVs where heterogeneous agents need to be considered, and new requests for flight operations are received continuously.
In this paper, we extend CBS and ECBS to efficiently incorporate heterogeneous agents with computational geometry and we reduce the search space with spatio-temporal pruning.
Moreover, our work introduces a batching method into CBS and ECBS to address increased amounts of requests for delivery operations in an efficient manner.
We compare the performance of our batching approach in terms of runtime and solution cost to a first-come first-served approach. Our scenarios are based on a study on UAV usage predicted for 2030 in a real area in Japan.
Our simulations indicate that our proposed ECBS based batching approach is more time efficient than incremental planning based on Cooperative A*, and hence can meet the requirements of timely and accurate response on delivery requests to users of such UTM services.",Unmanned Aircraft System Traffic Management; Pre-Flight Conflict Detection and Resolution; Multi-Agent Path Finding; Heterogeneous Agents,Florence,,Ho,National Institute of Informatics,Ana,,Salta,"INESC-ID and Instituto Superior Tecnico, Universidade Lisboa",Ruben,,Geraldes,National Institute of Informatics,Artur,,Goncalves,National Institute of Informatics,Marc,,Cavazza,University of Greenwich,Helmut,,Prendinger,National Institute of Informatics,,,,,,,,,,,,,,,,,,,,,,,,
fp0961,Distributed Heterogeneous Robot-Human Teams,Robotics Track,"We introduce a novel, scalable, distributed decision-making algorithm
using factor graphs and the sum product algorithm to control the
coordination of a heterogeneous multi-robot team in exploration
tasks. In addition, our algorithm supports seamless participation of
human operators at arbitrary levels of interaction. We present
experimental results performed using both simulated and actual teams
of unmanned aerial systems (UAS). Our experiments demonstrate
effective exploration while facilitating human participation with the
team. At the same time, we show how robots with differing capabilities
coordinate their behaviors effectively to leverage each other's
individual strengths, without having to explicitly account for every
possible joint behavior during system design. We demonstrate our
algorithm's suitability for tasks such as weather data collection
using a heterogeneous robot team consisting of fixed- and rotary-wing
UAS. In particular, during 60 flight hours of real-world experiments
collecting weather data, we show that robots using our algorithm were
about seven times more efficient at exploring their environment than
similar systems which flew preplanned flight profiles. One of our
primary contributions is to demonstrate coordinated autonomous control
and decision-making among robots operating in very different flight
regimes.",Robotics; Multi-robot systems; Robot control; Human-Robot Interaction,S M Al,,Mahi,Oklahoma State University,Kyungho,,Nam,Oklahoma State University,Christopher,,Crick,Oklahoma State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0965,Graph Based Optimization for Multiagent Cooperation,,"We address the problem of solving math programs defined over a graph where nodes represent agents and edges represent interaction among agents. The objective and constraint functions of this program model the task agent team must perform and the domain constraints. In this multiagent setting, no single agent observes the complete objective and all the constraints of the program. Thus, we develop a distributed message-passing approach to solve this optimization problem. We focus on the class of graph structured linear and quadratic programs (LPs/QPs) which can model important multiagent coordination frameworks such as distributed constraint optimization (DCOP). For DCOPs, our framework models functional constraints among agents (e.g. resource, network flow constraints) in a much more tractable fashion than previous approaches. Our iterative approach has several desirable properties---it is guaranteed to find the optimal solution for LPs, converges for general cyclic graphs, and is memory efficient making it suitable for resource limited agents, and has anytime property. Empirically, our approach provides solid empirical results on several standard benchmark problems when compared against previous approaches.",Distributed constraint optimization;multiagent cooperation; mathematical optimization,Arambam,James,Singh,Singapore Management University,Akshat,,Kumar,Singapore Management University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0969,Reaching Cooperation using Emerging Empathy and Counter-empathy,,"According to social neuropsychology, the cooperative behavior is largely influenced by empathy, which is deemed essential of emotional system and has wide impact on social interaction. In the work reported here, we believe that the emergence of empathy and counter-empathy is closely related to creatures' inertial impression on intragroup coexistence and competition. Based on this assumption, we establish a unified model of empathy and counter-empathy in light of Hebb's rule. We also present Adaptive Empathetic Learner (AEL), a training method for agents to enable affective utility evaluation and learning procedure in multi-agent system. In AEL, the empathy model is integrated into the adversarial bandit setting in order to achieve a high degree of versatility. Our algorithm is first verified in the survival game, which is designed to simulate the primitive hunting environment. In this game, empathy and cooperation emerge among agents with different power. In another test about Iterated Prisoners' Dilemma, cooperation was reached even between an AEL agent and a rational one. Moreover, when confronted with hostile, the AEL agent showed sufficient goodwill and vigilantly protected its safe payoffs. In the Ultimatum Game, it’s worth mentioning that absolute fairness could be achieved on account of the self-adaptation of empathy and counter-empathy.",cooperation; empathy and counter-empathy; multi-agent system; adversarial bandit,Jize,,Chen,Harbin Institute of Technology,Changhong,,Wang,Harbin Institute of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0974,Effective Collective Summarisation of Distributed Data in Mobile Multi-Agent Systems,,"One of the key applications of physically-deployed multi-agent systems, such as mobile robots, drones, or personal agents in human mobility scenarios, is to promote a pervasive notion of distributed sensing achieved by strict agent cooperation. A quintessential operation of distributed sensing is data summarisation over a region of space, which finds many applications in variations of counting problems: counting items, measuring space, averaging environmental values, and so on. A typical strategy to perform peer-to-peer data summarisation with local interactions is to progressively accumulate information towards one or more collector agents, though this typically exhibits several sources of fragility, especially in scenarios featuring high mobility.

In this paper, we introduce a new multi-agent algorithm for dynamic summarisation of distributed data, called ""parametric weighted multi-path"", based on a local strategy to break, send, and then recombine sensed data across neighbours based on their estimated distance, ultimately resulting in the formation of multiple, dynamic and emergent paths of information flow towards collectors. By empirical evaluation via simulation in synthetic and realistic case studies, accounting for various sources of volatility, using different state-of-the-art distance estimations, and comparing to other existing implementations of aggregation algorithms, we show that parametric weighted multi-path is able to retain adequate accuracy even in high-variability scenarios where all other algorithms are significantly diverging from correct estimations.",data aggregation; adaptive algorithm; aggregate programming; computational field; gradient,Giorgio,,Audrito,University of Torino,Sergio,,Bergamini,University of Torino,Ferruccio,,Damiani,University of Torino,Mirko,,Viroli,University of Bologna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0975,Distributed Environmental Modeling and Adaptive Sampling for Multi-Robot Sensor Coverage,,"We consider the problem of online distributed environmental modeling and adaptive sampling for multi-robot sensor coverage, where a team of robots spread out over the workspace in order to optimize the sensing performance over environmental phenomena, whose distribution is often referred to as a density function. Unlike most existing works that either assume certain knowledge of the density function beforehand or centrally learn the density function assuming global knowledge of collected data from all the robots, we propose a \emph{fully distributed} adaptive sampling approach to allow robots to efficiently learn the \emph{unknown} density function online. In particular, we developed adaptive coverage controllers based on the learned density functions for minimizing the sensing cost. To capture significantly different components of the environmental phenomenon with \emph{only locally collected data} for each robot when global knowledge is not available, we propose a \emph{distributed mixture of Gaussian Processes} algorithm that enables robots to collaboratively learn the global density function by exchanging only model-related parameters. We empirically demonstrate the effectiveness of our algorithm via evaluation on real-world data gathered from agricultural field robot and indoor static sensors.",Multi-Robot Systems; Mixture of Gaussian Processes; Adaptive Sampling; Distributed Robot Systems; Environmental Modeling,Wenhao,,Luo,Carnegie Mellon University,Changjoo,,Nam,Korea Institute of Science and Technology,George,,Kantor,Carnegie Mellon University,Katia,,Sycara,Carnegie Mellon University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0983,Evaluating the Effectiveness of Multi-Agent Organisational Paradigms in a Real-Time Strategy Environment,Engineering Multiagent Systems Track,"We study the impact of using different organisational paradigms on the design and implementation of a Multi-Agent System (MAS) for Real-Time Strategy (RTS) games. We examine systems designed and implemented according to a specific paradigm on their performance in a practical scenario, as well as examining software-engineering concepts like size and complexity entailed by the according implementations. In contrast to related theoretical work, we deal with the practical constraints and implications of the paradigms by targeting the prototypical RTS game StarCraft: Brood War. Through careful analysis of this environment, agent systems for four separate paradigms that operate at different levels of autonomy and communication are designed, implemented, and evaluated by thousands of instrumented runs. One of the main findings is that using a central processing agent, e.g. in a market-based approach, increases task performance, but at the cost of increased code complexity.",MAS; RTS; AI; StarCraft; swarms; hierarchies; markets,Buster,A.,Bernstein,Delft University of Technology,Jasper,C.M.,Geurtz,Delft University of Technology,Vincent,J.,Koeman,Delft University of Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0990,Explainable Agents and Robots: Results from a Systematic Literature Review,Robotics Track,"Humans are increasingly relying on complex systems that heavily adopts Artificial Intelligence (AI) techniques. Such systems are employed in a growing number of domains, and making them explainable is an impelling priority. Recently, the domain of eXplainable Artificial Intelligence (XAI) emerged with the aims of fostering transparency and trustworthiness. Several reviews have been conducted. Nevertheless, most of them deal with data-driven XAI to overcome the opaqueness of black-box algorithms. Contributions addressing goal-driven XAI (e.g., explainable agency for robots and agents) are still missing. This paper aims at filling this gap, proposing a Systematic Literature Review. The main findings are (i) a considerable portion of the papers propose conceptual studies, or lack evaluations or tackle relatively simple scenarios; (ii) almost all of the studied papers deal with robots/agents explaining their behaviors to the human users, and very few works addressed inter-robot (inter-agent) explainability. Finally, (iii) while providing explanations to non-expert users has been outlined as a necessity, only a few works addressed the issues of personalization and context-awareness.",Explainable AI; goal-based XAI; autonomous agents; human-robot interaction,Sule,,Anjomshoae,Umeå University,Amro,,Najjar,Umea University,Davide,,Calvaresi,University of Applied Sciences Western Switzerland (HES-SO),Kary,,Främling,Umeå University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp0991,Natural Strategic Ability under Imperfect Information,,"Strategies in game theory and multi-agent logics are mathematical objects of remarkable combinatorial complexity. Recently, the concept of natural strategies has been proposed to model more human-like reasoning about simple plans and their outcomes. So far, the theory of such simple strategic play was only considered in scenarios where all the agents have perfect information about the state of the game.

In this paper, we extend the notion of natural strategies to games with imperfect information. We also show that almost all the complexity results for model checking carry over from the perfect to imperfect information setting. That is, verification of natural strategies is usually no more complex for agents with uncertainty. This tells games of natural strategic ability clearly apart from most results in game theory and multi-agent logics.","[Agent Theories and Models] Logic and Game Theory; Logics for agents and multi-agents systems; [Verification and Validation of Agent-based Systems] Verification techniques for multi-agents systems, including model checking;",Wojciech,,Jamroga,Polish Academy of Sciences,Vadim,,Malvone,Université d’Evry,Aniello,,Murano,Universitá degli studi di Napoli Federico II,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp1005,Convergence of Multi-Agent Learning with a Finite Step Size in General-Sum Games,,"Learning in a multi-agent system is challenging because agents are simultaneously learning and the environment is not stationary, undermining convergence guarantees. To address this challenge, this paper presents a new gradient-based learning algorithm, called Gradient Ascent with Shrinking Policy Prediction (GA-SPP), which augments the basic gradient ascent approach with the concept of shrinking policy prediction. The key idea behind this algorithm is that an agent adjusts its strategy in response to the forecasted strategy of the other agent, instead of its current one. GA-SPP is shown formally to have Nash convergence in larger settings than existing gradient-based multi-agent learning methods. Furthermore, unlike existing gradient-based methods, GA-SPP's theoretical guarantees do not assume the learning rate to be infinitesimal.",Multi-Agent Learning; Nash Equilibrium; Convergence; Finite Step Size,Xinliang,,Song,Tsinghua University,Tonghan,,Wang,Tsinghua University,Chongjie,,Zhang,Tsinghua University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fp1007,Context-Aware Policy Reuse,,"Transfer learning can greatly speed up reinforcement learning for a new task by leveraging policies of relevant tasks. Existing works of policy reuse either focus on selecting a single best source policy for reuse without considering contexts, or fail to guarantee learning an optimal policy for a target task. To improve transfer efficiency and guarantee optimality, we develop a novel policy reuse method, called Context-Aware Policy reuSe (CAPS), that enables multi-policy reuse. Our method learns when and which source policy is best for reuse, as well as when to terminate its reuse. CAPS provides theoretical guarantees in convergence and optimality for both source policy selection and target task learning. Empirical results on a grid-based navigation domain and the Pygame Learning Environment demonstrate that CAPS significantly outperforms other state-of-the-art policy reuse methods.",policy reuse; transfer learning; reinforcement learning,Siyuan,,Li,Tsinghua University,Fangda,,Gu,Tsinghua University,Guangxiang,,Zhu,Tsinghua University,Chongjie,,Zhang,Tsinghua University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jams886,An Agent-Based Model of the Emergence and Evolution of a Language System for Boolean Coordination,JAAMAS Track,"This paper presents an agent-based model for studying the emergence and evolution of a language system Boolean coordination. The model has been implemented and tested by conducting a series of experiments that show that a language system for Boolean coordination emerges as a result of a process of self-organisation of the agents' linguistic interactions when these agents adapt their preferences for vocabulary, syntactic categories and word order to those they observe are used more often by other agents, and that such a language system is reliably transmitted across generations.",Language Evolution; Agent-Based Model; Coordination,Josefina,,Sierra-Santibanez,Technical University of Catalonia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jams887,Comparative Criteria for Partially Observable Contingent Planning,JAAMAS Track,"In contingent planning under partial observability with sensing actions, the solution can be represented as a plan tree, branching on various possible observations. Typically, one seeks a satisfying plan leading to a goal state at each leaf. In many applications, however, one may prefer some satisfying plans to others.
We focus on the problem of providing valid comparative criteria for contingent plan trees and graphs, allowing us to compare two plans and decide which one is preferable.
We suggest a set of such comparison criteria --- plan simplicity, dominance, and best and worst plan costs. In some cases certain branches of the plan correspond to an unlikely combination of mishaps, and can be ignored, and we provide methods for pruning such unlikely branches before comparing the plan graphs. We explain these criteria, and discuss their validity, correlations, and application to real world problems. We suggest efficient algorithms for computing the comparative criteria. We provide experimental results, showing that plans computed by existing contingent planners can be compared using the suggested criteria.",Contingent Planning; Empirical Evaluation; Partial Observability,Dorin,,Shmaryahu,Ben Gurion University of the Negev,Jöerg,,Hoffmann,Saarland University,Guy,,Shani,Ben Gurion University of the Negev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jams889,How Hard Is It to Control a Group?,,"We consider group identification models in which the aggregation of individual opinions concerning who is qualified in a given society determines the set of socially qualified individuals. In this setting, we study the extent to which social qualification can be changed when societies expand, shrink, or partition themselves. The answers we provide are with respect to the computational complexity of the corresponding control problems and fully cover the class of consent aggregation rules introduced by Samet \& Schmeidler (2003) as well as procedural rules for group identification. We obtain both polynomial-time solvability results and NP-hardness results. For some NP-hard problems, we also derive fixed-parameter algorithms.",Group identification; Consent rules; Procedural rules; Computational complexity;  Parameterized complexity; Control,Yongjie,,Yang,Saarland University,Dinko,,Dimitrov,Saarland University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jams890,An Agent Model Based on Open Linked Data for Building Internet of Agents Ecosystems,,"This paper presents an smart, collaborative and self-adaptive reactive agent model aimed at managing the resources of objects connected to Internet of Things (IoT). This agent model, called Linked Open Agent (LOA), is described using both a semantic agent contract built from descriptors published as linked data, and a workflow for agent control that is completed at runtime by the agent itself to address its behavior. The accuracy for semantic discovering agents partners was evaluated and compared with generic models of discovery such as the Yellow Pages of Java Agent DEvelopment Framework (JADE) and the Java implementation of the Universal Description, Discovery, and Integration (jUDDI). The results demonstrated that our method had a better accuracy for recovering agents than the accuracy of JADE and JUDDI.",Internet of Agents; Internet of Things; Linked Open Data; contract,Pablo,,Pico-Valencia,"Pontifical Catholic University of Ecuador, Esmeraldas",Juan,A.,Holgado-Terriza,University of Granada,José,,Senso,University of Granada,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jams891,Type Checking for Protocol Role Enactments via Commitments,,"This work presents a commitment-based agent typing system. Type checking is done dynamically when an agent enacts a commitment-based protocol role: verification checks if the agent meets the requirements displayed by the role it means to enact. An example implementation in the 2COMM4JADE framework is provided. 2COMM4JADE is based on the Agent&Artifact meta-model and exploits JADE and CArtAgO, by using CArtAgO artifacts in order to reify commitment protocols.
",Agent Typing; Social Relationships; Static and dynamic type checking; Commitments; Commitment-based Interaction Protocols,Matteo,,Baldoni,Università degli Studi di Torino,Cristina,,Baroglio,Università desgli Studi di Torino,Federico,,Capuzzimati,Università desgli Studi di Torino,Roberto,,Micalizio,Università desgli Studi di Torino,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jams892,A Context-aware Convention Formation Framework for Large-Scale Networks,,"Conventions can serve as a useful mechanism for deciding the dominant coordination strategy and facilitating consensus in a multiagent system (MAS). In this paper, we present a  decentralized convention formation framework that harnesses the structural properties and diversity of the network for creating social conventions within large and open multiagent convention spaces. We validate our convention formation framework using a language coordination problem in which agents in a MAS  construct a common lexicon in a decentralized fashion on various networks. Experimentation results indicate that our approach is both effective (able to converge into a large majority convention state with more than 90\% agents sharing a high-quality lexicon) and efficient (faster) as compared to state-of-the-art approaches for social conventions in large convention spaces.",multiagent systems; large dynamic networks;  contextual information; convention; diversity,Mohammad,Rashedul,Hasan,University of Nebraska-Lincoln,Anita,,Raja,Cooper Union,Ana,,Bazzan,Universidade Federal do Rio Grande do Sul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jams896,A Complete Multi-Robot Path-Planning Algorithm,JAAMAS Track,"In the domain of multi-robot path-planning problems, robots must move from their start locations to their goal locations while avoiding collisions with each other. The goal of the research problem that has been tackled in [2] is to find complete solutions for multi-robot path-planning problem, the theoretical analyses already demonstrated the completeness of Push and Spin (PASp) algorithm. However, the PASp algorithm does not guarantee the optimality of the solution.

In this work, some decisions are found within the complete solution that may optimize PASp performance. Two pluggable methods are proposed for this purpose; smooth operation, that aims to reduce the redundant moves in the generated paths, and heuristic search, that evaluates different alternative paths according to its occupancy by other robots. The improved version of PASp algorithm is referred to as PASp+ algorithm. The experimental results showed that, compared to PASp, Push and Swap (PAS), Push and Rotate (PAR), Bibox and the tractable multi-robot path-planning (MAPP) algorithms, adding heuristic search and smooth operation in PASp+ resulted a significant improvement shown in reducing the number of moves for all problem instances with high percentage reaches to 47.8%. However, in higher computation time.",complete algorithms; multi-robot; path planning algorithms,Ebtehal,Turki Saho,Alotaibi,Al-Imam Mohammad Ibn Saud Islamic University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jams897,Computing Optimal Coalition Structures in Polynomial Time,,"The optimal coalition structure determination problem is in general
computationally hard. In this article, we identify some problem instances
for which the space of possible coalition structures has a certain
form and constructively prove that the problem is polynomial time
solvable. Specifically, we
consider games with an ordering over the players and introduce a
\emph{distance metric} for measuring the distance between any two
structures. In terms of this metric, we define the property of
\emph{monotonicity}, meaning that coalition structures closer to the
optimal, as measured by the metric, have higher value than those
further away. Similarly, \emph{quasi-monotonicity} means that part
of the space of coalition structures is monotonic, while part of it is
non-monotonic. (Quasi)-monotonicity is a property that can be
satisfied by coalition games in characteristic function form and also
those in partition function form. For a setting with a monotonic
value function and a known player ordering, we prove that
the optimal coalition structure determination problem is polynomial
time solvable and devise such an algorithm using a greedy
approach. We extend this algorithm to quasi-monotonic value functions
and demonstrate how its time complexity improves from exponential
to polynomial as the degree of monotonicity of the value function
increases. We go further and consider a setting in which the value
function is monotonic and an ordering over the players is known to
exist but ordering itself is unknown. For this setting too, we prove
that the coalition structure determination problem is polynomial time
solvable and devise such an algorithm.",Cooperative game theory; coalition formation; multi agent systems,Shaheen,,Fatima,Loughborough University,Michael,,Wooldridge,Oxford University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,